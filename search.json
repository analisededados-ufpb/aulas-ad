[
  {
    "objectID": "1-introducao/0-apresentacao.html#conteúdo---unidade-1",
    "href": "1-introducao/0-apresentacao.html#conteúdo---unidade-1",
    "title": "Análise de dados",
    "section": "Conteúdo - Unidade 1",
    "text": "Conteúdo - Unidade 1\n\nIntrodução à análise de dados\n\nConceitos básicos\nIntrodução a ferramentas [Python, NumPy, Pandas, Jupyter]\nTratamento de dados\n\nCarregamento, limpeza, transformação\n\nVisualização básica\n\nExploratory Data Analysis (EDA)\n\nEstatística descritiva\nAnálise de correlação\nAnálise exploratória"
  },
  {
    "objectID": "1-introducao/0-apresentacao.html#conteúdo---unidade-2",
    "href": "1-introducao/0-apresentacao.html#conteúdo---unidade-2",
    "title": "Análise de dados",
    "section": "Conteúdo - Unidade 2",
    "text": "Conteúdo - Unidade 2\n\nInferência estatística\n\nSignificância estatística e amostragem\nIntervalos de confiança\nTestes de hipótese\nTestes de permutação e p-valor\n\nRegressão\n\nRegressão linear simples\nRegressão linear multivariada\nRegressão logística"
  },
  {
    "objectID": "1-introducao/0-apresentacao.html#conteúdo---unidade-3",
    "href": "1-introducao/0-apresentacao.html#conteúdo---unidade-3",
    "title": "Análise de dados",
    "section": "Conteúdo - Unidade 3",
    "text": "Conteúdo - Unidade 3\n\nClustering\n\nMedidas de similaridade e divergência\nAgrupamento com K-means\nAgrupamento hierárquico\n\nVisualização de dados\nProjeto de análise de dados"
  },
  {
    "objectID": "1-introducao/0-apresentacao.html#avaliações",
    "href": "1-introducao/0-apresentacao.html#avaliações",
    "title": "Análise de dados",
    "section": "Avaliações",
    "text": "Avaliações\n\nUnidades 1 e 2:\n\nAtividades práticas de análise de dados\n\nRelatórios\nQuestionários\n\n\nUnidade 3:\n\nProjeto de análise de dados\n\nRelatório analítico (ex: blog post, artigo)\nAplicação web / mobile\nDashboard"
  },
  {
    "objectID": "1-introducao/0-apresentacao.html#referências-básicas",
    "href": "1-introducao/0-apresentacao.html#referências-básicas",
    "title": "Análise de dados",
    "section": "Referências básicas",
    "text": "Referências básicas\n\nPython for Data Analysis. McKinney. 3 ed, 2022 (online)\nThe Data Science Design Manual. Skienna. 2017 (pdf, slides)\nPython Data Science Handbook. VanderPlas. 2022 (online)\nPractical Statistics for Data Scientists. Bruce. 2 ed, 2020 (pdf)\nR for Data Science. Wickham. 2017 (online)\nThink Stats: Exploratory Data Analysis in Python. Downey, 2014 (online)"
  },
  {
    "objectID": "1-introducao/0-apresentacao.html#referências-complementares",
    "href": "1-introducao/0-apresentacao.html#referências-complementares",
    "title": "Análise de dados",
    "section": "Referências complementares",
    "text": "Referências complementares\n\nCiência de Dados com R (online)\nModern Dive: An Introduction to Statistical and Data sciences via R (online; slides)\nIntroductory Statistics with Randomization and Simulation\nExploratory Data Analysis with R\nR Programming for Data Science\nAn Introduction to Statistical Learning\nThe Art of Data Science\nOpenIntro Statistics"
  },
  {
    "objectID": "1-introducao/0-apresentacao.html#ambientes-virtuais",
    "href": "1-introducao/0-apresentacao.html#ambientes-virtuais",
    "title": "Análise de dados",
    "section": "Ambientes virtuais",
    "text": "Ambientes virtuais\n\nGoogle Classroom\n\nAvisos gerais, slides e outros materiais\nPrecisa estar logado com a conta do DCX\n\nDiscord\n\nAvisos, dúvidas, discussões\n\nGitHub Classroom\n\nMaterial para atividades práticas, envio de código e relatórios\nCriem uma conta no GitHub e aprendam git!"
  },
  {
    "objectID": "1-introducao/0-apresentacao.html#ferramentas-para-começar-a-brincar",
    "href": "1-introducao/0-apresentacao.html#ferramentas-para-começar-a-brincar",
    "title": "Análise de dados",
    "section": "Ferramentas para começar a brincar",
    "text": "Ferramentas para começar a brincar\n\nPython: linguagem de programação que usaremos no curso\nJupyter Notebook: criar documentos com código executável\nNumPy: biblioteca Python para computação numérica\nPandas: biblioteca Python para análise de dados\nMatplotlib, Seaborn: bibliotecas Python de visualização de dados\nVS Code: IDE usada nas aulas\n\nInstalar extensão Python for VS Code\n\nGoogle Colab: ambiente de execução de notebooks\n\nPode ser usado para executar código integrado ao Github\n\n\n\n\nProf. Marcus Carvalho @ DCX / CCAE / UFPB"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#tendências-de-busca",
    "href": "1-introducao/1-introducao-ad.html#tendências-de-busca",
    "title": "Análise de dados",
    "section": "Tendências de busca",
    "text": "Tendências de busca\n\nFonte: Google Trends"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#dados-to-informação-to-conhecimento-to-sabedoria",
    "href": "1-introducao/1-introducao-ad.html#dados-to-informação-to-conhecimento-to-sabedoria",
    "title": "Análise de dados",
    "section": "Dados \\(\\to\\) Informação \\(\\to\\) Conhecimento \\(\\to\\) Sabedoria",
    "text": "Dados \\(\\to\\) Informação \\(\\to\\) Conhecimento \\(\\to\\) Sabedoria\n\nFonte: The Application of Visual Analytics to Financial Stability Monitoring"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#conhecimento-é-poder",
    "href": "1-introducao/1-introducao-ad.html#conhecimento-é-poder",
    "title": "Análise de dados",
    "section": "Conhecimento é poder!",
    "text": "Conhecimento é poder!"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#quem-tem-interesse",
    "href": "1-introducao/1-introducao-ad.html#quem-tem-interesse",
    "title": "Análise de dados",
    "section": "Quem tem interesse?",
    "text": "Quem tem interesse?\n\nEmpresas querem insights para melhorar seus negócios\nCidadãos querem mais transparência de dados e informação\nGovernos querem mecanismos para melhorar fiscalização"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#o-que-é-análise-e-ciência-de-dados",
    "href": "1-introducao/1-introducao-ad.html#o-que-é-análise-e-ciência-de-dados",
    "title": "Análise de dados",
    "section": "O que é análise e ciência de dados?",
    "text": "O que é análise e ciência de dados?\n\nComo toda área nova, não há definições muito claras\nAlgumas definições relacionadas da Gartner:\n\nBusiness Inteligence (BI): Aplicações, ferramentas e melhores práticas que permitem o acesso e a análise de informações para melhorar e otimizar decisões e desempenho.\nAnalytics: Processo de analizar informações de um domínio particular. Ou a aplicação de BI para uma área específica.\nBig data: Grande volume, velocidade e/ou variedade de dados que exigem processamento eficiente e inovador de informações permitindo insights, decisões e automação de processos.\nCientista de dados: Extrai insights de dados. Requer habilidades analíticas para detectar padrões."
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#ciência-de-dados",
    "href": "1-introducao/1-introducao-ad.html#ciência-de-dados",
    "title": "Análise de dados",
    "section": "Ciência de dados",
    "text": "Ciência de dados\n\n\n\nIncorpora elementos de:\n\nExploratory Data Analysis (EDA) e Visualização de dados;\nMachine Learning e Estatística;\nComputação de Alto Desempenho\n\nHabilidades necessárias:\n\nCiência da Computação\nMatemática e Estatística\nExpertise nos domínios da aplicação\n\n\n\n\n\n\nFonte: Steven Skiena - Lecture 1: Introduction to Data Science"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#por-que-ciência-de-dados-agora",
    "href": "1-introducao/1-introducao-ad.html#por-que-ciência-de-dados-agora",
    "title": "Análise de dados",
    "section": "Por que ciência de dados agora?",
    "text": "Por que ciência de dados agora?\n\n\nNovas tecnologias para processar dados em larga escala\n\nRedes sociais, IoT, Cloud Computing, Big Data, Machine Learning\n\nSucesso de análise de dados em empresas serviu de modelo\n\nGoogle, Facebook, Amazon, Microsoft"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#apreciando-dados",
    "href": "1-introducao/1-introducao-ad.html#apreciando-dados",
    "title": "Análise de dados",
    "section": "Apreciando dados",
    "text": "Apreciando dados\n\nCientistas da computação geralmente não apreciam dados\n\nSó é algo que eles carregam nos programas\nGeralmente usam dados aleatórios para testar programas\nConjuntos de dados interessantes são recursos raros, que requerem trabalho duro e imaginação para obtê-los"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#section",
    "href": "1-introducao/1-introducao-ad.html#section",
    "title": "Análise de dados",
    "section": "",
    "text": "Cientista real\n\n\nSe esforça para entender a bagunça do mundo real\nNada é completamente verdadeiro ou falso\nDirecionados a dados\nObsessão por descobrir\nUtiliza dados com erros\n\n\n\nCientista da computação\n\n\nConstroe seu próprio mundo virtual organizado\nTudo é completamente verdadeiro ou falso\nDirecionado a algoritmos\nObsessão por inventar\nUtiliza dados corretos\n\n\n\n\n\nCientistas de dados devem pensar como cientistas reais!"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#ciência-tradicional-x-ciência-de-dados",
    "href": "1-introducao/1-introducao-ad.html#ciência-tradicional-x-ciência-de-dados",
    "title": "Análise de dados",
    "section": "Ciência tradicional x ciência de dados",
    "text": "Ciência tradicional x ciência de dados\n\nCiência tradicional: formulação de hipóteses e obtenção de dados específicos para confirmá-las ou negá-las.\nCiência de dados: geração de dados em larga escala para realizar novas descobertas ao analisá-los.\nDuas formas importantes de pensar:\n\nDado um problema, quais dados ajudarão a resolvê-lo?\nDado um conjunto de dados, que problemas interessantes se aplicarão a ele?"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#genialidade-x-sabedoria",
    "href": "1-introducao/1-introducao-ad.html#genialidade-x-sabedoria",
    "title": "Análise de dados",
    "section": "Genialidade x Sabedoria",
    "text": "Genialidade x Sabedoria\n\nDesenvolvedores são contratados para produzir código\nCientistas de dados são contratados para produzir insights\nGenialidade é saber encontrar a resposta correta\nSabedoria é saber evitar as respostas erradas!\nA sabedoria vem de:\n\nExperiência, conhecimento geral, ouvir os outros\nHumildade: observar quanto, como e porque você errou\n\n\n\nCiência de dados se beneficia mais de sábios que de gênios"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#desenvolvendo-a-curiosidade",
    "href": "1-introducao/1-introducao-ad.html#desenvolvendo-a-curiosidade",
    "title": "Análise de dados",
    "section": "Desenvolvendo a curiosidade",
    "text": "Desenvolvendo a curiosidade\n\nO bom cientista de dados desenvolve curiosidade sobre o domínio / aplicação que ele está trabalhando\nConversa com pessoas envolvidas nos dados que trabalha\nAcompanha notícias para ter uma visão ampla do mundo\nEles são encorajados a perguntar:\n\nQue coisas interessantes você consegue aprender de um certo conjunto de dados?\nQue coisas você realmente quer saber?\nQue conjunto de dados pode levar você a isso?"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#imdb-dados-de-filmes-e-séries",
    "href": "1-introducao/1-introducao-ad.html#imdb-dados-de-filmes-e-séries",
    "title": "Análise de dados",
    "section": "IMDb: dados de filmes e séries",
    "text": "IMDb: dados de filmes e séries"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#imdb-dados-de-atores",
    "href": "1-introducao/1-introducao-ad.html#imdb-dados-de-atores",
    "title": "Análise de dados",
    "section": "IMDb: dados de atores",
    "text": "IMDb: dados de atores"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#imdb-perguntas-12",
    "href": "1-introducao/1-introducao-ad.html#imdb-perguntas-12",
    "title": "Análise de dados",
    "section": "IMDb: Perguntas? [1/2]",
    "text": "IMDb: Perguntas? [1/2]\n\nQuais atores atuaram em mais filmes? Nos melhores filmes? Ganharam mais dinheiro? Com carreiras mais longas?\nQuais os filmes com melhores/piores avaliações? Por ano? Por gênero? Mais caros? Com elencos mais poderosos?\nFilmes com maior faturamento recebem notas maiores e mais prêmios? É possível prever a nota e o faturamento?\nComo os filmes de Hollywood se comparam a filmes indianos em: avaliação, orçamento, rendimento? Filmes americanos são mais bem avaliados do que estrangeiros?"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#imdb-perguntas-22",
    "href": "1-introducao/1-introducao-ad.html#imdb-perguntas-22",
    "title": "Análise de dados",
    "section": "IMDb: Perguntas? [2/2]",
    "text": "IMDb: Perguntas? [2/2]\n\nComo é a rede social da participação de atores em filmes?\n\nExemplo: grafo que indica graus a partir de Kevin Bacon\n\nQual é a distribuição de idade de atores e atrizes em filmes? Atrizes tendem a ser mais jovens que atores?\nQuais gêneros de filmes recebem as melhores avaliações? Quais gêneros são mais caros? Quais faturam mais?\nQual a probabilidade de uma pessoa gostar de um filme? (Exemplo: recomendações do Netflix)"
  },
  {
    "objectID": "1-introducao/1-introducao-ad.html#referências",
    "href": "1-introducao/1-introducao-ad.html#referências",
    "title": "Análise de dados",
    "section": "Referências",
    "text": "Referências\n\nSkiena S.S. (2017) What is Data Science?. In: The Data Science Design Manual. Texts in Computer Science. Springer.\n\nCSE 519: Data Science - Steven Skiena - Lecture 1: Introduction to Data Science\n\n\n\n\nProf. Marcus Carvalho @ DCX / CCAE / UFPB"
  },
  {
    "objectID": "1-introducao/2-intro-python-jupyter.html",
    "href": "1-introducao/2-intro-python-jupyter.html",
    "title": "Aulas AD 1",
    "section": "",
    "text": "Nós iremos usar a linguagem Python para demonstrações nas aulas e atividades práticas do curso. Espera-se que você já tenha algum contato com Python em disciplinas anteriores. Além disso, nós usaremos o Jupyter Notebook para criação de notebooks que servirão para fazer as atividades práticas e os relatórios de análise de dados.\nPara se familiarizar com as ferramentas e montar o ambiente de desenvolvimento necessário para o curso, nessa atividade vocês irão instalar o Python e o Jupyter Notebook e seguir um passo a passo de um Notebook para revisar os conceitos básicos dessas ferramentas. Além de conseguir rodar um notebook na sua máquina local, você também deve ser capaz de experimentar a execução de notebooks Jupyter em ambientes de execução na nuvem, como através do GitHub Codespaces e do Google Colab.\nNesta atividade, você deve seguir o passo a passo dos capítulos 2 - Python Language Basics, IPython, and Jupyter Notebooks e 3 - Built-In Data Structures, Functions, and Files do livro Python for Data Science, seguindo suas instruções através de notebooks dos capítulos.\nNo repositório do livro no GitHub o autor disponibiliza o Notebook reference a estes capítulos nos arquivos ch02.ipynb e ch03.ipynb. Nesta atividade, você deve ser capaz de abrir cada Notebook em um ambiente de execução e executar o seu código.\nAntes de começar, você deve:\n\nInstalar o Python\nInstalar o Jupyter Notebook\nInstalar o git\n\n\n\nVamos iniciar com a execução do Notebook do capítulo 2 (ch02.ipynb)\n\n\nPara rodar localmente, você deve baixar o arquivo ch02.ipynb para a sua máquina. Para isso, você pode fazer um clone do repositório que possui todos os notebooks do livro, rodando em um terminal:\ngit clone https://github.com/wesm/pydata-book \nEm seguida, entre no diretório do repositório e mude para a branch da 3a edição do livro, que iremos usar nas atividades, rodando:\ncd pydata-book\ngit checkout 3rd-edition\n\n\n\n\n\n\nDica\n\n\n\nSe não quiser baixar todo o repositório do livro, você pode fazer o download apenas do arquivo específico no repositório.\n\n\n\n\n\nNo diretório onde está o arquivo do notebook (pydata-book), inicie o Jupyter rodando no terminal:\njupyter notebook\nEste comando deve abrir um browser com o Jupyter Notebook em execução, como na imagem abaixo:\n\nLocalize o nome do arquivo do notebook ch02.ipynb e clique em seu nome para abrir.\nSe deu tudo certo, você deve ver uma tela como a de baixo:\n\nO notebook combina blocos de texto (Markdown) com blocos de código. Você deve fazer a leitura do notebook e ser capaz de executar todas as células que contêm código. Para executar uma célula de código, clique na célula para selecioná-la e clique no botão Run do menu acima. O código é apresentado no bloco In e a saída gerada pela execução do código é apresentada no bloco Out, conforme imagem abaixo.\n\nVocê deve seguir a execução dos blocos na sequência, porque pode existir uma dependência do código de baixo com o de cima. Se quiser rodar todas as células na sequência, você pode ir no menu Cell -> Run All. Verique se houve algum erro na execução de alguma célula, verificando as saídas que cada uma gerou.\nVocê também pode editar o conteúdo do notebook, alterando o código ou o texto dos blocos, ou adicionando novas células.\nInsira uma nova célula de código no final do notebook com código Python que imprime essa mensagem:\nprint(\"Agora eu sei editar e executar um Jupyter Notebook!\")\nVocê pode inserir novas células escolhendo na barra de menu o tipo de célula (Code para código ou Markdown para texto) e clicando no botão +. Você também pode fazer isso clicando em Insert no menu superior.\n\n\n\nOs notebooks jupyter também podem ser executados diretamente de alguma IDE que dê suporte para ele, como VS Code (com a extensão Python) e PyCharm. Para isso vocês devem ter instalado o Python e o Jupyter Notebook anteriormente.\nTeste executar o notebook em sua IDE favorita (durante a disciplina usarei o VS Code para demonstrações).\n\n\n\n\nVocê também pode rodar o notebook em algum serviço da nuvem gratuito como o Google Colab. Para isso, acesse em seu browser o endereço https://colab.research.google.com.\n\n\n\n\n\n\nAtenção\n\n\n\nO ideal é estar logado com a sua conta Google do domínio @dcx.ufpb.br para usar o Google Notebook.\n\n\nAo abrir o Colab, você pode fazer o upload do arquivo notebook (.ipynb) que deseja executar. Uma forma ainda mais fácil é criar um notebook usando a opção GitHub, inserindo o link do repositório do livro https://github.com/wesm/pydata-book, escolhendo a ramificação (branch) 3rd-edition e clicando no nome do arquivo ch02.ipynb para abrí-lo, como na imagem abaixo:\n\nCom isso, você terá um ambiente de execução do notebook que pode ser editado e executado de forma similar ao ambiente Jupyter, mas que irá rodar na nuvem da Google e poderá ser compartilhado facilmente com outras pessoas. A tela de edição do notebook no Colab é mostrada abaixo:\n\n\n\n\nO GitHub também criou um ambiente de desenvolvimento e execução de código na própria plataforma, chamada GitHub Codespaces, que abre no browser uma IDE similar ao VS Code onde você também consegue executar notebooks Jupyter.\nPara criar um Codespace para o notebook da atividade, entre na página do repositório no GitHub, escolha a branch correta (3rd-edition), clique no botão Code e em seguida em Create codespace on 3rd-edition, conforme a imagem abaixo.\n\n\n\n\n\n\n\nDica\n\n\n\nVocê também pode criar um Codespace apertando a tecla . do teclado na página do repositório e da branch que você deseja.\n\n\n\n\n\nPara fazer outro teste dos passos feitos anteriormente e também fazer uma revisão de estruturas de dados, funções e arquivos em Python, você deve seguir o passo a passo do notebook do capítulo 3, lendo e executando os blocos do notebook.\nEscolha o ambiente de execução mais adequado para você (Jupyter, IDE ou Colab) e faça novamente os passos anteriores, agora com o arquivo ch03.ipynb do repositório.\n\n\n\n\nMcKinney, Wes. Python for Data Analysis(3rd Edition). O’Reilly Media, 2022."
  },
  {
    "objectID": "2-eda/1-intro-eda.html#o-que-é-eda",
    "href": "2-eda/1-intro-eda.html#o-que-é-eda",
    "title": "Análise de dados",
    "section": "O que é EDA?",
    "text": "O que é EDA?\n\nSumarização e visualização de dados para analisar suas características principais\nCiclo iterativo:\n\nFaça perguntas sobre os dados.\nProcure respostas visualizando, transformando e modelando os dados.\nUse o que aprendeu para refinar perguntas e/ou fazer novas perguntas.\n\nInicialmente, investigue qualquer ideia que surgir\n\nAlgumas darão certo, outras não."
  },
  {
    "objectID": "2-eda/1-intro-eda.html#section",
    "href": "2-eda/1-intro-eda.html#section",
    "title": "Análise de dados",
    "section": "",
    "text": "“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#fazendo-perguntas",
    "href": "2-eda/1-intro-eda.html#fazendo-perguntas",
    "title": "Análise de dados",
    "section": "Fazendo perguntas",
    "text": "Fazendo perguntas\n\nSeu objetivo na EDA é desenvolver um entendimento dos dados\n\nA forma mais fácil é fazendo perguntas para guiar sua investigação\nFoca sua atenção em partes específicas dos dados\nAjuda a decidir que gráficos, modelos ou transformações fazer\n\nÉ um processo criativo\n\nProduzir grande quantidade de perguntas para obter qualidade\nComo fazer perguntas reveladoras sem conhecer os dados?\n\nCada pergunta expõe um novo aspecto dos dados\n\nAumenta a chance de novas descobertas\nLeva a insights interessantes deixando as perguntas levarem a outras"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#tipos-de-perguntas",
    "href": "2-eda/1-intro-eda.html#tipos-de-perguntas",
    "title": "Análise de dados",
    "section": "Tipos de perguntas",
    "text": "Tipos de perguntas\n\nDois tipos muito úteis para novas descobertas:\n\nQue tipo de variação ocorre em cada variável?\nQue tipo de covariação ocorre entre as minhas variáveis?\n\nAlgumas definições:\n\nVariável: quantidade, qualidade ou propriedade que se pode medir\nValor: estado de uma variável quando você a mede (pode mudar)\nObservação: conjunto de medições feitas em condições similares"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#o-que-é-variância-e-por-que-analisá-la",
    "href": "2-eda/1-intro-eda.html#o-que-é-variância-e-por-que-analisá-la",
    "title": "Análise de dados",
    "section": "O que é variância e por que analisá-la?",
    "text": "O que é variância e por que analisá-la?\n\nTendência de variável mudar seu valor entre uma medição e outra\nCada variável tem seu próprio padrão de variação\n\nPode revelar informações interessantes\n\nVisualizar a distribuição dos dados ajuda a entender padrões\nComo visualizar distribuições?\n\nDepende se dados são categóricos ou contínuos"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#visualizando-dados-categóricos",
    "href": "2-eda/1-intro-eda.html#visualizando-dados-categóricos",
    "title": "Análise de dados",
    "section": "Visualizando dados categóricos",
    "text": "Visualizando dados categóricos\n\nVariável que só pode assumir pequeno conjunto de valores\nExaminando a distribuição com gráfico de barras:\n\n\n\n\ndfw\n\n\n\n\n\n  \n    \n      \n      Count\n    \n    \n      Cause\n      \n    \n  \n  \n    \n      Carrier\n      64263.16\n    \n    \n      ATC\n      84856.50\n    \n    \n      Weather\n      11235.42\n    \n    \n      Security\n      343.15\n    \n    \n      Inbound\n      118427.82\n    \n  \n\n\n\n\n\n\nax = dfw.plot.bar(legend=False, figsize=(5, 4))"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#visualizando-dados-contínuos",
    "href": "2-eda/1-intro-eda.html#visualizando-dados-contínuos",
    "title": "Análise de dados",
    "section": "Visualizando dados contínuos",
    "text": "Visualizando dados contínuos\n\nVariável com valor em um conjunto infinito de valores possíveis\nHistograma ou computando faixas manualmente\n\n\n\n\nbinPop = pd.cut(population, 10)\nbinPop.value_counts()\n\n(0.527, 4.233]      24\n(4.233, 7.902]      14\n(7.902, 11.571]      6\n(11.571, 15.24]      2\n(15.24, 18.909]      1\n(18.909, 22.578]     1\n(22.578, 26.247]     1\n(33.585, 37.254]     1\n(26.247, 29.916]     0\n(29.916, 33.585]     0\nName: Population, dtype: int64\n\n\n\n\nax = population.plot.hist(figsize=(5, 4))\nax = ax.set_xlabel('Population (millions)')"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-histograma",
    "href": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-histograma",
    "title": "Análise de dados",
    "section": "Visualizando dados contínuos: Histograma",
    "text": "Visualizando dados contínuos: Histograma\n\nO parâmetro bins define a quantidade de intervalos\n\nExemplo para estados com menos de 10 milhões de habitantes\n\n\n\nax = population[population < 10].plot.hist(bins=5, figsize=(5, 4))"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-density-plot",
    "href": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-density-plot",
    "title": "Análise de dados",
    "section": "Visualizando dados contínuos: density plot",
    "text": "Visualizando dados contínuos: density plot\n\nSimilar ao histograma, mas com linha contínua suavizada\n\nExemplo para taxas de homicídio nos Estados Unidos\n\n\n\nax = murder_rate.plot.hist(density=True, xlim=[0,12], bins=range(1,12))\nax = murder_rate.plot.density(ax=ax)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-percentis",
    "href": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-percentis",
    "title": "Análise de dados",
    "section": "Visualizando dados contínuos: percentis",
    "text": "Visualizando dados contínuos: percentis\n\nFaz recortes dos dados ordenados em posições específicas\nPode ser usado para examinar distribuição dos dados\nÉ muito comum reportar os quartis (25, 50 e 75 percentil)\n\n50 percentil é a mediana, divide os dados ao meio\n\nTambém é usado para analisar valores na cauda (ex: 99 percentil)\nExemplo: percentis para taxa de homicídios nos Estados Unidos\n\n\nmurder_rate.quantile([0.05, 0.25, 0.5, 0.75, 0.95])\n\n0.05    1.600\n0.25    2.425\n0.50    4.000\n0.75    5.550\n0.95    6.510\nName: Murder.Rate, dtype: float64"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-boxplot",
    "href": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-boxplot",
    "title": "Análise de dados",
    "section": "Visualizando dados contínuos: boxplot",
    "text": "Visualizando dados contínuos: boxplot\n\nMostra visualmente estatísticas populares de uma distribuição"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-boxplot-1",
    "href": "2-eda/1-intro-eda.html#visualizando-dados-contínuos-boxplot-1",
    "title": "Análise de dados",
    "section": "Visualizando dados contínuos: boxplot",
    "text": "Visualizando dados contínuos: boxplot\n\n\n\nExemplo: populações dos EUA\n\nObservamos que:\n\nA mediana é aproximadamente 5 milhões de habitantes\nMetade dos estados tem entre 2 e 7 milhões de habitantes\nPopulações acima de 13 milhões são consideradas incomuns (outliers)\n\n\n\n\n\nax = population.plot.box()"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#outliers",
    "href": "2-eda/1-intro-eda.html#outliers",
    "title": "Análise de dados",
    "section": "Outliers",
    "text": "Outliers\n\nOutliers são valores muito distantes dos outros\n\nPodem ser dados com erros, mas também podem ser válidos\n\nSe for erro podem ser excluídos, mas com boa justificativa\n\nRequer investigação mais detalhada para entender o caso\nExemplo das populações outliers e impacto nas estatísticas:\n\n\n\n\n\nstate[state['Population'] > 13]\n\n\n\n\n\n  \n    \n      \n      State\n      Population\n      Murder.Rate\n      Abbreviation\n    \n  \n  \n    \n      4\n      California\n      37.253956\n      4.4\n      CA\n    \n    \n      8\n      Florida\n      18.801310\n      5.8\n      FL\n    \n    \n      31\n      New York\n      19.378102\n      3.1\n      NY\n    \n    \n      42\n      Texas\n      25.145561\n      4.4\n      TX\n    \n  \n\n\n\n\n\n\nstate.Population.agg([\"mean\", \"median\"])\n\nmean      6.162876\nmedian    4.436369\nName: Population, dtype: float64"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#o-que-é-covariância-e-por-que-analisá-la",
    "href": "2-eda/1-intro-eda.html#o-que-é-covariância-e-por-que-analisá-la",
    "title": "Análise de dados",
    "section": "O que é covariância e por que analisá-la?",
    "text": "O que é covariância e por que analisá-la?\n\nCovariância descreve o comportamento entre variáveis.\n\nTendência de 2 ou mais variáveis variarem juntas, de forma relacionada.\n\nÚtil para encontrar padrões de relacionamento entre variáveis\n\nE criar modelos que estimam valores com base nessas relações\nEx: estimar votos de candidatos com base nos gastos de campanha\n\nA melhor forma de identificar covariância é visualizar o relacionamento entre variáveis\n\nNovamente, isto depende dos tipos das variáveis"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-duas-variáveis-categóricas",
    "href": "2-eda/1-intro-eda.html#covariância-duas-variáveis-categóricas",
    "title": "Análise de dados",
    "section": "Covariância: duas variáveis categóricas",
    "text": "Covariância: duas variáveis categóricas\n\nVamos analisar uma base de dados de empréstimos\n\n\n\n\nloans\n\n\n\n\n\n  \n    \n      \n      status\n      grade\n    \n  \n  \n    \n      0\n      Fully Paid\n      B\n    \n    \n      1\n      Charged Off\n      C\n    \n    \n      2\n      Fully Paid\n      C\n    \n    \n      3\n      Fully Paid\n      C\n    \n    \n      4\n      Current\n      B\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      450956\n      Current\n      D\n    \n    \n      450957\n      Current\n      D\n    \n    \n      450958\n      Current\n      D\n    \n    \n      450959\n      Current\n      D\n    \n    \n      450960\n      Fully Paid\n      A\n    \n  \n\n450961 rows × 2 columns\n\n\n\n\n\nO nível do empréstimo (grade) é uma variável categórica ordinal\n\nHá uma ordem do nível melhor (A) para o pior (G)\n\nO resultado (status) é uma variável não ordinal\n\nNão há ordem pré-definida dos seus valores"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-duas-variáveis-categóricas-1",
    "href": "2-eda/1-intro-eda.html#covariância-duas-variáveis-categóricas-1",
    "title": "Análise de dados",
    "section": "Covariância: duas variáveis categóricas",
    "text": "Covariância: duas variáveis categóricas\n\nPodemos contar as observações para cada combinação\n\nVisualizando com tabela pivot e mapa de calor (heatmap)\n\n\n\n\n\nloans.pivot_table(index='grade', columns='status',\n                  aggfunc=lambda x: len(x))\n\n\n\n\n\n  \n    \n      status\n      Charged Off\n      Current\n      Fully Paid\n      Late\n    \n    \n      grade\n      \n      \n      \n      \n    \n  \n  \n    \n      A\n      1562\n      50051\n      20408\n      469\n    \n    \n      B\n      5302\n      93852\n      31160\n      2056\n    \n    \n      C\n      6023\n      88928\n      23147\n      2777\n    \n    \n      D\n      5007\n      53281\n      13681\n      2308\n    \n    \n      E\n      2842\n      24639\n      5949\n      1374\n    \n    \n      F\n      1526\n      8444\n      2328\n      606\n    \n    \n      G\n      409\n      1990\n      643\n      199\n    \n  \n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax = sns.heatmap(loans_pivot, annot=True, fmt=\"d\")"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-uma-categórica-e-uma-contínua",
    "href": "2-eda/1-intro-eda.html#covariância-uma-categórica-e-uma-contínua",
    "title": "Análise de dados",
    "section": "Covariância: uma categórica e uma contínua",
    "text": "Covariância: uma categórica e uma contínua\n\nCovariância de 2 variáveis da base de dados de voôs cancelados\n\nairline (categórica não ordinal): nome da empresa aérea\npct_carrier_delay (contínua): % de atrasos causados pela empresa\n\n\n\n\nairline_stats\n\n\n\n\n\n  \n    \n      \n      airline\n      pct_carrier_delay\n      pct_weather_delay\n      pct_atc_delay\n    \n  \n  \n    \n      0\n      American\n      8.153226\n      0.762097\n      1.971774\n    \n    \n      1\n      American\n      5.959924\n      1.585878\n      3.706107\n    \n    \n      2\n      American\n      7.157270\n      2.026706\n      2.706231\n    \n    \n      3\n      American\n      12.100000\n      0.000000\n      11.033333\n    \n    \n      4\n      American\n      7.333333\n      1.774194\n      3.365591\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      33463\n      Southwest\n      6.186422\n      1.651940\n      8.798491\n    \n    \n      33464\n      Southwest\n      9.522167\n      0.261084\n      3.591133\n    \n    \n      33465\n      Southwest\n      9.164179\n      0.343284\n      2.664179\n    \n    \n      33466\n      Southwest\n      5.152293\n      0.122817\n      1.964520\n    \n    \n      33467\n      Southwest\n      3.964393\n      0.019449\n      1.700479\n    \n  \n\n33468 rows × 4 columns"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-uma-categórica-e-uma-contínua-1",
    "href": "2-eda/1-intro-eda.html#covariância-uma-categórica-e-uma-contínua-1",
    "title": "Análise de dados",
    "section": "Covariância: uma categórica e uma contínua",
    "text": "Covariância: uma categórica e uma contínua\n\nBoxplot e density plot da contínua agrupado pela categórica\n\n\n\n\nax = airline_stats.boxplot(by='airline',\n  column='pct_carrier_delay', figsize=(6, 5))\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nax = sns.kdeplot(data=airline_stats,\n  x=\"pct_carrier_delay\", hue=\"airline\")"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-uma-categórica-e-uma-contínua-2",
    "href": "2-eda/1-intro-eda.html#covariância-uma-categórica-e-uma-contínua-2",
    "title": "Análise de dados",
    "section": "Covariância: uma categórica e uma contínua",
    "text": "Covariância: uma categórica e uma contínua\n\nviolin plot é outra alternativa ao boxplot\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nax = sns.violinplot(data=airline_stats,\n  x='airline', y='pct_carrier_delay', ax=ax, inner='quartile')\n\n\n\n\n\n\nMostra a densidade da contínua para cada grupo da categóricas\nÉ mais fácil observar a concentração dos dados.\nNota-se:\n\nUma concentração mais perto de 0% para a Alaska\nDelta e United com mais valores extremos (outliers)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas",
    "href": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas",
    "title": "Análise de dados",
    "section": "Covariância: duas variáveis contínuas",
    "text": "Covariância: duas variáveis contínuas\n\n\n\nVamos analisar a variação diária da cotação de empresas de telecomunicação na bolsa de valores\nSe quisermos comparar a covariância da cotação de duas empresas?\n\nTeremos duas variáveis contínuas\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      T\n      CTL\n      FTR\n      VZ\n      LVLT\n    \n  \n  \n    \n      2012-07-02\n      0.422496\n      0.140847\n      0.070879\n      0.554180\n      -0.519998\n    \n    \n      2012-07-03\n      -0.177448\n      0.066280\n      0.070879\n      -0.025976\n      -0.049999\n    \n    \n      2012-07-05\n      -0.160548\n      -0.132563\n      0.055128\n      -0.051956\n      -0.180000\n    \n    \n      2012-07-06\n      0.342205\n      0.132563\n      0.007875\n      0.140106\n      -0.359999\n    \n    \n      2012-07-09\n      0.136883\n      0.124279\n      -0.023626\n      0.253943\n      0.180000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2015-06-25\n      0.049342\n      -1.600000\n      -0.040000\n      -0.187790\n      -0.330002\n    \n    \n      2015-06-26\n      -0.256586\n      0.039999\n      -0.070000\n      0.029650\n      -0.739998\n    \n    \n      2015-06-29\n      -0.098685\n      -0.559999\n      -0.060000\n      -0.504063\n      -1.360000\n    \n    \n      2015-06-30\n      -0.503298\n      -0.420000\n      -0.070000\n      -0.523829\n      0.199997\n    \n    \n      2015-07-01\n      -0.019737\n      0.080000\n      -0.050000\n      0.355811\n      0.139999\n    \n  \n\n754 rows × 5 columns"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas-1",
    "href": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas-1",
    "title": "Análise de dados",
    "section": "Covariância: duas variáveis contínuas",
    "text": "Covariância: duas variáveis contínuas\n\nScatterplot comparando cotação diária da Verizon (V) x AT&T (T)\n\nCom transparência (alpha) destacamos a concentração dos pontos\n\n\n\n\n\nax = telecom.plot.scatter(x='T', y='VZ',\n       figsize=(6, 5))\n\n\n\n\n\n\nax = telecom.plot.scatter(x='T', y='VZ',\n       alpha=0.4, linewidth=0, figsize=(6, 5))"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas-2",
    "href": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas-2",
    "title": "Análise de dados",
    "section": "Covariância: duas variáveis contínuas",
    "text": "Covariância: duas variáveis contínuas\n\nOutras alternativas: hexbin e jointplot (com kind='hex')\n\n\n\n\nax = telecom.plot.hexbin(x='T', y='VZ', gridsize=30, sharex=False, figsize=(6, 5))\n\n\n\n\n\n\nax = sns.jointplot(x='T', y='VZ', kind=\"hex\", height=5, data=telecom)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas-3",
    "href": "2-eda/1-intro-eda.html#covariância-duas-variáveis-contínuas-3",
    "title": "Análise de dados",
    "section": "Covariância: duas variáveis contínuas",
    "text": "Covariância: duas variáveis contínuas\n\nAnalisando tamanho x valor de casas em uma região dos EUA\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      TaxAssessedValue\n      SqFtTotLiving\n      ZipCode\n    \n  \n  \n    \n      3\n      361000.0\n      2000\n      98108\n    \n    \n      4\n      459000.0\n      3150\n      98108\n    \n    \n      10\n      202000.0\n      830\n      98108\n    \n    \n      11\n      210000.0\n      1130\n      98108\n    \n    \n      12\n      193000.0\n      1560\n      98108\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      498049\n      346000.0\n      1430\n      98105\n    \n    \n      498050\n      463000.0\n      1610\n      98105\n    \n    \n      498051\n      553000.0\n      1580\n      98105\n    \n    \n      498052\n      571000.0\n      1840\n      98105\n    \n    \n      498053\n      694000.0\n      2420\n      98105\n    \n  \n\n19690 rows × 3 columns\n\n\n\n\n\nax = sns.jointplot(data=kc_tax0, x='SqFtTotLiving', y='TaxAssessedValue', kind=\"hex\", height=5)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#covariância-duas-contínuas-e-uma-categórica",
    "href": "2-eda/1-intro-eda.html#covariância-duas-contínuas-e-uma-categórica",
    "title": "Análise de dados",
    "section": "Covariância: duas contínuas e uma categórica",
    "text": "Covariância: duas contínuas e uma categórica\n\n\n\nA localização (ZipCode) da casa também influencia o preço?\nUsamos facets para quebrar em um subplot por categoria\nNota-se a influência tanto da localização quanto do tamanho no valor da casa\n\n\n\n\nCode\ndef hexbin(x, y, color, **kwargs):\n    cmap = sns.light_palette(color, as_cmap=True)\n    plt.hexbin(x, y, gridsize=25, cmap=cmap, **kwargs)\n\ng = sns.FacetGrid(kc_tax_zip, col='ZipCode', col_wrap=2)\nax = g.map(hexbin, 'SqFtTotLiving', 'TaxAssessedValue', \n      extent=[0, 3500, 0, 700000])\n\ng.set_axis_labels('Finished Square Feet', 'Tax Assessed Value')\ng.set_titles('Zip code {col_name:.0f}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n::::"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---diamantes",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---diamantes",
    "title": "Análise de dados",
    "section": "Exemplo EDA - diamantes",
    "text": "Exemplo EDA - diamantes\n\nDados de diamantes com variáveis: quilates (caret), qualidade do corte (cut), cor (color), claridade (clarity), preço (price), etc.\n\n\n\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53935\n      0.72\n      Ideal\n      D\n      SI1\n      60.8\n      57.0\n      2757\n      5.75\n      5.76\n      3.50\n    \n    \n      53936\n      0.72\n      Good\n      D\n      SI1\n      63.1\n      55.0\n      2757\n      5.69\n      5.75\n      3.61\n    \n    \n      53937\n      0.70\n      Very Good\n      D\n      SI1\n      62.8\n      60.0\n      2757\n      5.66\n      5.68\n      3.56\n    \n    \n      53938\n      0.86\n      Premium\n      H\n      SI2\n      61.0\n      58.0\n      2757\n      6.15\n      6.12\n      3.74\n    \n    \n      53939\n      0.75\n      Ideal\n      D\n      SI2\n      62.2\n      55.0\n      2757\n      5.83\n      5.87\n      3.64\n    \n  \n\n53940 rows × 10 columns"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-variância",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-variância",
    "title": "Análise de dados",
    "section": "Exemplo EDA - diamantes: Variância",
    "text": "Exemplo EDA - diamantes: Variância\n\n\n\nCorte (categórica ordinal)\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nax = sns.countplot(data=diamond, x='cut',\n  order=['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'])\n\n\n\n\n\n\nQuilates (contínua)\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nax = sns.histplot(data=diamond, x=\"carat\",\n                  binwidth=0.2)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-covariância",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-covariância",
    "title": "Análise de dados",
    "section": "Exemplo EDA - diamantes: covariância",
    "text": "Exemplo EDA - diamantes: covariância\n\nRelação de preço e corte dos diamantes com density e box plot\n\nPor que melhor corte (Ideal) é mais barato?\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax = sns.kdeplot(x=\"price\", hue=\"cut\", data=diamond)\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\nax = sns.boxplot(x='price', y='cut', data=diamond)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-covariância-1",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-covariância-1",
    "title": "Análise de dados",
    "section": "Exemplo EDA - diamantes: covariância",
    "text": "Exemplo EDA - diamantes: covariância\n\nRelação de corte e cor (duas variáveis categóricas)\n\n\ncut_color = diamond[['cut', 'color']].pivot_table(index='cut', columns='color', aggfunc=len)\nfig, ax = plt.subplots(figsize=(9, 5))\nax = sns.heatmap(cut_color, annot=True, fmt=\"d\", cmap=\"YlGnBu\")"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-covariância-2",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-covariância-2",
    "title": "Análise de dados",
    "section": "Exemplo EDA - diamantes: covariância",
    "text": "Exemplo EDA - diamantes: covariância\n\nRelação de preço e quilates (duas variáveis contínuas)\n\n\nax = sns.jointplot(x=\"carat\", y=\"price\", linewidth=0, alpha=0.01, height=5, data=diamond)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-2-contínuas-e-1-categórica",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-2-contínuas-e-1-categórica",
    "title": "Análise de dados",
    "section": "Exemplo EDA - diamantes: 2 contínuas e 1 categórica",
    "text": "Exemplo EDA - diamantes: 2 contínuas e 1 categórica\n\ng = sns.FacetGrid(diamond, col=\"cut\", col_wrap=3)\nax = g.map_dataframe(sns.scatterplot, x=\"carat\", y=\"price\", alpha=0.1, linewidth=0)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-2-contínuas-e-2-categóricas",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---diamantes-2-contínuas-e-2-categóricas",
    "title": "Análise de dados",
    "section": "Exemplo EDA - diamantes: 2 contínuas e 2 categóricas",
    "text": "Exemplo EDA - diamantes: 2 contínuas e 2 categóricas\n\n\n\nCode\ndiamond[\"price (k$)\"] = diamond[\"price\"] / 1000\ng = sns.FacetGrid(diamond, row=\"cut\", col=\"clarity\", margin_titles=True, height=1.3)\nax = g.map_dataframe(sns.scatterplot, x=\"carat\", y=\"price (k$)\", alpha=0.3, linewidth=0)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#padrões-nos-dados",
    "href": "2-eda/1-intro-eda.html#padrões-nos-dados",
    "title": "Análise de dados",
    "section": "Padrões nos dados",
    "text": "Padrões nos dados\n\nSe encontrar padrões, pergunte-se:\n\nEste padrão acontece por coincidência?\nComo você descreve o relaciomento implicado pelo padrão?\nQuão forte é o relacionado implicado pelo padrão?\nQue outras variáveis afetam este relacionamento?\nO relacionamento muda se você olhar subgrupos individuais dos dados?"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---vulcão",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---vulcão",
    "title": "Análise de dados",
    "section": "Exemplo EDA - vulcão",
    "text": "Exemplo EDA - vulcão\n\nDados de erupções do vulcão Old Faithful Geyser\n\neruptions: duração da erupção em segundos\nwaiting: tempo entre a erupção atual e a seguinte\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      eruptions\n      waiting\n    \n  \n  \n    \n      0\n      3.600\n      79\n    \n    \n      1\n      1.800\n      54\n    \n    \n      2\n      3.333\n      74\n    \n    \n      3\n      2.283\n      62\n    \n    \n      4\n      4.533\n      85\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      267\n      4.117\n      81\n    \n    \n      268\n      2.150\n      46\n    \n    \n      269\n      4.417\n      90\n    \n    \n      270\n      1.817\n      46\n    \n    \n      271\n      4.467\n      74\n    \n  \n\n272 rows × 2 columns"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---vulcão-variância",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---vulcão-variância",
    "title": "Análise de dados",
    "section": "Exemplo EDA - vulcão: variância",
    "text": "Exemplo EDA - vulcão: variância\n\nAnalisando a variância de eruptions e waiting\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax = sns.histplot(x='eruptions', data=faithful)\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax = sns.histplot(x='waiting', data=faithful)"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#exemplo-eda---vulcão-covariância",
    "href": "2-eda/1-intro-eda.html#exemplo-eda---vulcão-covariância",
    "title": "Análise de dados",
    "section": "Exemplo EDA - vulcão: covariância",
    "text": "Exemplo EDA - vulcão: covariância\n\n\n\nfig, ax = plt.subplots(figsize=(7, 6))\nax = sns.scatterplot(x='eruptions', y='waiting', data=faithful)\n\n\n\n\n\n\nTempos de espera mais longos após erupções mais longas\nVariância gera incerteza\nCovariância reduz incerteza\nUma variável pode ajudar a prever a outra"
  },
  {
    "objectID": "2-eda/1-intro-eda.html#referências",
    "href": "2-eda/1-intro-eda.html#referências",
    "title": "Análise de dados",
    "section": "Referências",
    "text": "Referências\n\nBruce, P. et al. Practical Statistics for Data Scientists. 2nd Edition. O’Reilly, 20220. Code and Datasets: https://github.com/gedeck/practical-statistics-for-data-scientists\nWickhan, H. R for Data Science. O’Reilly, 2017. Open Access: http://r4ds.had.co.nz\nGrus, Joel. Data Science do Zero. Editora Alta Books, 2021. Disponível em: Minha Biblioteca da UFPB."
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#probabilidade-vs.-estatística",
    "href": "2-eda/2-estatistica-descritiva.html#probabilidade-vs.-estatística",
    "title": "Análise de dados",
    "section": "Probabilidade vs. Estatística",
    "text": "Probabilidade vs. Estatística\n\nA probabilidade busca prever a chance de eventos futuros ocorrerem\n\nA estatística analisa a frequência de eventos passados\n\n\n\nA probabilidade é um ramo da matemática teórica sobre consequências de definições\nA estatística é matemática aplicada na busca de entender observações do mundo real"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#distribuições-de-variáveis-aleatórias",
    "href": "2-eda/2-estatistica-descritiva.html#distribuições-de-variáveis-aleatórias",
    "title": "Análise de dados",
    "section": "Distribuições de variáveis aleatórias",
    "text": "Distribuições de variáveis aleatórias\n\nVariáveis Aleatórias (VAs) são funções numéricas onde valores possuem probabilidades\nA função densidade de probabilidade (FDP) mostra VAs (como histogramas)"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#distribuições-de-variáveis-aleatórias-1",
    "href": "2-eda/2-estatistica-descritiva.html#distribuições-de-variáveis-aleatórias-1",
    "title": "Análise de dados",
    "section": "Distribuições de variáveis aleatórias",
    "text": "Distribuições de variáveis aleatórias\n\nFunção Densidade Acumulada (FDA) é o somatório da FDP\n\nFDA é a integral da FDP, enquanto a FDP é a derivada da FDA\n\n\n\\[C(X \\leq k) = \\sum_{x \\leq k} P(X = x)\\]"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#visualizando-distribuições-acumuladas",
    "href": "2-eda/2-estatistica-descritiva.html#visualizando-distribuições-acumuladas",
    "title": "Análise de dados",
    "section": "Visualizando distribuições acumuladas",
    "text": "Visualizando distribuições acumuladas\n\nAs vendas de iPhone estão bombando?"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#o-crescimento-é-mesmo-significativo",
    "href": "2-eda/2-estatistica-descritiva.html#o-crescimento-é-mesmo-significativo",
    "title": "Análise de dados",
    "section": "O crescimento é mesmo significativo?",
    "text": "O crescimento é mesmo significativo?\n\nFDAs podem dar uma visão errônea do crescimento\n\nA mudança incremental é a derivada da FDA, que é difícil de visualizar"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#estatística-descritiva-vs.-inferencial",
    "href": "2-eda/2-estatistica-descritiva.html#estatística-descritiva-vs.-inferencial",
    "title": "Análise de dados",
    "section": "Estatística descritiva vs. inferencial",
    "text": "Estatística descritiva vs. inferencial\n\nEstatística descritiva: captura propriedades e distribuição dos dados\n\nMedidas de tendência central descrevem o centro de sua distribuição\nMedidas de variabilidade ou dispersão descrevem o seu espalhamento\n\n\n\n\nEstatística inferencial: tomar decisões e achar relações nos dados\n\nUsa fundamentos e teorias da probabilidade\nPreocupa-se em modelar fenômenos aleatórios\nExemplo: as diferenças nos dados entre duas situações podem ser atribuídas a diferenças reais ou ao acaso?"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#estatística-descritiva",
    "href": "2-eda/2-estatistica-descritiva.html#estatística-descritiva",
    "title": "Análise de dados",
    "section": "Estatística descritiva",
    "text": "Estatística descritiva\nObjetivos:\n\nDar uma noção de como os dados estão distribuídos\nEntender melhor a natureza dos dados\nIdentificar pontos anormais (possíveis outliers)\nApresentação gráfica de aspectos importantes dos dados\nIdentificar relações entre variáveis"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#medidas-de-centralidade-com-número-único",
    "href": "2-eda/2-estatistica-descritiva.html#medidas-de-centralidade-com-número-único",
    "title": "Análise de dados",
    "section": "Medidas de centralidade com número único",
    "text": "Medidas de centralidade com número único\n\nUm número que seja representativo da maior parte dos dados\nA princípio pode parecer trivial: é só calcular a média\n\nEm alguns casos ela pode não ser a melhor medida de valor central\n\n\n\n\nÍndices de tendência central (average) mais populares:\n\nMédia amostral\nMediana amostral\nModa amostral"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-média-aritmética",
    "href": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-média-aritmética",
    "title": "Análise de dados",
    "section": "Medida de centralidade: média aritmética",
    "text": "Medida de centralidade: média aritmética\n\nSoma dividida pelo tamanho da amostra: \\(\\mu_x = \\frac{\\sum_{i = 1}^{n} x_i}{n}\\)\nChamada de primeiro momento da distribuição ou valor esperado\nA média é significativa para distribuições simétricas\n\nMas é sensível a outliers, principalmente se a amostra é pequena"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-variações-da-média",
    "href": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-variações-da-média",
    "title": "Análise de dados",
    "section": "Medida de centralidade: variações da média",
    "text": "Medida de centralidade: variações da média\n\nA média truncada (trimmed mean) remove os \\(p\\) menores e os \\(p\\) maiores valores antes de calcular a média\n\nAjuda a tirar o viés de valores extremos\n\n\n\\[\\text{Trimmed mean} = \\frac{\\sum^{n-p}_{i=p+1} x_i}{n − 2p}\\]"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-variações-da-média-1",
    "href": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-variações-da-média-1",
    "title": "Análise de dados",
    "section": "Medida de centralidade: variações da média",
    "text": "Medida de centralidade: variações da média\n\nA média ponderada (weighted mean) considera um peso \\(w\\) para cada valor\n\nExemplo: para calcular incidência média de covid-19 nos estados do Brasil, devem considerar a população de cada estado como peso\n\n\n\\[\\text{Weighted mean} = \\frac{\\sum^{n}_{i=1} w_i x_i}{\\sum{w_i}}\\]"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-mediana",
    "href": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-mediana",
    "title": "Análise de dados",
    "section": "Medida de centralidade: mediana",
    "text": "Medida de centralidade: mediana\n\nValor do meio dos dados ordenados (2o quartil ou 50-percentil)\n\nEx: [1, 4, 4, 6, 7, 9, 14] # Mediana: \\(6\\)\n\nSe o tamanho for par, a mediana é a média dos dois valores do meio\n\nEx: [2, 5, 8, 10, 15, 16] # Mediana: \\(\\frac{8+10}{2} = 9\\)\nÉ mais resistente a outliers, mas descarta muita informação"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-moda",
    "href": "2-eda/2-estatistica-descritiva.html#medida-de-centralidade-moda",
    "title": "Análise de dados",
    "section": "Medida de centralidade: moda 😎",
    "text": "Medida de centralidade: moda 😎\n\nÉ o valor que detém o maior número de observações\n\nOu seja, o valor ou valores mais frequentes\nNo gráfico de barras ou histograma, ele é a maior barra\nTambém é resistente a outliers, mas descarta muita informação"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#efeito-de-outliers-na-média",
    "href": "2-eda/2-estatistica-descritiva.html#efeito-de-outliers-na-média",
    "title": "Análise de dados",
    "section": "Efeito de outliers na média",
    "text": "Efeito de outliers na média\n\nimport pandas as pd\n\nx = pd.Series([1.2, 1.3, 1.5, 1.4, 1.5, 1.7, 1.4, 1.5, 1.6, 1.5, 1.4, 1.5, 1.5,\n               1.9, 1.7, 1.6, 1.8, 1.9, 1.7, 1.6, 1.8, 1.9, 1.7, 1.6, 1.7, 1.8,\n               1.6, 1.5, 1.4, 1.8, 1.6, 1.5, 1.6, 1.7, 1.5, 1.3, 1.4, 1.5, 1.5, 120])\nprint(f\"Média: {x.mean()} / Mediana: {x.median()} / Moda: {float(x.mode())}\")\n\nMédia: 4.54 / Mediana: 1.6 / Moda: 1.5\n\n\n\nx_f = x[x < 5]\nprint(f\"Média: {x_f.mean()} / Mediana: {x_f.median()} / Moda: {float(x_f.mode())}\")\n\nMédia: 1.5794871794871794 / Mediana: 1.6 / Moda: 1.5"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#qual-métrica-usar",
    "href": "2-eda/2-estatistica-descritiva.html#qual-métrica-usar",
    "title": "Análise de dados",
    "section": "Qual métrica usar?",
    "text": "Qual métrica usar?\n\n\n\n\n\nTipo de variável / escala\nÍndice de tendência central\n\n\n\n\nCategórica nominal\nModa\n\n\nCategórica ordinal\nMediana\n\n\nContínua Simétrica e sem outliers\nMédia\n\n\nContínua Assimétrica ou com outliers\nMediana\n\n\n\n\n\n\n\nBill Gates adiciona $250 à renda média, mas não muda mediana\nNo geral: se histograma é enviesado, usar mediana. Caso contrário, usar a média"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#exemplo-medidas-de-centralidade",
    "href": "2-eda/2-estatistica-descritiva.html#exemplo-medidas-de-centralidade",
    "title": "Análise de dados",
    "section": "Exemplo: medidas de centralidade",
    "text": "Exemplo: medidas de centralidade\n\n\n\nDados de população e taxa de homicídios nos EUA:\n\n\n\n             State  Population  Murder.Rate Abbreviation\n0          Alabama     4779736          5.7           AL\n1           Alaska      710231          5.6           AK\n2          Arizona     6392017          4.7           AZ\n3         Arkansas     2915918          5.6           AR\n4       California    37253956          4.4           CA\n5         Colorado     5029196          2.8           CO\n6      Connecticut     3574097          2.4           CT\n7         Delaware      897934          5.8           DE\n8          Florida    18801310          5.8           FL\n9          Georgia     9687653          5.7           GA\n10          Hawaii     1360301          1.8           HI\n11           Idaho     1567582          2.0           ID\n12        Illinois    12830632          5.3           IL\n13         Indiana     6483802          5.0           IN\n14            Iowa     3046355          1.9           IA\n15          Kansas     2853118          3.1           KS\n16        Kentucky     4339367          3.6           KY\n17       Louisiana     4533372         10.3           LA\n18           Maine     1328361          1.6           ME\n19        Maryland     5773552          6.1           MD\n20   Massachusetts     6547629          2.0           MA\n21        Michigan     9883640          5.4           MI\n22       Minnesota     5303925          1.6           MN\n23     Mississippi     2967297          8.6           MS\n24        Missouri     5988927          6.6           MO\n25         Montana      989415          3.6           MT\n26        Nebraska     1826341          2.9           NE\n27          Nevada     2700551          6.0           NV\n28   New Hampshire     1316470          0.9           NH\n29      New Jersey     8791894          3.9           NJ\n30      New Mexico     2059179          4.8           NM\n31        New York    19378102          3.1           NY\n32  North Carolina     9535483          5.1           NC\n33    North Dakota      672591          3.0           ND\n34            Ohio    11536504          4.0           OH\n35        Oklahoma     3751351          4.5           OK\n36          Oregon     3831074          2.0           OR\n37    Pennsylvania    12702379          4.8           PA\n38    Rhode Island     1052567          2.4           RI\n39  South Carolina     4625364          6.4           SC\n40    South Dakota      814180          2.3           SD\n41       Tennessee     6346105          5.7           TN\n42           Texas    25145561          4.4           TX\n43            Utah     2763885          2.3           UT\n44         Vermont      625741          1.6           VT\n45        Virginia     8001024          4.1           VA\n46      Washington     6724540          2.5           WA\n47   West Virginia     1852994          4.0           WV\n48       Wisconsin     5686986          2.9           WI\n49         Wyoming      563626          2.7           WY\n\n\n\n\nMédia da População\n\n\nstate['Population'].mean()\n\n6162876.3\n\n\n\nMédia truncada (10%)\n\n\nfrom scipy.stats import trim_mean\ntrim_mean(state['Population'], 0.1)\n\n4783697.125\n\n\n\nMediana\n\n\nstate['Population'].median()\n\n4436369.5"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#exemplo-medidas-de-centralidade-1",
    "href": "2-eda/2-estatistica-descritiva.html#exemplo-medidas-de-centralidade-1",
    "title": "Análise de dados",
    "section": "Exemplo: medidas de centralidade",
    "text": "Exemplo: medidas de centralidade\n\n\n\nDados de população e taxa de homicídios nos EUA\n\n\n\n             State  Population  Murder.Rate Abbreviation\n0          Alabama     4779736          5.7           AL\n1           Alaska      710231          5.6           AK\n2          Arizona     6392017          4.7           AZ\n3         Arkansas     2915918          5.6           AR\n4       California    37253956          4.4           CA\n5         Colorado     5029196          2.8           CO\n6      Connecticut     3574097          2.4           CT\n7         Delaware      897934          5.8           DE\n8          Florida    18801310          5.8           FL\n9          Georgia     9687653          5.7           GA\n10          Hawaii     1360301          1.8           HI\n11           Idaho     1567582          2.0           ID\n12        Illinois    12830632          5.3           IL\n13         Indiana     6483802          5.0           IN\n14            Iowa     3046355          1.9           IA\n15          Kansas     2853118          3.1           KS\n16        Kentucky     4339367          3.6           KY\n17       Louisiana     4533372         10.3           LA\n18           Maine     1328361          1.6           ME\n19        Maryland     5773552          6.1           MD\n20   Massachusetts     6547629          2.0           MA\n21        Michigan     9883640          5.4           MI\n22       Minnesota     5303925          1.6           MN\n23     Mississippi     2967297          8.6           MS\n24        Missouri     5988927          6.6           MO\n25         Montana      989415          3.6           MT\n26        Nebraska     1826341          2.9           NE\n27          Nevada     2700551          6.0           NV\n28   New Hampshire     1316470          0.9           NH\n29      New Jersey     8791894          3.9           NJ\n30      New Mexico     2059179          4.8           NM\n31        New York    19378102          3.1           NY\n32  North Carolina     9535483          5.1           NC\n33    North Dakota      672591          3.0           ND\n34            Ohio    11536504          4.0           OH\n35        Oklahoma     3751351          4.5           OK\n36          Oregon     3831074          2.0           OR\n37    Pennsylvania    12702379          4.8           PA\n38    Rhode Island     1052567          2.4           RI\n39  South Carolina     4625364          6.4           SC\n40    South Dakota      814180          2.3           SD\n41       Tennessee     6346105          5.7           TN\n42           Texas    25145561          4.4           TX\n43            Utah     2763885          2.3           UT\n44         Vermont      625741          1.6           VT\n45        Virginia     8001024          4.1           VA\n46      Washington     6724540          2.5           WA\n47   West Virginia     1852994          4.0           WV\n48       Wisconsin     5686986          2.9           WI\n49         Wyoming      563626          2.7           WY\n\n\n\n\nMédia da Taxa de homicídios\n\n\nstate['Murder.Rate'].mean()\n\n4.066\n\n\n\nProblema: na média aritmética os estados têm mesmo peso\n\nSolução: média ponderada pela população\n\nMédia ponderada\n\n\nfrom numpy import average\naverage(state['Murder.Rate'], weights=state['Population'])\n\n4.445833981123393"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#sumariando-variabilidade",
    "href": "2-eda/2-estatistica-descritiva.html#sumariando-variabilidade",
    "title": "Análise de dados",
    "section": "Sumariando variabilidade",
    "text": "Sumariando variabilidade\n“Então há um homem que morreu afogado atravessando um riacho com uma profundidade média de seis polegadas”\n\n\nA tendência central não é suficiente\nPodemos adicionar a ela um índice de dispersão:\n\nRange / Intervalo (ex: valores mínimo e máximo)\nVariância / Desvio padrão / Coeficiente de variação\nPercentis extremos (ex: 10-percentil e 90-percentil)\nIntervalo semi-quartis (SIQR)"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#sumariando-a-variabilidade",
    "href": "2-eda/2-estatistica-descritiva.html#sumariando-a-variabilidade",
    "title": "Análise de dados",
    "section": "Sumariando a variabilidade",
    "text": "Sumariando a variabilidade\n\nHistograma do desempenho de dois sistemas:\n\n\n\n\n\nA média é a mesma. Mas qual você prefere?"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#range",
    "href": "2-eda/2-estatistica-descritiva.html#range",
    "title": "Análise de dados",
    "section": "Range",
    "text": "Range\nIntervalo: [min, max]\n\nÚtil apenas quando os valores de uma variavél são limitados (bounded)\n\nDá para ter noção desses limites com o range\n\nVaria de acordo com o número de observações\n\nNão dá para saber se o range é significativo ou não"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#variância-e-desvio-padrão",
    "href": "2-eda/2-estatistica-descritiva.html#variância-e-desvio-padrão",
    "title": "Análise de dados",
    "section": "Variância e desvio padrão",
    "text": "Variância e desvio padrão\nVariância amostral (unidade dos dados ao quadrado):\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n - 1}\\]\nDesvio padrão amostral (mesma unidade dos dados):\n\\[s = \\sqrt{s^2}\\]\nCoeficiente de variação (independente da média e da unidade):\n\\[COV = \\bar{C} = \\frac{s}{\\bar{x}}\\]"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#quantis",
    "href": "2-eda/2-estatistica-descritiva.html#quantis",
    "title": "Análise de dados",
    "section": "Quantis",
    "text": "Quantis\n\n0,05-quantil; 0,95-quantil: similar ao range\nDecis \\((\\frac{1}{10})\\), Quartis \\((\\frac{1}{4})\\)\nIntervalos entre quartis\n\nInter-Quartile Range: \\[\\mathit{IQR} = Q3 - Q1\\]\nSemi IQR: \\[\\mathit{SIQR} = \\frac{Q3 - Q1}{2}\\]"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#top-frequent",
    "href": "2-eda/2-estatistica-descritiva.html#top-frequent",
    "title": "Análise de dados",
    "section": "Top frequent",
    "text": "Top frequent\n\nPara dados categóricos, a dispersão pode ser a porcentagem de observações das categorias mais frequentes\nExemplo: top 90%, top 45%\n\nQual está mais espalhado?"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#como-selecionar-um-índice-de-dispersão",
    "href": "2-eda/2-estatistica-descritiva.html#como-selecionar-um-índice-de-dispersão",
    "title": "Análise de dados",
    "section": "Como selecionar um índice de dispersão?",
    "text": "Como selecionar um índice de dispersão?\n\n\n\n\n\nTipo de variável\nÍndice de dispersão\n\n\n\n\nCategórica\nTop frequent\n\n\nContínua Simétrica\nVariância, desvio padrão ou COV\n\n\nContínua Assimétrica\nQuantis, IQR ou SIQR\n\n\n\n\n\n\n\nIntervalo (range) pode ser usado para complementar a análise de dispersão"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#exemplos---índices-de-dispersão",
    "href": "2-eda/2-estatistica-descritiva.html#exemplos---índices-de-dispersão",
    "title": "Análise de dados",
    "section": "Exemplos - Índices de dispersão",
    "text": "Exemplos - Índices de dispersão"
  },
  {
    "objectID": "2-eda/2-estatistica-descritiva.html#referências",
    "href": "2-eda/2-estatistica-descritiva.html#referências",
    "title": "Análise de dados",
    "section": "Referências",
    "text": "Referências\n\nSlides baseados no material de:\n\nProf. Raquel Lopes (UFCG)\nProf. Steven Skiena (Stony Brok University)\n\nOutras referências:\n\nProbability and Statistics for Engineering and the Sciences. Jay Devore\nThe Art of Computer Systems Performance Analysis. Raj Jain"
  },
  {
    "objectID": "2-eda/3-correlacao.html#relação-entre-variáveis",
    "href": "2-eda/3-correlacao.html#relação-entre-variáveis",
    "title": "Análise de dados",
    "section": "Relação entre variáveis",
    "text": "Relação entre variáveis\n\nEm análise exploratória, é útil examinar se há associação entre duas variáveis numéricas\nCaracterísticas importantes para essa associação:\n\nFormato: linear, exponencial, parabólica, linear e depois assintótica, outro formato arbitrário, etc.\nForça: correlação forte, fraca, nenhuma\nSinal: correlação positiva ou negativa, quando é perceptível\nPontos extremos fora da associação\n\nJá vimos como visualizar a covariância com gráficos de dispersão\nOutra técnica para relacionar variáveis é a análise de correlação"
  },
  {
    "objectID": "2-eda/3-correlacao.html#correlação",
    "href": "2-eda/3-correlacao.html#correlação",
    "title": "Análise de dados",
    "section": "Correlação",
    "text": "Correlação\n\nMedida que indica se a relação entre 2 variáveis \\(x\\) e \\(y\\) é estatisticamente significativa\nO coeficiente de correlação é uma estatística que mede em que grau \\(y\\) é uma função de \\(x\\) e vice versa\nO coeficiente de Pearson é o mais popular e usado principalmente para relações lineares\nOs coeficientes de Spearman e Kendall são baseados em ranking\n\nSão menos sensíveis a outliers e capturam algumas relações não lineares"
  },
  {
    "objectID": "2-eda/3-correlacao.html#coeficiente-de-correlação-de-pearson",
    "href": "2-eda/3-correlacao.html#coeficiente-de-correlação-de-pearson",
    "title": "Análise de dados",
    "section": "Coeficiente de Correlação de Pearson",
    "text": "Coeficiente de Correlação de Pearson\n\nIndica a força e a direção da correlação linear entre duas variáveis\nSeu coeficiente amostral para as variáveis x e y é definido por:\n\n\\[r_{xy} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\bar{x}}{s_x} \\right) \\left( \\frac{y_i - \\bar{y}}{s_y} \\right)\\]\n\nO valor do coeficiente de correlação \\(r_{xy}\\) varia de -1 a 1:\n\nSe \\(r_{xy}\\) é positivo: quando \\(x\\) aumenta, \\(y\\) aumenta linearmente\nSe \\(r_{xy}\\) é negativo: quando \\(x\\) aumenta, \\(y\\) diminui linearmente\nSe \\(r_{xy}\\) é aproximadamente 0: não há relação linear entre \\(x\\) e \\(y\\)"
  },
  {
    "objectID": "2-eda/3-correlacao.html#coeficiente-de-correlação-de-pearson-1",
    "href": "2-eda/3-correlacao.html#coeficiente-de-correlação-de-pearson-1",
    "title": "Análise de dados",
    "section": "Coeficiente de Correlação de Pearson",
    "text": "Coeficiente de Correlação de Pearson\n\nValores tipicamente usados para indicar a força da correlação:\n\nSe \\(r_{xy} \\gt 0.7\\) ou \\(r_{xy} \\lt -0.7\\): correlação forte\nSe \\(0.3 \\lt r_{xy} \\lt 0.7\\) ou \\(-0.7 \\lt r_{xy} \\lt - 0.3\\): correlação moderada\nSe \\(-0.3 \\lt r_{xy} \\lt 0.3\\): correlação fraca\nSe \\(r_{xy} \\approx 0\\): não há correlação (ou não se pode concluir nada)\n\nPorém, a avaliação da correlação vai depender de cada contexto\n\nNão há um número apenas que possa lhe responder tudo sobre a associação entre as duas variáveis\nA correlação servirá para complementar e quantificar observações feitas em gráficos de dispersão"
  },
  {
    "objectID": "2-eda/3-correlacao.html#exemplo-quarteto-de-anscombe",
    "href": "2-eda/3-correlacao.html#exemplo-quarteto-de-anscombe",
    "title": "Análise de dados",
    "section": "Exemplo: quarteto de Anscombe",
    "text": "Exemplo: quarteto de Anscombe\n\nQuatro datasets com pares de variáveis \\(x\\) e \\(y\\)\n\nNota-se estatísticas bem similares em cada dataset…\n\n\n\n\nanscombe.groupby(['dataset']).x.describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      dataset\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      I\n      11.0\n      9.0\n      3.316625\n      4.0\n      6.5\n      9.0\n      11.5\n      14.0\n    \n    \n      II\n      11.0\n      9.0\n      3.316625\n      4.0\n      6.5\n      9.0\n      11.5\n      14.0\n    \n    \n      III\n      11.0\n      9.0\n      3.316625\n      4.0\n      6.5\n      9.0\n      11.5\n      14.0\n    \n    \n      IV\n      11.0\n      9.0\n      3.316625\n      8.0\n      8.0\n      8.0\n      8.0\n      19.0\n    \n  \n\n\n\n\n\n\n\nanscombe.groupby(['dataset']).y.describe()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      dataset\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      I\n      11.0\n      7.500909\n      2.031568\n      4.26\n      6.315\n      7.58\n      8.57\n      10.84\n    \n    \n      II\n      11.0\n      7.500909\n      2.031657\n      3.10\n      6.695\n      8.14\n      8.95\n      9.26\n    \n    \n      III\n      11.0\n      7.500000\n      2.030424\n      5.39\n      6.250\n      7.11\n      7.98\n      12.74\n    \n    \n      IV\n      11.0\n      7.500909\n      2.030579\n      5.25\n      6.170\n      7.04\n      8.19\n      12.50"
  },
  {
    "objectID": "2-eda/3-correlacao.html#exemplo-quarteto-de-anscombe-1",
    "href": "2-eda/3-correlacao.html#exemplo-quarteto-de-anscombe-1",
    "title": "Análise de dados",
    "section": "Exemplo: quarteto de Anscombe",
    "text": "Exemplo: quarteto de Anscombe\n\nCalculando a correlação linear entre \\(x\\) e \\(y\\) em cada dataset:\n\n\n\nanscombe.groupby(['dataset']).corr()\n\n\n\n\n\n  \n    \n      \n      \n      x\n      y\n    \n    \n      dataset\n      \n      \n      \n    \n  \n  \n    \n      I\n      x\n      1.000000\n      0.816421\n    \n    \n      y\n      0.816421\n      1.000000\n    \n    \n      II\n      x\n      1.000000\n      0.816237\n    \n    \n      y\n      0.816237\n      1.000000\n    \n    \n      III\n      x\n      1.000000\n      0.816287\n    \n    \n      y\n      0.816287\n      1.000000\n    \n    \n      IV\n      x\n      1.000000\n      0.816521\n    \n    \n      y\n      0.816521\n      1.000000\n    \n  \n\n\n\n\n\n\nTodos os datasets são iguais? E se olharmos os dados?"
  },
  {
    "objectID": "2-eda/3-correlacao.html#exemplo-quarteto-de-anscombe-2",
    "href": "2-eda/3-correlacao.html#exemplo-quarteto-de-anscombe-2",
    "title": "Análise de dados",
    "section": "Exemplo: quarteto de Anscombe",
    "text": "Exemplo: quarteto de Anscombe\n\n\n\nax = sns.lmplot(data=anscombe, x=\"x\", y=\"y\", col=\"dataset\", height=3,\n                hue=\"dataset\", col_wrap=2, palette=\"muted\", ci=None)\n\n\n\n\n\n\nRelações diferentes, mesma quantificação\n\nNo 2o grupo não há uma relação linear\nNo 3o há relação perfeita entre maioria das observações, com uma exceção\nNo 4o não há relação; há uma exceção que faz parecer que há uma relação"
  },
  {
    "objectID": "2-eda/3-correlacao.html#outros-coeficientes-de-correlação",
    "href": "2-eda/3-correlacao.html#outros-coeficientes-de-correlação",
    "title": "Análise de dados",
    "section": "Outros coeficientes de correlação",
    "text": "Outros coeficientes de correlação\n\nCoeficientes de Spearman e Kendall são baseados em ranking\n\nMenos sensíveis a outliers e capturam algumas relações não lineares\n\n\n\npd.DataFrame({\n    'pearson': anscombe.groupby(['dataset']).corr(method='pearson').y.xs('x', level=1),\n    'spearman': anscombe.groupby(['dataset']).corr(method='spearman').y.xs('x', level=1),\n    'kendall': anscombe.groupby(['dataset']).corr(method='kendall').y.xs('x', level=1)\n})\n\n\n\n\n\n  \n    \n      \n      pearson\n      spearman\n      kendall\n    \n    \n      dataset\n      \n      \n      \n    \n  \n  \n    \n      I\n      0.816421\n      0.818182\n      0.636364\n    \n    \n      II\n      0.816237\n      0.690909\n      0.563636\n    \n    \n      III\n      0.816287\n      0.990909\n      0.963636\n    \n    \n      IV\n      0.816521\n      0.500000\n      0.426401"
  },
  {
    "objectID": "2-eda/3-correlacao.html#força-e-direção-da-correlação",
    "href": "2-eda/3-correlacao.html#força-e-direção-da-correlação",
    "title": "Análise de dados",
    "section": "Força e direção da correlação",
    "text": "Força e direção da correlação\n\nExemplos de relação entre o valor esperado do coeficiente (linear) e vários tipos de associação entre duas variáveis:\n\n\nFonte: wikipedia"
  },
  {
    "objectID": "2-eda/3-correlacao.html#exemplo-dados-com-relação-linear",
    "href": "2-eda/3-correlacao.html#exemplo-dados-com-relação-linear",
    "title": "Análise de dados",
    "section": "Exemplo: dados com relação linear",
    "text": "Exemplo: dados com relação linear\n\n\n\nrng = np.random.default_rng(12345)\nx = 100 * rng.normal(size=100)\nruido = 40 * rng.normal(size=100)\ny = 0.5 * x + ruido + 20\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.lmplot(data=df, x='x', y='y', height=4, aspect=1.3)\nprint(f\"Correlação: {df.x.corr(df.y)}\")\n\nCorrelação: 0.7460018986073979\n\n\n\n\n\n\n\n# Com o dobro do erro / ruído\n# Dados mais espalhados, correlação diminui\nruido2 = 80 * rng.normal(size=100)\ny = 0.5 * x + ruido2 + 20\ndf = pd.DataFrame({'x': x, 'y': y})\nsns.lmplot(x='x', y='y', data=df, height=4, aspect=1.3)\nprint(f\"Correlação: {df.x.corr(df.y)}\")\n\nCorrelação: 0.4340703264617881"
  },
  {
    "objectID": "2-eda/3-correlacao.html#exemplo-dados-com-relação-não-linear",
    "href": "2-eda/3-correlacao.html#exemplo-dados-com-relação-não-linear",
    "title": "Análise de dados",
    "section": "Exemplo: dados com relação não linear",
    "text": "Exemplo: dados com relação não linear\n\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nx = rng.uniform(low=1, high=20, size=100)\ny = 100 * np.exp(-1.2 * x)\ndf = pd.DataFrame({'x': x, 'y': y})\nax = sns.scatterplot(data=df, x='x', y='y', linewidth=0, ax=ax)\n\n\n\n\n\n\npd.Series({\n    'pearson': df.x.corr(df.y, method='pearson'),\n    'spearman': df.x.corr(df.y, method='spearman'),\n    'kendall': df.x.corr(df.y, method='kendall')\n}, name='correlacao').to_frame()\n\n\n\n\n\n  \n    \n      \n      correlacao\n    \n  \n  \n    \n      pearson\n      -0.466498\n    \n    \n      spearman\n      -1.000000\n    \n    \n      kendall\n      -1.000000"
  },
  {
    "objectID": "2-eda/3-correlacao.html#exemplo-dados-com-relação-não-linear-1",
    "href": "2-eda/3-correlacao.html#exemplo-dados-com-relação-não-linear-1",
    "title": "Análise de dados",
    "section": "Exemplo: dados com relação não linear",
    "text": "Exemplo: dados com relação não linear\n\nVisualizando os mesmos dados em escala logarítmica\n\n\n\n\n# Escala normal\nfig, ax = plt.subplots(figsize=(6, 5))\nax = sns.scatterplot(data=df, x='x', y='y', linewidth=0, ax=ax)\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 5))\nax = sns.scatterplot(data=df, x='x', y='y', linewidth=0, ax=ax)\nax.set_yscale('log')"
  },
  {
    "objectID": "2-eda/3-correlacao.html#correlação-vs.-causalidade",
    "href": "2-eda/3-correlacao.html#correlação-vs.-causalidade",
    "title": "Análise de dados",
    "section": "Correlação vs. Causalidade",
    "text": "Correlação vs. Causalidade\n\nATENÇÃO! Correlação não implica causalidade!\n\nEx: a quantidade de remédios consumidos tem forte correlação com a quantidade de pessoas doentes; mas o remédio não causa a doença\n\n\n\nFonte: XKCD\nLinks: correlações espúrias, wikipedia e aula na Khan Academy"
  },
  {
    "objectID": "2-eda/3-correlacao.html#cuidado-com-sumários-datasaurus",
    "href": "2-eda/3-correlacao.html#cuidado-com-sumários-datasaurus",
    "title": "Análise de dados",
    "section": "Cuidado com sumários: datasaurus",
    "text": "Cuidado com sumários: datasaurus\n\n\n\n# Correlação\ndatasaurus = pd.read_csv(DATASAURUS_CSV)\ndatasaurus.groupby(['dataset']).corr().y.xs('x', level=1).to_frame()\n\n\n\n\n\n  \n    \n      \n      y\n    \n    \n      dataset\n      \n    \n  \n  \n    \n      away\n      -0.064128\n    \n    \n      bullseye\n      -0.068586\n    \n    \n      circle\n      -0.068343\n    \n    \n      dino\n      -0.064472\n    \n    \n      dots\n      -0.060341\n    \n    \n      h_lines\n      -0.061715\n    \n    \n      high_lines\n      -0.068504\n    \n    \n      slant_down\n      -0.068980\n    \n    \n      slant_up\n      -0.068609\n    \n    \n      star\n      -0.062961\n    \n    \n      v_lines\n      -0.069446\n    \n    \n      wide_lines\n      -0.066575\n    \n    \n      x_shape\n      -0.065583\n    \n  \n\n\n\n\n\n\n# Média e desvio padrão\ndatasaurus.groupby(['dataset']).agg(['mean', 'std'])\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n    \n      \n      mean\n      std\n      mean\n      std\n    \n    \n      dataset\n      \n      \n      \n      \n    \n  \n  \n    \n      away\n      54.266100\n      16.769825\n      47.834721\n      26.939743\n    \n    \n      bullseye\n      54.268730\n      16.769239\n      47.830823\n      26.935727\n    \n    \n      circle\n      54.267320\n      16.760013\n      47.837717\n      26.930036\n    \n    \n      dino\n      54.263273\n      16.765142\n      47.832253\n      26.935403\n    \n    \n      dots\n      54.260303\n      16.767735\n      47.839829\n      26.930192\n    \n    \n      h_lines\n      54.261442\n      16.765898\n      47.830252\n      26.939876\n    \n    \n      high_lines\n      54.268805\n      16.766704\n      47.835450\n      26.939998\n    \n    \n      slant_down\n      54.267849\n      16.766759\n      47.835896\n      26.936105\n    \n    \n      slant_up\n      54.265882\n      16.768853\n      47.831496\n      26.938608\n    \n    \n      star\n      54.267341\n      16.768959\n      47.839545\n      26.930275\n    \n    \n      v_lines\n      54.269927\n      16.769959\n      47.836988\n      26.937684\n    \n    \n      wide_lines\n      54.266916\n      16.770000\n      47.831602\n      26.937902\n    \n    \n      x_shape\n      54.260150\n      16.769958\n      47.839717\n      26.930002"
  },
  {
    "objectID": "2-eda/3-correlacao.html#cuidado-com-sumários-datasaurus-1",
    "href": "2-eda/3-correlacao.html#cuidado-com-sumários-datasaurus-1",
    "title": "Análise de dados",
    "section": "Cuidado com sumários: datasaurus",
    "text": "Cuidado com sumários: datasaurus\n\nMuito parecidos?\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax = sns.boxplot(x='x', y='dataset', data=datasaurus)\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax = sns.boxplot(x='y', y='dataset', data=datasaurus)"
  },
  {
    "objectID": "2-eda/3-correlacao.html#cuidado-com-sumários-datasaurus-2",
    "href": "2-eda/3-correlacao.html#cuidado-com-sumários-datasaurus-2",
    "title": "Análise de dados",
    "section": "Cuidado com sumários: datasaurus",
    "text": "Cuidado com sumários: datasaurus\n\ng = sns.FacetGrid(datasaurus, col=\"dataset\", col_wrap=5, height=2, hue=\"dataset\")\nax = g.map_dataframe(sns.scatterplot, x=\"x\", y=\"y\", linewidth=0)"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#regressão-linear",
    "href": "3-regressao/1-intro-regressao.html#regressão-linear",
    "title": "Análise de dados",
    "section": "Regressão linear",
    "text": "Regressão linear\n\nCom frequência queremos saber se uma variável \\(x\\) está associada a uma variável \\(y\\)\n\nSe sim, qual a relação e como podemos usar isso para prever \\(y\\)?\n\nA regressão linear simples oferece um modelo do relacionamento entre a magnitude de duas variáveis\n\nExemplo: quando \\(x\\) cresce, \\(y\\) também cresce\n\nJá usamos a correlação para avaliar a relação entre variáveis\n\nA correlação mede a força da associação entre duas variáveis\nA regressão quantifica a natureza desta relação"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#regressão-linear-simples",
    "href": "3-regressao/1-intro-regressao.html#regressão-linear-simples",
    "title": "Análise de dados",
    "section": "Regressão linear simples",
    "text": "Regressão linear simples\n\nQuanto a variável \\(y\\) mudará quando \\(x\\) muda uma certa quantia?\nTenta estimar \\(y\\) com base em \\(x\\) usando a equação da reta:\n\n\\[y = b_0 + b_1 x\\]\n\n\\(b_0\\) é o valor constante da reta, onde ela intercepta o eixo \\(y\\)\n\\(b_1\\) é valor que representa a inclinação da reta\n\nTambém é o coeficiente da variável \\(x\\)\n\n\\(y\\) é chamada de variável dependente ou resposta\n\\(x\\) é chamada de variável independente ou preditora"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#exemplo-respiração-vs-exposição-à-poeira",
    "href": "3-regressao/1-intro-regressao.html#exemplo-respiração-vs-exposição-à-poeira",
    "title": "Análise de dados",
    "section": "Exemplo: respiração vs exposição à poeira",
    "text": "Exemplo: respiração vs exposição à poeira\n\nEncontrar a “melhor” linha para estimar a capacidade do pulmão (PEFR) em função da exposição à poeira de algodão (Exposure):\n\nEquação da reta: \\(PEFR = b_0 + b_1 Exposure\\)\n\n\n\n\n\n\n\n\n\n\n\nRegressão com scikit-learn:\n\n\npredictors = ['Exposure']\noutcome = 'PEFR'\nmodel = LinearRegression()\nmodel.fit(lung[predictors], lung[outcome])\nprint(f'b0 (intercept): {model.intercept_:.2f}')\nprint(f'b1 (coefficient): {model.coef_[0]:.2f}')\n\nb0 (intercept): 424.58\nb1 (coefficient): -4.18"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#exemplo-respiração-vs-exposição-à-poeira-1",
    "href": "3-regressao/1-intro-regressao.html#exemplo-respiração-vs-exposição-à-poeira-1",
    "title": "Análise de dados",
    "section": "Exemplo: respiração vs exposição à poeira",
    "text": "Exemplo: respiração vs exposição à poeira\n\nEquação da reta: \\(PEFR = 424.58 - 4.18 Exposure\\)\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(4, 4))\nax.set_xlim(0, 23)\nax.set_ylim(295, 450)\nax.set_xlabel('Exposure')\nax.set_ylabel('PEFR')\nax.plot((0, 23), model.predict(pd.DataFrame({'Exposure': [0, 23]})))\nax.text(0.4, model.intercept_, r'$b_0$', size='larger')\n\nx = pd.DataFrame({'Exposure': [7.5,17.5]})\ny = model.predict(x)\nax.plot((7.5, 7.5, 17.5), (y[0], y[1], y[1]), '--')\nax.text(5, np.mean(y), r'$\\Delta Y$', size='larger')\nax.text(12, y[1] - 10, r'$\\Delta X$', size='larger')\nax.text(12, 390, r'$b_1 = \\frac{\\Delta Y}{\\Delta X}$', size='larger')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nO intercept \\(b_0 = 424.58\\) é a estimativa de PEFR para Exposure = 0\nO coeficiente \\(b_1 = –4.18\\) representa a redução do PEFR à cada ano adicional de exposição à poeira (Exposure)"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#regressão-linear-predições-e-resíduos",
    "href": "3-regressao/1-intro-regressao.html#regressão-linear-predições-e-resíduos",
    "title": "Análise de dados",
    "section": "Regressão linear: predições e resíduos",
    "text": "Regressão linear: predições e resíduos\n\n\n\nOs dados nem sempre seguem perfeitamente uma reta\nPara cada valor original \\(y_i\\) teremos um erro explícito \\(e_i\\):\n\n\\(y_i = b_0 + b_1 x_i + e_i\\)\n\nO erro residual é a diferença entre o valor original e o estimado (\\(\\hat{Y}_i\\)):\n\n\\(\\hat{e}_i = (y_i) - (\\hat{y}_i)\\)\n\n\n\n\n\nCode\nfitted = model.predict(lung[predictors])\nresiduals = lung[outcome] - fitted\n\nax = lung.plot.scatter(x='Exposure', y='PEFR', figsize=(5, 4))\nax.plot(lung.Exposure, fitted)\nfor x, yactual, yfitted in zip(lung.Exposure, lung.PEFR, fitted): \n    ax.plot((x, x), (yactual, yfitted), '--', color='C1')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs linhas tracejadas representam o erro residual"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#regressão-ajuste-do-modelo-aos-dados",
    "href": "3-regressao/1-intro-regressao.html#regressão-ajuste-do-modelo-aos-dados",
    "title": "Análise de dados",
    "section": "Regressão: ajuste do modelo aos dados",
    "text": "Regressão: ajuste do modelo aos dados\n\nAfinal, como a equação da reta é determinada?\n\nVocê pode se imaginar fazendo na mão para casos mais fáceis\n\nNa prática, busca-se valores de \\(\\hat{b}_0\\) e \\(\\hat{b}_1\\) da reta que minimize a soma dos erros ao quadrado (sum of squares error):\n\n\\[SSE = \\sum^{n}_{i = 1} \\left( y_i - \\hat{y}_i \\right)^2 = \\sum^{n}_{i = 1} \\left( y_i - \\hat{b}_0 - \\hat{b}_1 X_1 \\right)^2\\]\n\nMétodo chamado de mínimos quadrados ou ordinary least squares (OLS), que é rápido de computar em softwares estatísticos"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#avaliando-a-qualidade-do-modelo",
    "href": "3-regressao/1-intro-regressao.html#avaliando-a-qualidade-do-modelo",
    "title": "Análise de dados",
    "section": "Avaliando a qualidade do modelo",
    "text": "Avaliando a qualidade do modelo\n\nJá temos um modelo: \\(y = b_0 + b_1 x\\)\nQuão bom é o? Quanto da variação de \\(y\\) de fato está sendo explicada pela variação de \\(x\\)?\nQuanto da variação total de \\(y\\) está sendo devido a erros randômicos ou sistemáticos e não devido à variação de \\(x\\)?"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#avaliando-a-qualidade-do-modelo-1",
    "href": "3-regressao/1-intro-regressao.html#avaliando-a-qualidade-do-modelo-1",
    "title": "Análise de dados",
    "section": "Avaliando a qualidade do modelo",
    "text": "Avaliando a qualidade do modelo\nAlém do \\(SSE\\), outras métricas são usadas para avaliar o modelo:\n\nVariação total de \\(y\\) (sum of squares total):\n\n\\[SST = \\sum^{n}_{i = 1} \\left( y_i - \\overline{y} \\right)^2 = \\sum^{n}_{i = 1} y_i^2 - n\\overline{y}^2\\]\n\nVariação de \\(y\\) explicada pela regressão (sum of squares regression):\n\n\\[SSR = \\sum^{n}_{i = 1} \\left( \\hat{y}_i - \\overline{y} \\right)^2\\]"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#avaliando-a-qualidade-do-modelo-2",
    "href": "3-regressao/1-intro-regressao.html#avaliando-a-qualidade-do-modelo-2",
    "title": "Análise de dados",
    "section": "Avaliando a qualidade do modelo",
    "text": "Avaliando a qualidade do modelo\n\nDividimos a variação total em duas partes:\n\nA parte não explicada pela regressão (\\(SSE\\))\nA parte explicada pela regressão (\\(SSR\\))\n\nA diferença entre \\(SST\\) e \\(SSE\\) é a soma dos quadrados explicada pela regressão:\n\n\\(SSR = SST - SSE \\Rightarrow SST = SSR + SSE\\)\n\nA fração de variação capturada pelo modelo indica a sua qualidade, chamado coeficiente de determinação:\n\n\\[R^2 = \\frac{SSR}{SST} = \\frac{SST - SSE}{SST}\\]"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#coeficiente-de-determinação-r2",
    "href": "3-regressao/1-intro-regressao.html#coeficiente-de-determinação-r2",
    "title": "Análise de dados",
    "section": "Coeficiente de determinação \\(R^2\\)",
    "text": "Coeficiente de determinação \\(R^2\\)\n\nUm valor mais altos de \\(R^2\\) significa uma melhor regressão:\n\n\\(R^2 = 1 \\implies\\) Regressão perfeita\n\\(R^2 = 0 \\implies\\) Regressão péssima\n\nPor que esse coeficiente se chama de \\(R^2\\)?\n\nPorque é igual ao quadrado da correlação amostral \\(R_{y\\hat{y}}\\) (coeficiente de Pearson) entre \\(y\\) e \\(y\\hat{y}\\)\n\nOu seja:\n\nCoeficiente de determinação \\(R^2\\) = Coeficiente de correlação \\(R_{y\\hat{y}}^2\\)"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#exemplo-avaliando-o-modelo",
    "href": "3-regressao/1-intro-regressao.html#exemplo-avaliando-o-modelo",
    "title": "Análise de dados",
    "section": "Exemplo: avaliando o modelo",
    "text": "Exemplo: avaliando o modelo\n\nAvaliando o modelo com o scikit-learn:\n\n\nRMSE = np.sqrt(mean_squared_error(lung[outcome], fitted))\nr2 = r2_score(lung[outcome], fitted)\nprint(f'Root Mean Square Error (RMSE): {RMSE:.0f}')\nprint(f'Coefficiente of determination (r2): {r2:.4f}')\n\nRoot Mean Square Error (RMSE): 101\nCoefficiente of determination (r2): 0.0767"
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#exemplo-avaliando-o-modelo-1",
    "href": "3-regressao/1-intro-regressao.html#exemplo-avaliando-o-modelo-1",
    "title": "Análise de dados",
    "section": "Exemplo: avaliando o modelo",
    "text": "Exemplo: avaliando o modelo\n\nAplicando a regressão e avaliando o modelo com o statsmodels:\n\n\nmodel = sm.OLS(lung[outcome], lung[predictors].assign(const=1))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   PEFR   R-squared:                       0.077\nModel:                            OLS   Adj. R-squared:                  0.069\nMethod:                 Least Squares   F-statistic:                     9.974\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):            0.00201\nTime:                        11:14:40   Log-Likelihood:                -735.68\nNo. Observations:                 122   AIC:                             1475.\nDf Residuals:                     120   BIC:                             1481.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nExposure      -4.1846      1.325     -3.158      0.002      -6.808      -1.561\nconst        424.5828     20.796     20.417      0.000     383.408     465.757\n==============================================================================\nOmnibus:                        0.767   Durbin-Watson:                   1.111\nProb(Omnibus):                  0.681   Jarque-Bera (JB):                0.891\nSkew:                          -0.162   Prob(JB):                        0.641\nKurtosis:                       2.734   Cond. No.                         35.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "3-regressao/1-intro-regressao.html#regressão-predição-vs-explicação-dos-dados",
    "href": "3-regressao/1-intro-regressao.html#regressão-predição-vs-explicação-dos-dados",
    "title": "Análise de dados",
    "section": "Regressão: predição vs explicação dos dados",
    "text": "Regressão: predição vs explicação dos dados\n\nHistoricamente a regressão era usada para entender e explicar a relação entre variáveis\n\nQuantifica a influência de variáveis em outras\nO foco não é prever casos específicos, mas compreender a relação geral\n\nCom o surgimento do big data e machine learning, a regressão também passou a ser muito usada para a predição de novos dados\nAtenção: um modelo de regressão também não prova relação causalidade entre as variáveis"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#regressão-linear-múltipla",
    "href": "3-regressao/2-regressao-multipla.html#regressão-linear-múltipla",
    "title": "Análise de dados",
    "section": "Regressão linear múltipla",
    "text": "Regressão linear múltipla\n\nEstima o valor de uma variável resposta \\(y\\) em função de \\(k\\) variáveis preditoras \\((x_1, ..., x_k)\\) com base em uma relação linear:\n\n\\[y = b_0 + b_1 x_1 + b_2 x_2 + ... + b_k x_k + e\\]\n\nOnde:\n\n\\((b_0, ..., b_k)\\) são os \\(k+1\\) parâmetros da regressão\n\\(e\\) representa o erro do modelo"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#exemplo-de-regressão-múltipla-preço-de-casas",
    "href": "3-regressao/2-regressao-multipla.html#exemplo-de-regressão-múltipla-preço-de-casas",
    "title": "Análise de dados",
    "section": "Exemplo de regressão múltipla: preço de casas",
    "text": "Exemplo de regressão múltipla: preço de casas\n\nO governo precisa estimar o preço de casas para cobrar impostos\nPodemos estimar o preço de uma casa (AdjSalePrice) com base em outras variáveis: área (SqFtTotLiving), banheiros (Bathrooms), quartos (Bedrooms), nível (BldgGrade)\n\n\n\n\n\n\n\n  \n    \n      \n      AdjSalePrice\n      SqFtTotLiving\n      SqFtLot\n      Bathrooms\n      Bedrooms\n      BldgGrade\n    \n  \n  \n    \n      1\n      300805.0\n      2400\n      9373\n      3.00\n      6\n      7\n    \n    \n      2\n      1076162.0\n      3764\n      20156\n      3.75\n      4\n      10\n    \n    \n      3\n      761805.0\n      2060\n      26036\n      1.75\n      4\n      8\n    \n    \n      4\n      442065.0\n      3200\n      8618\n      3.75\n      5\n      7\n    \n    \n      5\n      297065.0\n      1720\n      8620\n      1.75\n      4\n      7"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#exemplo-de-regressão-múltipla-preço-de-casas-1",
    "href": "3-regressao/2-regressao-multipla.html#exemplo-de-regressão-múltipla-preço-de-casas-1",
    "title": "Análise de dados",
    "section": "Exemplo de regressão múltipla: preço de casas",
    "text": "Exemplo de regressão múltipla: preço de casas\n\nAplicando e avaliando o modelo de regressão com scikit-learn\n\n\n\n\npredictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms',\n              'Bedrooms', 'BldgGrade']\noutcome = 'AdjSalePrice'\n\nhouse_lm = LinearRegression()\nhouse_lm.fit(house[predictors], house[outcome])\n\nprint(f'Intercept: {house_lm.intercept_:.3f}')\nprint('Coefficients:')\nfor name, coef in zip(predictors, house_lm.coef_):\n    print(f' {name}: {coef}')\n\nIntercept: -521871.368\nCoefficients:\n SqFtTotLiving: 228.83060360240796\n SqFtLot: -0.06046682065307607\n Bathrooms: -19442.840398321056\n Bedrooms: -47769.95518521438\n BldgGrade: 106106.96307898083\n\n\n\n\nfitted = house_lm.predict(house[predictors])\nRMSE = np.sqrt(mean_squared_error(house[outcome], fitted))\nr2 = r2_score(house[outcome], fitted)\nprint(f'Root Mean Square Error (RMSE): {RMSE:.0f}')\nprint(f'Coefficiente of determination (r2): {r2:.4f}')\n\nRoot Mean Square Error (RMSE): 261220\nCoefficiente of determination (r2): 0.5406"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#exemplo-de-regressão-múltipla-preço-de-casas-2",
    "href": "3-regressao/2-regressao-multipla.html#exemplo-de-regressão-múltipla-preço-de-casas-2",
    "title": "Análise de dados",
    "section": "Exemplo de regressão múltipla: preço de casas",
    "text": "Exemplo de regressão múltipla: preço de casas\n\nAvaliando o modelo de regressão linear com o statsmodels\n\n\nmodel = sm.OLS(house[outcome], house[predictors].assign(const=1))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           AdjSalePrice   R-squared:                       0.541\nModel:                            OLS   Adj. R-squared:                  0.540\nMethod:                 Least Squares   F-statistic:                     5338.\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):               0.00\nTime:                        11:14:47   Log-Likelihood:            -3.1517e+05\nNo. Observations:               22687   AIC:                         6.304e+05\nDf Residuals:                   22681   BIC:                         6.304e+05\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nSqFtTotLiving   228.8306      3.899     58.694      0.000     221.189     236.472\nSqFtLot          -0.0605      0.061     -0.988      0.323      -0.180       0.059\nBathrooms     -1.944e+04   3625.388     -5.363      0.000   -2.65e+04   -1.23e+04\nBedrooms      -4.777e+04   2489.732    -19.187      0.000   -5.27e+04   -4.29e+04\nBldgGrade      1.061e+05   2396.445     44.277      0.000    1.01e+05    1.11e+05\nconst         -5.219e+05   1.57e+04    -33.342      0.000   -5.53e+05   -4.91e+05\n==============================================================================\nOmnibus:                    29676.557   Durbin-Watson:                   1.247\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         19390738.346\nSkew:                           6.889   Prob(JB):                         0.00\nKurtosis:                     145.559   Cond. No.                     2.86e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.86e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#interpretando-resultados-da-regressão-múltipla",
    "href": "3-regressao/2-regressao-multipla.html#interpretando-resultados-da-regressão-múltipla",
    "title": "Análise de dados",
    "section": "Interpretando resultados da regressão múltipla",
    "text": "Interpretando resultados da regressão múltipla\n\nOs coeficientes das variáveis preditoras têm o mesmo significado da regressão simples\n\nIndica quanto a variável resposta muda para cada unidade da variável preditora correspondente\n\nRoot mean squared error, raíz da média dos erros ao quadrado, é uma métrica útil para comparar a qualidade de diferentes modelos\n\n\\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}}\\]"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#interpretando-resultados-da-regressão-múltipla-1",
    "href": "3-regressao/2-regressao-multipla.html#interpretando-resultados-da-regressão-múltipla-1",
    "title": "Análise de dados",
    "section": "Interpretando resultados da regressão múltipla",
    "text": "Interpretando resultados da regressão múltipla\n\nAlém do R-squared (\\(R^2\\)), algumas ferramentas como o statsmodels também reportam outras métricas:\n\nAdjusted R-squared penaliza modelos com muitos preditores\n\nAIC e BIC são métricas que também penalizam a adição de variáveis\n\nO Teste-t é usado para medir a importância de cada preditor\n\np-valor da estatística t (P>|t|) indica significância estatística de preditor\nQuanto maior o t e menor seu p-valor, mais significativo é o preditor\n\nO Teste-F verifica se a regressão é significativa (\\(SSR > SSE\\))\nNo geral, p-valores \\(< 0.05\\) indicam significância estatística\n\nVeremos sobre testes de hipótese e p-valor mais para frente"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#variáveis-preditoras-não-numéricas",
    "href": "3-regressao/2-regressao-multipla.html#variáveis-preditoras-não-numéricas",
    "title": "Análise de dados",
    "section": "Variáveis preditoras não numéricas",
    "text": "Variáveis preditoras não numéricas\n\nDeve-se converter variáveis categóricas e boolean em númericas\n\n\n\n\n\n\n\n\n  \n    \n      \n      PropertyType\n      NbrLivingUnits\n      SqFtFinBasement\n      YrBuilt\n      YrRenovated\n      NewConstruction\n    \n  \n  \n    \n      1\n      Multiplex\n      2\n      0\n      1991\n      0\n      False\n    \n    \n      2\n      Single Family\n      1\n      1452\n      2005\n      0\n      True\n    \n    \n      3\n      Single Family\n      1\n      900\n      1947\n      0\n      False\n    \n    \n      4\n      Single Family\n      1\n      1640\n      1966\n      0\n      False\n    \n    \n      5\n      Single Family\n      1\n      0\n      1948\n      0\n      False\n    \n  \n\n\n\n\n\n\nVariáveis dummy: cada categoria vira uma coluna, com valor 0 ou 1\n\n\nX = pd.get_dummies(house[predictors_full], drop_first=True)\nX['NewConstruction'] = [1 if nc else 0 for nc in X['NewConstruction']]\n\n\n\n\n\n\n  \n    \n      \n      NbrLivingUnits\n      SqFtFinBasement\n      YrBuilt\n      YrRenovated\n      NewConstruction\n      PropertyType_Single Family\n      PropertyType_Townhouse\n    \n  \n  \n    \n      1\n      2\n      0\n      1991\n      0\n      0\n      0\n      0\n    \n    \n      2\n      1\n      1452\n      2005\n      0\n      1\n      1\n      0\n    \n    \n      3\n      1\n      900\n      1947\n      0\n      0\n      1\n      0\n    \n    \n      4\n      1\n      1640\n      1966\n      0\n      0\n      1\n      0\n    \n    \n      5\n      1\n      0\n      1948\n      0\n      0\n      1\n      0"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#variáveis-preditoras-não-numéricas-1",
    "href": "3-regressao/2-regressao-multipla.html#variáveis-preditoras-não-numéricas-1",
    "title": "Análise de dados",
    "section": "Variáveis preditoras não numéricas",
    "text": "Variáveis preditoras não numéricas\n\nVariáveis categóricas ordinais podem ser transformadas diretamente em numéricas, mantendo a sua ordem\n\nPor exemplo, a variável BldgGrade é uma categórica ordinal que representa o nível da casa e foi associada a um número inteiro\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Value\n      Description\n    \n  \n  \n    \n      0\n      1\n      Cabin\n    \n    \n      1\n      2\n      Substandard\n    \n    \n      2\n      5\n      Fair\n    \n    \n      3\n      10\n      Very good\n    \n    \n      4\n      12\n      Luxury\n    \n    \n      5\n      13\n      Mansion\n    \n  \n\n\n\n\n\n\nDeve-se ter cuidado porque o valor numérico da variável também representa sua intensidade"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#avaliando-modelo-com-mais-variáveis",
    "href": "3-regressao/2-regressao-multipla.html#avaliando-modelo-com-mais-variáveis",
    "title": "Análise de dados",
    "section": "Avaliando modelo com mais variáveis",
    "text": "Avaliando modelo com mais variáveis\n\n\\(R^2\\) melhorou (era \\(0.541\\)), mas nem sempre é o caso\n\n\nhouse_full = sm.OLS(house[outcome], X.assign(const=1))\nresults = house_full.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           AdjSalePrice   R-squared:                       0.595\nModel:                            OLS   Adj. R-squared:                  0.594\nMethod:                 Least Squares   F-statistic:                     2771.\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):               0.00\nTime:                        11:14:48   Log-Likelihood:            -3.1375e+05\nNo. Observations:               22687   AIC:                         6.275e+05\nDf Residuals:                   22674   BIC:                         6.276e+05\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==============================================================================================\n                                 coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nSqFtTotLiving                198.6364      4.234     46.920      0.000     190.338     206.934\nSqFtLot                        0.0771      0.058      1.330      0.184      -0.037       0.191\nBathrooms                   4.286e+04   3808.114     11.255      0.000    3.54e+04    5.03e+04\nBedrooms                   -5.187e+04   2396.904    -21.638      0.000   -5.66e+04   -4.72e+04\nBldgGrade                   1.373e+05   2441.242     56.228      0.000    1.32e+05    1.42e+05\nNbrLivingUnits              5723.8438   1.76e+04      0.326      0.744   -2.87e+04    4.01e+04\nSqFtFinBasement                7.0611      4.627      1.526      0.127      -2.009      16.131\nYrBuilt                    -3574.2210     77.228    -46.282      0.000   -3725.593   -3422.849\nYrRenovated                   -2.5311      3.924     -0.645      0.519     -10.222       5.160\nNewConstruction            -2489.1122   5936.692     -0.419      0.675   -1.41e+04    9147.211\nPropertyType_Single Family  2.997e+04   2.61e+04      1.149      0.251   -2.12e+04    8.11e+04\nPropertyType_Townhouse      9.286e+04    2.7e+04      3.438      0.001    3.99e+04    1.46e+05\nconst                       6.182e+06   1.55e+05     39.902      0.000    5.88e+06    6.49e+06\n==============================================================================\nOmnibus:                    31006.128   Durbin-Watson:                   1.393\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         26251977.078\nSkew:                           7.427   Prob(JB):                         0.00\nKurtosis:                     168.984   Cond. No.                     2.98e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.98e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#correlação-entre-variáveis-preditoras",
    "href": "3-regressao/2-regressao-multipla.html#correlação-entre-variáveis-preditoras",
    "title": "Análise de dados",
    "section": "Correlação entre variáveis preditoras",
    "text": "Correlação entre variáveis preditoras\n\nAlgo estranho no model: o coeficiente de Bedrooms foi negativo\n\nQuanto mais quartos, menor o valor da casa?!\n\nCorrelação entre preditores pode gerar resultados contraditórios\n\n\nX.corr()['Bedrooms'].sort_values(ascending=False)\n\nBedrooms                      1.000000\nSqFtTotLiving                 0.600317\nBathrooms                     0.537998\nBldgGrade                     0.368125\nSqFtFinBasement               0.312380\nPropertyType_Single Family    0.175518\nNbrLivingUnits                0.168103\nYrBuilt                       0.148334\nSqFtLot                       0.069091\nYrRenovated                   0.045970\nNewConstruction               0.011105\nPropertyType_Townhouse       -0.235182\nName: Bedrooms, dtype: float64"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#correlação-entre-variáveis-preditoras-1",
    "href": "3-regressao/2-regressao-multipla.html#correlação-entre-variáveis-preditoras-1",
    "title": "Análise de dados",
    "section": "Correlação entre variáveis preditoras",
    "text": "Correlação entre variáveis preditoras\n\nRemovendo variáveis correlacionadas com Bedrooms\n\nO coeficiente de Bedrooms passou a ser positivo\n\n\n\nX2 = X.filter(['Bedrooms', 'BldgGrade', 'PropertyType_Townhouse', 'PropertyType_Single Family', 'YrBuilt'])\nhouse_mcol = sm.OLS(house[outcome], X2.assign(const=1))\nresults = house_mcol.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           AdjSalePrice   R-squared:                       0.516\nModel:                            OLS   Adj. R-squared:                  0.516\nMethod:                 Least Squares   F-statistic:                     4836.\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):               0.00\nTime:                        11:14:48   Log-Likelihood:            -3.1576e+05\nNo. Observations:               22687   AIC:                         6.315e+05\nDf Residuals:                   22681   BIC:                         6.316e+05\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================================\n                                 coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nBedrooms                    2.715e+04   2228.534     12.183      0.000    2.28e+04    3.15e+04\nBldgGrade                    2.49e+05   1785.425    139.461      0.000    2.45e+05    2.52e+05\nPropertyType_Townhouse     -4.736e+04    1.9e+04     -2.497      0.013   -8.45e+04   -1.02e+04\nPropertyType_Single Family  -1.99e+04   1.71e+04     -1.161      0.246   -5.35e+04    1.37e+04\nYrBuilt                    -3211.7449     69.197    -46.414      0.000   -3347.376   -3076.114\nconst                       4.914e+06    1.3e+05     37.764      0.000    4.66e+06    5.17e+06\n==============================================================================\nOmnibus:                    31054.093   Durbin-Watson:                   1.319\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         23187763.917\nSkew:                           7.504   Prob(JB):                         0.00\nKurtosis:                     158.899   Cond. No.                     1.44e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.44e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#variáveis-com-pouca-significância-no-teste-t",
    "href": "3-regressao/2-regressao-multipla.html#variáveis-com-pouca-significância-no-teste-t",
    "title": "Análise de dados",
    "section": "Variáveis com pouca significância no teste-t",
    "text": "Variáveis com pouca significância no teste-t\n\nRemovendo PropertyType_Single Family com p-valor \\(<0.05\\)\n\n\n\nCode\nX3 = X2.drop(columns=['PropertyType_Single Family'])\nhouse_lm2 = sm.OLS(house[outcome], X3.assign(const=1))\nresults = house_lm2.fit()\nprint(results.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           AdjSalePrice   R-squared:                       0.516\nModel:                            OLS   Adj. R-squared:                  0.516\nMethod:                 Least Squares   F-statistic:                     6045.\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):               0.00\nTime:                        11:14:48   Log-Likelihood:            -3.1576e+05\nNo. Observations:               22687   AIC:                         6.315e+05\nDf Residuals:                   22682   BIC:                         6.316e+05\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==========================================================================================\n                             coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------\nBedrooms                2.756e+04   2200.308     12.526      0.000    2.32e+04    3.19e+04\nBldgGrade               2.489e+05   1781.631    139.682      0.000    2.45e+05    2.52e+05\nPropertyType_Townhouse -2.708e+04   7379.696     -3.670      0.000   -4.15e+04   -1.26e+04\nYrBuilt                -3219.8804     68.842    -46.772      0.000   -3354.815   -3084.946\nconst                    4.91e+06    1.3e+05     37.746      0.000    4.65e+06    5.16e+06\n==============================================================================\nOmnibus:                    31049.473   Durbin-Watson:                   1.319\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         23170913.709\nSkew:                           7.501   Prob(JB):                         0.00\nKurtosis:                     158.842   Cond. No.                     1.44e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.44e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#problema-da-multicolinearidade",
    "href": "3-regressao/2-regressao-multipla.html#problema-da-multicolinearidade",
    "title": "Análise de dados",
    "section": "Problema da multicolinearidade",
    "text": "Problema da multicolinearidade\n\nQuando duas variáveis preditoras são linearmente dependentes, dizemos que elas são colineares (forte correlação linear)\n\nElas deixam de ser independentes entre si e ambas trazem essencialmente a mesma informação\nNenhuma pode contribuir significativamente para a variação de \\(y\\) depois que a outra foi incluída\n\nA multicolinearidade afeta os resultados da regressão\n\nVariáveis devem ser removidas até eliminar a multicolinearidade"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#problema-da-multicolinearidade-1",
    "href": "3-regressao/2-regressao-multipla.html#problema-da-multicolinearidade-1",
    "title": "Análise de dados",
    "section": "Problema da multicolinearidade",
    "text": "Problema da multicolinearidade\n\nPrecisamos encontrar a correlação entre vários pares \\(x_i\\)\n\nQuando a correlação for alta devemos eliminar uma das variáveis do modelo e refazer os cálculos\nSe a significância da regressão aumentar, então concluímos que a correlação forte estava causando o problema\n\n\n\n\nX.corr()\n\n\n\n\n\n  \n    \n      \n      SqFtTotLiving\n      SqFtLot\n      Bathrooms\n      Bedrooms\n      BldgGrade\n      NbrLivingUnits\n      SqFtFinBasement\n      YrBuilt\n      YrRenovated\n      NewConstruction\n      PropertyType_Single Family\n      PropertyType_Townhouse\n    \n  \n  \n    \n      SqFtTotLiving\n      1.000000\n      0.195967\n      0.764155\n      0.600317\n      0.770499\n      0.059581\n      0.407234\n      0.311560\n      0.067191\n      0.113734\n      0.176935\n      -0.194445\n    \n    \n      SqFtLot\n      0.195967\n      1.000000\n      0.107439\n      0.069091\n      0.145508\n      -0.005113\n      0.035430\n      0.068400\n      0.006911\n      -0.067419\n      0.099319\n      -0.097781\n    \n    \n      Bathrooms\n      0.764155\n      0.107439\n      1.000000\n      0.537998\n      0.658738\n      0.111242\n      0.272772\n      0.468610\n      0.066480\n      0.197005\n      -0.066454\n      0.047977\n    \n    \n      Bedrooms\n      0.600317\n      0.069091\n      0.537998\n      1.000000\n      0.368125\n      0.168103\n      0.312380\n      0.148334\n      0.045970\n      0.011105\n      0.175518\n      -0.235182\n    \n    \n      BldgGrade\n      0.770499\n      0.145508\n      0.658738\n      0.368125\n      1.000000\n      -0.048098\n      0.137714\n      0.434524\n      0.025361\n      0.168542\n      0.000326\n      0.023983\n    \n    \n      NbrLivingUnits\n      0.059581\n      -0.005113\n      0.111242\n      0.168103\n      -0.048098\n      1.000000\n      0.066807\n      -0.109187\n      0.011369\n      -0.036865\n      -0.274530\n      -0.030503\n    \n    \n      SqFtFinBasement\n      0.407234\n      0.035430\n      0.272772\n      0.312380\n      0.137714\n      0.066807\n      1.000000\n      -0.210810\n      0.109495\n      -0.113499\n      0.082861\n      -0.104528\n    \n    \n      YrBuilt\n      0.311560\n      0.068400\n      0.468610\n      0.148334\n      0.434524\n      -0.109187\n      -0.210810\n      1.000000\n      -0.247338\n      0.386531\n      -0.232476\n      0.291983\n    \n    \n      YrRenovated\n      0.067191\n      0.006911\n      0.066480\n      0.045970\n      0.025361\n      0.011369\n      0.109495\n      -0.247338\n      1.000000\n      -0.075830\n      0.050243\n      -0.054187\n    \n    \n      NewConstruction\n      0.113734\n      -0.067419\n      0.197005\n      0.011105\n      0.168542\n      -0.036865\n      -0.113499\n      0.386531\n      -0.075830\n      1.000000\n      -0.261869\n      0.292238\n    \n    \n      PropertyType_Single Family\n      0.176935\n      0.099319\n      -0.066454\n      0.175518\n      0.000326\n      -0.274530\n      0.082861\n      -0.232476\n      0.050243\n      -0.261869\n      1.000000\n      -0.926657\n    \n    \n      PropertyType_Townhouse\n      -0.194445\n      -0.097781\n      0.047977\n      -0.235182\n      0.023983\n      -0.030503\n      -0.104528\n      0.291983\n      -0.054187\n      0.292238\n      -0.926657\n      1.000000"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#problema-da-multicolinearidade-2",
    "href": "3-regressao/2-regressao-multipla.html#problema-da-multicolinearidade-2",
    "title": "Análise de dados",
    "section": "Problema da multicolinearidade",
    "text": "Problema da multicolinearidade\n\n\nCode\nfrom scipy.stats import pearsonr\nimport matplotlib.pyplot as plt \n\ndef corrfunc(x, y, ax=None, **kws):\n    \"\"\"Plot the correlation coefficient in the top left hand corner of a plot.\"\"\"\n    r, _ = pearsonr(x, y)\n    ax = ax or plt.gca()\n    ax.annotate(f'ρ = {r:.2f}', xy=(.2, .99), xycoords=ax.transAxes)\n\n#g = sns.pairplot(X2, height=1.3)\n#g.map_lower(corrfunc)\n#plt.show()"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#variável-de-confusão-confounding",
    "href": "3-regressao/2-regressao-multipla.html#variável-de-confusão-confounding",
    "title": "Análise de dados",
    "section": "Variável de confusão (confounding)",
    "text": "Variável de confusão (confounding)\n\nUma variável importante que influencia a variável dependente (e algumas independentes) que não está no modelo\n\nExemplo: falta variável que represente a localização da casa, que pode influenciar no preço. Adicionando variável ZipGroup (5 regiões)\n\n\n\n\nCode\nzip_groups = pd.DataFrame([\n    *pd.DataFrame({\n        'ZipCode': house['ZipCode'],\n        'residual' : house[outcome] - house_lm.predict(house[predictors]),\n    })\n    .groupby(['ZipCode'])\n    .apply(lambda x: {\n        'ZipCode': x.iloc[0,0],\n        'count': len(x),\n        'median_residual': x.residual.median()\n    })\n]).sort_values('median_residual')\nzip_groups['cum_count'] = np.cumsum(zip_groups['count'])\nzip_groups['ZipGroup'] = pd.qcut(zip_groups['cum_count'], 5, labels=False, retbins=False)\nzip_groups.head()\n\nto_join = zip_groups[['ZipCode', 'ZipGroup']].set_index('ZipCode')\nhouse = house.join(to_join, on='ZipCode')\nhouse['ZipGroup'] = house['ZipGroup'].astype('category')\n\npredictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n              'BldgGrade', 'PropertyType', 'ZipGroup']\noutcome = 'AdjSalePrice'\n\nX4 = pd.get_dummies(house[predictors], drop_first=True)\n\nhouse_lm4 = sm.OLS(house[outcome], X4.assign(const=1))\nresults = house_lm4.fit()\nprint(results.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           AdjSalePrice   R-squared:                       0.638\nModel:                            OLS   Adj. R-squared:                  0.637\nMethod:                 Least Squares   F-statistic:                     3626.\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):               0.00\nTime:                        11:14:48   Log-Likelihood:            -3.1248e+05\nNo. Observations:               22687   AIC:                         6.250e+05\nDf Residuals:                   22675   BIC:                         6.251e+05\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n==============================================================================================\n                                 coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nSqFtTotLiving                210.6127      3.679     57.249      0.000     203.402     217.824\nSqFtLot                        0.4550      0.055      8.288      0.000       0.347       0.563\nBathrooms                   5928.4256   3406.349      1.740      0.082    -748.253    1.26e+04\nBedrooms                   -4.168e+04   2260.875    -18.437      0.000   -4.61e+04   -3.73e+04\nBldgGrade                   9.854e+04   2198.695     44.818      0.000    9.42e+04    1.03e+05\nPropertyType_Single Family  1.932e+04   1.49e+04      1.300      0.194   -9813.581    4.85e+04\nPropertyType_Townhouse      -7.82e+04   1.62e+04     -4.839      0.000    -1.1e+05   -4.65e+04\nZipGroup_1                  5.332e+04   5141.118     10.371      0.000    4.32e+04    6.34e+04\nZipGroup_2                  1.163e+05   4835.157     24.043      0.000    1.07e+05    1.26e+05\nZipGroup_3                  1.784e+05   4968.852     35.896      0.000    1.69e+05    1.88e+05\nZipGroup_4                  3.384e+05   4724.478     71.629      0.000    3.29e+05    3.48e+05\nconst                      -6.666e+05   2.04e+04    -32.739      0.000   -7.07e+05   -6.27e+05\n==============================================================================\nOmnibus:                    33232.769   Durbin-Watson:                   1.466\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         39575049.715\nSkew:                           8.440   Prob(JB):                         0.00\nKurtosis:                     206.913   Cond. No.                     5.50e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.5e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#interação-entre-variáveis",
    "href": "3-regressao/2-regressao-multipla.html#interação-entre-variáveis",
    "title": "Análise de dados",
    "section": "Interação entre variáveis",
    "text": "Interação entre variáveis\n\nPodemos considerar a interação entre variáveis no modelo\n\nCombinando variáveis ZipGroup (região) e SqFtTotLiving (área)\nO metro quadrado custa mais em algumas áreas do que em outras\n\n\n\n\nCode\nmodel = smf.ols(formula='AdjSalePrice ~  SqFtTotLiving*ZipGroup + SqFtLot + ' +\n     'Bathrooms + Bedrooms + BldgGrade + PropertyType', data=house)\nresults = model.fit()\nprint(results.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           AdjSalePrice   R-squared:                       0.682\nModel:                            OLS   Adj. R-squared:                  0.682\nMethod:                 Least Squares   F-statistic:                     3247.\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):               0.00\nTime:                        11:14:48   Log-Likelihood:            -3.1098e+05\nNo. Observations:               22687   AIC:                         6.220e+05\nDf Residuals:                   22671   BIC:                         6.221e+05\nDf Model:                          15                                         \nCovariance Type:            nonrobust                                         \n=================================================================================================\n                                    coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nIntercept                     -4.853e+05   2.05e+04    -23.701      0.000   -5.25e+05   -4.45e+05\nZipGroup[T.1]                 -1.113e+04   1.34e+04     -0.830      0.407   -3.74e+04    1.52e+04\nZipGroup[T.2]                  2.032e+04   1.18e+04      1.717      0.086   -2877.441    4.35e+04\nZipGroup[T.3]                   2.05e+04   1.21e+04      1.697      0.090   -3180.870    4.42e+04\nZipGroup[T.4]                 -1.499e+05   1.13e+04    -13.285      0.000   -1.72e+05   -1.28e+05\nPropertyType[T.Single Family]  1.357e+04   1.39e+04      0.975      0.330   -1.37e+04    4.09e+04\nPropertyType[T.Townhouse]     -5.884e+04   1.51e+04     -3.888      0.000   -8.85e+04   -2.92e+04\nSqFtTotLiving                   114.7650      4.863     23.600      0.000     105.233     124.297\nSqFtTotLiving:ZipGroup[T.1]      32.6043      5.712      5.708      0.000      21.409      43.799\nSqFtTotLiving:ZipGroup[T.2]      41.7822      5.187      8.056      0.000      31.616      51.948\nSqFtTotLiving:ZipGroup[T.3]      69.3415      5.619     12.341      0.000      58.329      80.354\nSqFtTotLiving:ZipGroup[T.4]     226.6836      4.820     47.032      0.000     217.237     236.131\nSqFtLot                           0.6869      0.052     13.296      0.000       0.586       0.788\nBathrooms                     -3619.4533   3202.296     -1.130      0.258   -9896.174    2657.267\nBedrooms                       -4.18e+04   2120.279    -19.715      0.000    -4.6e+04   -3.76e+04\nBldgGrade                      1.047e+05   2069.472     50.592      0.000    1.01e+05    1.09e+05\n==============================================================================\nOmnibus:                    30927.394   Durbin-Watson:                   1.581\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         34361794.502\nSkew:                           7.279   Prob(JB):                         0.00\nKurtosis:                     193.101   Cond. No.                     5.80e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.8e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#análise-dos-resíduos-outliers",
    "href": "3-regressao/2-regressao-multipla.html#análise-dos-resíduos-outliers",
    "title": "Análise de dados",
    "section": "Análise dos resíduos: Outliers",
    "text": "Análise dos resíduos: Outliers\n\nPodemos analisar os resíduos da regressão para mais diagnósticos\n\nAnalisando para um ZipCode específico:\n\n\n\n\nCode\nhouse_98105 = house.loc[house['ZipCode'] == 98105, ]\n\npredictors = ['SqFtTotLiving', 'SqFtLot', 'Bathrooms', 'Bedrooms',\n              'BldgGrade']\noutcome = 'AdjSalePrice'\n\nhouse_outlier = sm.OLS(house_98105[outcome], house_98105[predictors].assign(const=1))\nresult_98105 = house_outlier.fit()\nprint(result_98105.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           AdjSalePrice   R-squared:                       0.795\nModel:                            OLS   Adj. R-squared:                  0.792\nMethod:                 Least Squares   F-statistic:                     238.7\nDate:                Fri, 28 Oct 2022   Prob (F-statistic):          1.69e-103\nTime:                        11:14:48   Log-Likelihood:                -4226.0\nNo. Observations:                 313   AIC:                             8464.\nDf Residuals:                     307   BIC:                             8486.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nSqFtTotLiving   209.6023     24.408      8.587      0.000     161.574     257.631\nSqFtLot          38.9333      5.330      7.305      0.000      28.445      49.421\nBathrooms      2282.2641      2e+04      0.114      0.909    -3.7e+04    4.16e+04\nBedrooms      -2.632e+04   1.29e+04     -2.043      0.042   -5.17e+04    -973.867\nBldgGrade        1.3e+05   1.52e+04      8.533      0.000       1e+05     1.6e+05\nconst         -7.725e+05   9.83e+04     -7.861      0.000   -9.66e+05   -5.79e+05\n==============================================================================\nOmnibus:                       82.127   Durbin-Watson:                   1.508\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              586.561\nSkew:                           0.859   Prob(JB):                    4.26e-128\nKurtosis:                       9.483   Cond. No.                     5.63e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.63e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#análise-dos-resíduos-outliers-1",
    "href": "3-regressao/2-regressao-multipla.html#análise-dos-resíduos-outliers-1",
    "title": "Análise de dados",
    "section": "Análise dos resíduos: Outliers",
    "text": "Análise dos resíduos: Outliers\n\nPara identificar outliers , podemos analisar estatísticas do resíduo padrão (resíduo dividido pelo erro padrão)\n\n\ninfluence = OLSInfluence(result_98105)\nsresiduals = influence.resid_studentized_internal\nprint(f\"Maior erro padrão (superestimado): {sresiduals.min()}. Erro absoluto: {result_98105.resid.loc[sresiduals.idxmin()]}\")\n\nMaior erro padrão (superestimado): -4.326731804078561. Erro absoluto: -757753.6192115824\n\n\n\nA maior superestimativa foi de 4 erros padrões\n\nAnalisando o outlier, aparenta ser um erro no registro dos dados\n\n\n\noutlier = house_98105.loc[sresiduals.idxmin(), :]\nprint('AdjSalePrice', outlier[outcome])\nprint(outlier[predictors])\n\nAdjSalePrice 119748.0\nSqFtTotLiving    2900\nSqFtLot          7276\nBathrooms         3.0\nBedrooms            6\nBldgGrade           7\nName: 24333, dtype: object"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#propriedades-desejáveis-dos-resíduos",
    "href": "3-regressao/2-regressao-multipla.html#propriedades-desejáveis-dos-resíduos",
    "title": "Análise de dados",
    "section": "Propriedades desejáveis dos resíduos",
    "text": "Propriedades desejáveis dos resíduos\n\nHomocedasticidade: a variância dos resíduos deve se manter constante ao longo dos valores estimados\n\nVisualização da estimativa x resíduo indica que variância dos resíduos é maior para valores de casas muito baratas ou muito caras\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(6, 4))\nsns.regplot(x=result_98105.fittedvalues, y=np.abs(result_98105.resid), \n            scatter_kws={'alpha': 0.25},\n            line_kws={'color': 'C1'},\n            lowess=True, ax=ax)\nax.set_xlabel('predicted')\nax.set_ylabel('abs(residual)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#propriedades-desejáveis-dos-resíduos-1",
    "href": "3-regressao/2-regressao-multipla.html#propriedades-desejáveis-dos-resíduos-1",
    "title": "Análise de dados",
    "section": "Propriedades desejáveis dos resíduos",
    "text": "Propriedades desejáveis dos resíduos\n\nNormalidade: idealmente, resíduos devem seguir distribuição normal\nPodemos visualizar o histograma do erro padrão dos resíduos\n\nAparenta ter cauda maior do que a normal, com leve viés à direita\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(6, 4))\npd.Series(influence.resid_studentized_internal).hist(ax=ax)\nax.set_xlabel('std. residual')\nax.set_ylabel('Frequency')\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3-regressao/2-regressao-multipla.html#partial-residual-plot",
    "href": "3-regressao/2-regressao-multipla.html#partial-residual-plot",
    "title": "Análise de dados",
    "section": "Partial residual plot",
    "text": "Partial residual plot\n\nAnalisa visualmente a estimativa em relação a cada preditor\n\nPlota o preditor no eixo \\(x\\) e o resíduo parcial no eixo \\(y\\)\n\nRelação do preço com SqFtTotLiving parece ser não linear\n\n\n\n\nfig = sm.graphics.plot_ccpr_grid(result_98105, fig=plt.figure(figsize=(10, 12)))"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#regressão-logística",
    "href": "3-regressao/3-regressao-logistica.html#regressão-logística",
    "title": "Análise de dados",
    "section": "Regressão logística",
    "text": "Regressão logística\n\nSimilar à regressão linear múltipla, mas resposta é binária\n\nÉ um método popular de classificação (prevê uma classe/categoria)\n\nA variável resposta é pensada como a probabilidade do valor ser 1\nFunção da resposta logística ou logit inversa dos preditores\n\nPara os valores de \\(p\\) ficarem em uma escala de 0 a 1\n\n\n\\[p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_q x_q)}}\\]"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#regressão-logística-1",
    "href": "3-regressao/3-regressao-logistica.html#regressão-logística-1",
    "title": "Análise de dados",
    "section": "Regressão logística",
    "text": "Regressão logística\n\nOdds é a probabilidade de sucesso dividido pelo insucesso\n\n\\(Odds (Y = 1) = \\frac{p}{1 - p}\\)\n\nPodemos obter a probabilidade dos odds com a função inversa:\n\n\\(p = \\frac{Odds}{1 + Odds}\\)\n\nCombinando com a função da resposta logística e aplicando log, teremos a função log-odds ou logit:\n\n\\[log(Odds(Y = 1)) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_q x_q\\]"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#regressão-logística-2",
    "href": "3-regressao/3-regressao-logistica.html#regressão-logística-2",
    "title": "Análise de dados",
    "section": "Regressão logística",
    "text": "Regressão logística\n\nA função logit mapeia \\(p\\) de \\((0,1)\\) para valores de \\((-\\infty, \\infty)\\)\n\nSe a probabilidade \\(p\\) for maior que um limiar, classificamos como 1\n\n\n\n\nCode\np = np.arange(0.01, 1, 0.01)\ndf = pd.DataFrame({\n    'p': p,\n    'logit': np.log(p / (1 - p)),\n    'odds': p / (1 - p),\n})\n\nfig, ax = plt.subplots(figsize=(4, 4))\nax.axhline(0, color='grey', linestyle='--')\nax.axvline(0.5, color='grey', linestyle='--')\nax.plot(df['p'], df['logit'])\nax.set_xlabel('Probability')\nax.set_ylabel('logit(p)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#exemplo-de-regressão-logística-dados-de-empréstimo",
    "href": "3-regressao/3-regressao-logistica.html#exemplo-de-regressão-logística-dados-de-empréstimo",
    "title": "Análise de dados",
    "section": "Exemplo de regressão logística: dados de empréstimo",
    "text": "Exemplo de regressão logística: dados de empréstimo\n\nEstimar status de empréstimo: pago (paid off) ou não pago (default)\n\n\n\nCode\nloan_data = pd.read_csv(LOAN_DATA_CSV, index_col=0)\n\n# convert to categorical\nloan_data.outcome = loan_data.outcome.astype('category')\nloan_data.outcome.cat.reorder_categories(['paid off', 'default'])\nloan_data.purpose_ = loan_data.purpose_.astype('category')\nloan_data.home_ = loan_data.home_.astype('category')\nloan_data.emp_len_ = loan_data.emp_len_.astype('category')\n\noutcome = 'outcome'\npredictors = ['payment_inc_ratio', 'purpose_', 'home_', 'emp_len_', \n              'borrower_score']\nloan_data[[outcome] + predictors]\n\n\n\n\n\n\n  \n    \n      \n      outcome\n      payment_inc_ratio\n      purpose_\n      home_\n      emp_len_\n      borrower_score\n    \n  \n  \n    \n      1\n      default\n      2.39320\n      major_purchase\n      RENT\n      > 1 Year\n      0.65\n    \n    \n      2\n      default\n      4.57170\n      small_business\n      OWN\n      > 1 Year\n      0.80\n    \n    \n      3\n      default\n      9.71600\n      other\n      RENT\n      > 1 Year\n      0.60\n    \n    \n      4\n      default\n      12.21520\n      debt_consolidation\n      RENT\n      > 1 Year\n      0.50\n    \n    \n      5\n      default\n      3.90888\n      other\n      RENT\n      > 1 Year\n      0.55\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      45338\n      paid off\n      4.57850\n      credit_card\n      RENT\n      > 1 Year\n      0.35\n    \n    \n      45339\n      paid off\n      3.20333\n      other\n      MORTGAGE\n      > 1 Year\n      0.50\n    \n    \n      45340\n      paid off\n      9.98460\n      debt_consolidation\n      MORTGAGE\n      > 1 Year\n      0.55\n    \n    \n      45341\n      paid off\n      6.45250\n      debt_consolidation\n      MORTGAGE\n      > 1 Year\n      0.65\n    \n    \n      45342\n      paid off\n      11.87150\n      debt_consolidation\n      MORTGAGE\n      > 1 Year\n      0.50\n    \n  \n\n45342 rows × 6 columns"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#exemplo-de-regressão-logística-dados-de-empréstimo-1",
    "href": "3-regressao/3-regressao-logistica.html#exemplo-de-regressão-logística-dados-de-empréstimo-1",
    "title": "Análise de dados",
    "section": "Exemplo de regressão logística: dados de empréstimo",
    "text": "Exemplo de regressão logística: dados de empréstimo\n\n\n\nA regressão logística é um tipo de General Linear Model (GLM) com:\n\nDistribuição de probabilidade: binomial\nFunção de ligação (link) para mapear preditores: logit\n\n\n\n\nAplicando a regressão com scikit-learn\n\n\n\nCode\nX = pd.get_dummies(loan_data[predictors], prefix='', prefix_sep='', \n                   drop_first=True)\ny = loan_data[outcome] # .cat.categories\n\nlogit_reg = LogisticRegression(penalty='l2', C=1e42, solver='liblinear')\nlogit_reg.fit(X, y)\n\nprint('intercept ', logit_reg.intercept_[0])\nprint('classes', logit_reg.classes_)\npd.DataFrame({'coeff': logit_reg.coef_[0]}, \n             index=X.columns)\n\n\nintercept  -1.6378908649112442\nclasses ['default' 'paid off']\n\n\n\n\n\n\n  \n    \n      \n      coeff\n    \n  \n  \n    \n      payment_inc_ratio\n      -0.079739\n    \n    \n      borrower_score\n      4.612183\n    \n    \n      debt_consolidation\n      -0.249414\n    \n    \n      home_improvement\n      -0.407734\n    \n    \n      major_purchase\n      -0.229710\n    \n    \n      medical\n      -0.510744\n    \n    \n      other\n      -0.620800\n    \n    \n      small_business\n      -1.214936\n    \n    \n      OWN\n      -0.048211\n    \n    \n      RENT\n      -0.157288\n    \n    \n      > 1 Year\n      0.356794"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#regressão-logística-3",
    "href": "3-regressao/3-regressao-logistica.html#regressão-logística-3",
    "title": "Análise de dados",
    "section": "Regressão logística",
    "text": "Regressão logística\n\nO valor estimado da regressão logística é \\(\\hat{Y} = log(Odds(Y = 1))\\)\nA probabilidade estimada é dada pela função resposta logística:\n\n\\[ \\hat{p} = \\frac{1}{1 + e^{-\\hat{Y}}}\\]\n\nEstatísticas dos valores e das probabilidades estimadas:\n\n\n\n\npred = pd.DataFrame(logit_reg.predict_log_proba(X), columns=logit_reg.classes_)\nprint(pred.describe())\n\n            default      paid off\ncount  45342.000000  45342.000000\nmean      -0.757872     -0.760434\nstd        0.378093      0.390456\nmin       -2.769376     -3.538593\n25%       -0.985739     -0.977207\n50%       -0.697406     -0.688907\n75%       -0.472183     -0.467069\nmax       -0.029485     -0.064753\n\n\n\n\npred = pd.DataFrame(logit_reg.predict_proba(X), columns=logit_reg.classes_)\nprint(pred.describe())\n\n            default      paid off\ncount  45342.000000  45342.000000\nmean       0.500000      0.500000\nstd        0.167354      0.167354\nmin        0.062701      0.029054\n25%        0.373163      0.376361\n50%        0.497875      0.502125\n75%        0.623639      0.626837\nmax        0.970946      0.937299"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#regressão-logística-4",
    "href": "3-regressao/3-regressao-logistica.html#regressão-logística-4",
    "title": "Análise de dados",
    "section": "Regressão logística",
    "text": "Regressão logística\n\nUsa-se um limiar da probabilidade para definir resposta (0 ou 1)\n\nGeralmente, \\(p > 0.5\\) se considera valor 1\n\n\n\n\n\n\n\n\n  \n    \n      \n      default\n      paid off\n    \n  \n  \n    \n      0\n      0.242505\n      0.757495\n    \n    \n      1\n      0.314114\n      0.685886\n    \n    \n      2\n      0.516650\n      0.483350\n    \n    \n      3\n      0.588009\n      0.411991\n    \n    \n      4\n      0.458639\n      0.541361\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      45337\n      0.547161\n      0.452839\n    \n    \n      45338\n      0.462878\n      0.537122\n    \n    \n      45339\n      0.447686\n      0.552314\n    \n    \n      45340\n      0.278304\n      0.721696\n    \n    \n      45341\n      0.542658\n      0.457342\n    \n  \n\n45342 rows × 2 columns"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#interpretando-coeficientes-e-odds-ratio",
    "href": "3-regressao/3-regressao-logistica.html#interpretando-coeficientes-e-odds-ratio",
    "title": "Análise de dados",
    "section": "Interpretando coeficientes e odds ratio",
    "text": "Interpretando coeficientes e odds ratio\n\nOdds ratio mede a força da associação entre dois eventos \\(A\\) e \\(B\\)\n\n\\[\\text{odds ratio} = \\frac{Odds(Y = 1 | X = 1)}{Odds(Y = 1 | X = 0)}\\]\n\nVisualizando mudança no odds ratio para cada mudança \\(x\\):\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(5, 3))\nax.plot(df['logit'], df['odds'])\nax.set_xlabel('log(odds ratio)')\nax.set_ylabel('odds ratio')\nax.set_xlim(0, 5.1)\nax.set_ylim(-5, 105)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#avaliando-o-modelo",
    "href": "3-regressao/3-regressao-logistica.html#avaliando-o-modelo",
    "title": "Análise de dados",
    "section": "Avaliando o modelo",
    "text": "Avaliando o modelo\n\nA regressão linear é ajustada com método dos mínimos quadrados\nA regressão logística não tem solução fechada e é ajustado com estimativa por máxima verossimilhança – ou Maximum Likelihood Estimation (MLE)\n\nEncontra modelo que log odds mais se aproximam da variável resposta"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#avaliando-o-modelo-com-statsmodels",
    "href": "3-regressao/3-regressao-logistica.html#avaliando-o-modelo-com-statsmodels",
    "title": "Análise de dados",
    "section": "Avaliando o modelo com statsmodels",
    "text": "Avaliando o modelo com statsmodels\n\ny_numbers = [1 if yi == 'default' else 0 for yi in y]\nlogit_reg_sm = sm.GLM(y_numbers, X.assign(const=1), family=sm.families.Binomial())\nlogit_result = logit_reg_sm.fit()\nprint(logit_result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                45342\nModel:                            GLM   Df Residuals:                    45330\nModel Family:                Binomial   Df Model:                           11\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -28757.\nDate:                Fri, 28 Oct 2022   Deviance:                       57515.\nTime:                        11:14:58   Pearson chi2:                 4.54e+04\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.1112\nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\npayment_inc_ratio      0.0797      0.002     32.058      0.000       0.075       0.085\nborrower_score        -4.6126      0.084    -55.203      0.000      -4.776      -4.449\ndebt_consolidation     0.2494      0.028      9.030      0.000       0.195       0.303\nhome_improvement       0.4077      0.047      8.747      0.000       0.316       0.499\nmajor_purchase         0.2296      0.054      4.277      0.000       0.124       0.335\nmedical                0.5105      0.087      5.882      0.000       0.340       0.681\nother                  0.6207      0.039     15.738      0.000       0.543       0.698\nsmall_business         1.2153      0.063     19.192      0.000       1.091       1.339\nOWN                    0.0483      0.038      1.271      0.204      -0.026       0.123\nRENT                   0.1573      0.021      7.420      0.000       0.116       0.199\n > 1 Year             -0.3567      0.053     -6.779      0.000      -0.460      -0.254\nconst                  1.6381      0.074     22.224      0.000       1.494       1.783\n======================================================================================"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#avaliando-outro-modelo-com-fórmula-e-splines",
    "href": "3-regressao/3-regressao-logistica.html#avaliando-outro-modelo-com-fórmula-e-splines",
    "title": "Análise de dados",
    "section": "Avaliando outro modelo com fórmula e splines",
    "text": "Avaliando outro modelo com fórmula e splines\n\nimport statsmodels.formula.api as smf\nformula = ('outcome ~ bs(payment_inc_ratio, df=8) + purpose_ + ' +\n           'home_ + emp_len_ + bs(borrower_score, df=3)')\nmodel = smf.glm(formula=formula, data=loan_data, family=sm.families.Binomial())\nresults = model.fit()\nprint(results.summary())\n\n                             Generalized Linear Model Regression Results                             \n=====================================================================================================\nDep. Variable:     ['outcome[default]', 'outcome[paid off]']   No. Observations:                45342\nModel:                                                   GLM   Df Residuals:                    45321\nModel Family:                                       Binomial   Df Model:                           20\nLink Function:                                         Logit   Scale:                          1.0000\nMethod:                                                 IRLS   Log-Likelihood:                -28731.\nDate:                                       Fri, 28 Oct 2022   Deviance:                       57462.\nTime:                                               11:14:58   Pearson chi2:                 4.54e+04\nNo. Iterations:                                            6   Pseudo R-squ. (CS):             0.1122\nCovariance Type:                                   nonrobust                                         \n==================================================================================================\n                                     coef    std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------------\nIntercept                          1.5756      0.331      4.765      0.000       0.928       2.224\npurpose_[T.debt_consolidation]     0.2486      0.028      8.998      0.000       0.194       0.303\npurpose_[T.home_improvement]       0.4097      0.047      8.757      0.000       0.318       0.501\npurpose_[T.major_purchase]         0.2382      0.054      4.416      0.000       0.132       0.344\npurpose_[T.medical]                0.5206      0.087      5.980      0.000       0.350       0.691\npurpose_[T.other]                  0.6284      0.040     15.781      0.000       0.550       0.706\npurpose_[T.small_business]         1.2250      0.063     19.305      0.000       1.101       1.349\nhome_[T.OWN]                       0.0498      0.038      1.309      0.191      -0.025       0.124\nhome_[T.RENT]                      0.1577      0.021      7.431      0.000       0.116       0.199\nemp_len_[T. > 1 Year]             -0.3526      0.053     -6.699      0.000      -0.456      -0.249\nbs(payment_inc_ratio, df=8)[0]     0.7042      0.342      2.060      0.039       0.034       1.374\nbs(payment_inc_ratio, df=8)[1]     0.6621      0.198      3.351      0.001       0.275       1.049\nbs(payment_inc_ratio, df=8)[2]     0.8118      0.245      3.309      0.001       0.331       1.293\nbs(payment_inc_ratio, df=8)[3]     1.0377      0.223      4.644      0.000       0.600       1.476\nbs(payment_inc_ratio, df=8)[4]     1.1901      0.233      5.112      0.000       0.734       1.646\nbs(payment_inc_ratio, df=8)[5]     2.8404      0.316      8.980      0.000       2.220       3.460\nbs(payment_inc_ratio, df=8)[6]    -1.3427      1.229     -1.092      0.275      -3.752       1.067\nbs(payment_inc_ratio, df=8)[7]     7.1094      6.393      1.112      0.266      -5.420      19.639\nbs(borrower_score, df=3)[0]       -2.9011      0.533     -5.448      0.000      -3.945      -1.857\nbs(borrower_score, df=3)[1]       -2.6056      0.196    -13.284      0.000      -2.990      -2.221\nbs(borrower_score, df=3)[2]       -5.7421      0.508    -11.313      0.000      -6.737      -4.747\n=================================================================================================="
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#análise-de-resíduos-parciais",
    "href": "3-regressao/3-regressao-logistica.html#análise-de-resíduos-parciais",
    "title": "Análise de dados",
    "section": "Análise de resíduos parciais",
    "text": "Análise de resíduos parciais\n\nVisualizando o efeito de uma variável nos resíduos\n\n\n\nCode\nfrom statsmodels.genmod.generalized_linear_model import GLMResults\ndef partialResidualPlot(model, df, outcome, feature, fig, ax):\n    y_actual = [0 if s == 'default' else 1 for s in df[outcome]]\n    y_pred = model.predict(df)\n    org_params = model.params.copy()\n    zero_params = model.params.copy()\n    # set model parametes of other features to 0\n    for i, name in enumerate(zero_params.index):\n        if feature in name:\n            continue\n        zero_params[i] = 0.0\n    model.initialize(model.model, zero_params)\n    feature_prediction = model.predict(df)\n    ypartial = -np.log(1/feature_prediction - 1)\n    ypartial = ypartial - np.mean(ypartial)\n    model.initialize(model.model, org_params)\n    results = pd.DataFrame({\n        'feature': df[feature],\n        'residual': -2 * (y_actual - y_pred),\n        'ypartial': ypartial/ 2,\n    })\n    results = results.sort_values(by=['feature'])\n\n    ax.scatter(results.feature, results.residual, marker=\".\", s=72./fig.dpi)\n    ax.plot(results.feature, results.ypartial, color='black')\n    ax.set_xlabel(feature)\n    ax.set_ylabel(f'Residual + {feature} contribution')\n    return ax\n\nfig, ax = plt.subplots(figsize=(5, 5))\npartialResidualPlot(results, loan_data, 'outcome', 'payment_inc_ratio', fig, ax)\nax.set_xlim(0, 25)\nax.set_ylim(-2.5, 2.5)\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação",
    "href": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação",
    "title": "Análise de dados",
    "section": "Avaliando modelos de classificação",
    "text": "Avaliando modelos de classificação\n\nMatriz de confusão\n\n\n\n\n# Confusion matrix\npred = logit_reg.predict(X)\npred_y = logit_reg.predict(X) == 'default'\ntrue_y = y == 'default'\ntrue_pos = true_y & pred_y\ntrue_neg = ~true_y & ~pred_y\nfalse_pos = ~true_y & pred_y\nfalse_neg = true_y & ~pred_y\n\nconf_mat = pd.DataFrame([[np.sum(true_pos), np.sum(false_neg)], [np.sum(false_pos), np.sum(true_neg)]],\n                       index=['Y = default', 'Y = paid off'],\n                       columns=['Yhat = default', 'Yhat = paid off'])\nprint(conf_mat)\n\n              Yhat = default  Yhat = paid off\nY = default            14337             8334\nY = paid off            8149            14522\n\n\n\n\nprint(confusion_matrix(y, logit_reg.predict(X)))\n\n[[14337  8334]\n [ 8149 14522]]"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação-1",
    "href": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação-1",
    "title": "Análise de dados",
    "section": "Avaliando modelos de classificação",
    "text": "Avaliando modelos de classificação\n\n\n\nPrecisão\n\n\\(\\text{precision} = \\frac{\\sum{\\text{TruePositive}}}{\\sum{\\text{TruePositive}} + \\sum{\\text{FalsePositive}}}\\)\n\nRevocação / Sensibilidade\n\n\\(\\text{recall} = \\frac{\\sum{\\text{TruePositive}}}{\\sum{\\text{TruePositive}} + \\sum{\\text{FalseNegative}}}\\)\n\nEspecificidade\n\n\\(\\text{specificity} = \\frac{\\sum{\\text{TrueNegative}}}{\\sum{\\text{TruePositive}} + \\sum{\\text{FalsePositive}}}\\)\n\n\n\n\nconf_mat = confusion_matrix(y, logit_reg.predict(X))\nprint('Precision', conf_mat[0, 0] / sum(conf_mat[:, 0]))\nprint('Recall', conf_mat[0, 0] / sum(conf_mat[0, :]))\nprint('Specificity', conf_mat[1, 1] / sum(conf_mat[1, :]))\n\nPrecision 0.6375967268522637\nRecall 0.6323938070662961\nSpecificity 0.640554011733051\n\n\n\nprecision_recall_fscore_support(y, logit_reg.predict(X),\n                                labels=['default', 'paid off'])\n\n(array([0.63759673, 0.63536927]),\n array([0.63239381, 0.64055401]),\n array([0.63498461, 0.63795111]),\n array([22671, 22671]))"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação-2",
    "href": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação-2",
    "title": "Análise de dados",
    "section": "Avaliando modelos de classificação",
    "text": "Avaliando modelos de classificação\n\nCurva ROC: trade-off entre revocação e especificidade\n\nLinha tracejada representa classificação aleatória\nDesejável curva que suba muito rapidamente\n\n\n\n\nCode\nfpr, tpr, thresholds = roc_curve(y, logit_reg.predict_proba(X)[:, 0], \n                                 pos_label='default')\nroc_df = pd.DataFrame({'recall': tpr, 'specificity': 1 - fpr})\n\nax = roc_df.plot(x='specificity', y='recall', figsize=(4, 4), legend=False)\nax.set_ylim(0, 1)\nax.set_xlim(1, 0)\nax.plot((1, 0), (0, 1))\nax.set_xlabel('specificity')\nax.set_ylabel('recall')\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação-3",
    "href": "3-regressao/3-regressao-logistica.html#avaliando-modelos-de-classificação-3",
    "title": "Análise de dados",
    "section": "Avaliando modelos de classificação",
    "text": "Avaliando modelos de classificação\n\nAUC: área abaixo da curva ROC\n\nAUC maior indica classificador mais eficaz\n\n\n\nprint(np.sum(roc_df.recall[:-1] * np.diff(1 - roc_df.specificity)))\nprint(roc_auc_score([1 if yi == 'default' else 0 for yi in y], logit_reg.predict_proba(X)[:, 0]))\n\n0.6917107174637571\n0.691710795288669\n\n\n\n\nCode\nfpr, tpr, thresholds = roc_curve(y, logit_reg.predict_proba(X)[:,0], \n                                 pos_label='default')\nroc_df = pd.DataFrame({'recall': tpr, 'specificity': 1 - fpr})\n\nax = roc_df.plot(x='specificity', y='recall', figsize=(4, 4), legend=False)\nax.set_ylim(0, 1)\nax.set_xlim(1, 0)\n# ax.plot((1, 0), (0, 1))\nax.set_xlabel('specificity')\nax.set_ylabel('recall')\nax.fill_between(roc_df.specificity, 0, roc_df.recall, alpha=0.3)\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "analise_eleicoes.html",
    "href": "analise_eleicoes.html",
    "title": "Análise das eleições presidenciais 2018 x 2022",
    "section": "",
    "text": "eleicoes = read_csv(\"dados/eleicoes_resultado_presidente_uf_2018_2022.csv\")\n\nNew names:\nRows: 756 Columns: 8\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): nm_urna_candidato, uf dbl (6): ...1, ano_eleicao, nr_turno, nr_candidato,\nvotos, perc_votos\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\nglimpse(eleicoes)\n\nRows: 756\nColumns: 8\n$ ...1              <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ ano_eleicao       <dbl> 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018…\n$ nr_turno          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ nr_candidato      <dbl> 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, …\n$ nm_urna_candidato <chr> \"CIRO GOMES\", \"CIRO GOMES\", \"CIRO GOMES\", \"CIRO GOME…\n$ uf                <chr> \"AC\", \"AL\", \"AM\", \"AP\", \"BA\", \"CE\", \"DF\", \"ES\", \"GO\"…\n$ votos             <dbl> 21809, 155457, 138997, 50553, 693273, 1998597, 26627…\n$ perc_votos        <dbl> 5.170730, 10.122572, 7.498796, 12.337312, 9.408547, …\n\n\n\njb <- eleicoes %>%\n  filter(nm_urna_candidato == \"JAIR BOLSONARO\")\n\nglimpse(eleicoes)\n\nRows: 756\nColumns: 8\n$ ...1              <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ ano_eleicao       <dbl> 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018…\n$ nr_turno          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ nr_candidato      <dbl> 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, …\n$ nm_urna_candidato <chr> \"CIRO GOMES\", \"CIRO GOMES\", \"CIRO GOMES\", \"CIRO GOME…\n$ uf                <chr> \"AC\", \"AL\", \"AM\", \"AP\", \"BA\", \"CE\", \"DF\", \"ES\", \"GO\"…\n$ votos             <dbl> 21809, 155457, 138997, 50553, 693273, 1998597, 26627…\n$ perc_votos        <dbl> 5.170730, 10.122572, 7.498796, 12.337312, 9.408547, …\n\neleicoes_dif <- eleicoes %>%\n  filter(nr_turno == 1, nm_urna_candidato == \"JAIR BOLSONARO\" | nr_candidato == 13) %>%\n  group_by(uf, ano_eleicao) %>%\n  summarise(\n    perc_votos_jb = perc_votos[nm_urna_candidato == \"JAIR BOLSONARO\"],\n    perc_votos_pt = perc_votos[nr_candidato == 13],\n    dif_jb_pt = perc_votos_jb - perc_votos_pt\n  ) %>%\n  mutate(\n    direcao = ifelse(dif_jb_pt[ano_eleicao == 2022] >= dif_jb_pt[ano_eleicao == 2018],\n                     \"cresceu\", \"diminuiu\")\n  )\n\n`summarise()` has grouped output by 'uf'. You can override using the `.groups`\nargument.\n\neleicoes_dif\n\n# A tibble: 56 × 6\n# Groups:   uf [28]\n   uf    ano_eleicao perc_votos_jb perc_votos_pt dif_jb_pt direcao \n   <chr>       <dbl>         <dbl>         <dbl>     <dbl> <chr>   \n 1 AC           2018          62.2          18.5     43.7  diminuiu\n 2 AC           2022          62.5          29.3     33.2  diminuiu\n 3 AL           2018          34.4          44.8    -10.3  diminuiu\n 4 AL           2022          36.0          56.5    -20.5  diminuiu\n 5 AM           2018          43.5          40.3      3.18 diminuiu\n 6 AM           2022          42.8          49.6     -6.78 diminuiu\n 7 AP           2018          40.7          32.8      7.97 diminuiu\n 8 AP           2022          43.4          45.7     -2.26 diminuiu\n 9 BA           2018          23.4          60.3    -36.9  diminuiu\n10 BA           2022          24.3          69.7    -45.4  diminuiu\n# … with 46 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\neleicoes_dif_w <- eleicoes_dif %>%\n  pivot_wider(id_cols = uf, names_from = ano_eleicao,\n              values_from = dif_jb_pt, names_prefix = 'dif_') %>%\n  ungroup() %>%\n  mutate(\n    direcao = ifelse(dif_2022 > dif_2018, \"aumentou\", \"diminuiu\"),\n    dif_2022_2018 = round(dif_2022 - dif_2018),\n    uf = fct_reorder(uf, dif_2022_2018, .desc = T)\n  )\n\n\np <- ggplot(eleicoes_dif_w, aes(y = uf)) +\n    geom_col(aes(dif_2022, fill = dif_2022), col = 'darkgray', show.legend = F) +\n    geom_vline(aes(xintercept=0), lty=2) +\n    geom_point(aes(dif_2018)) +\n    geom_segment(aes(x=dif_2018, xend=dif_2022, yend=uf),\n                 arrow = arrow(length = unit(0.05, \"inches\"))) +\n    #geom_label(aes(x=(dif_2022 + dif_2018)/2, y=uf, col=direcao,\n    #               label=paste0(abs(round(dif_2022_2018)), '%')), size=1.8, alpha=0.9,\n    #           label.padding=unit(0.09, \"lines\"), nudge_y = 0.3, show.legend = F) +\n    geom_text(aes(x=(dif_2022 + dif_2018)/2, y=uf, label=paste0(abs(dif_2022_2018), '%')),\n              size=1.9, alpha=0.9, nudge_y = 0.23, fontface = \"bold\", show.legend = F) +\n    #geom_line(aes(group = uf), arrow = arrow(length=unit(0.30,\"cm\"), ends=\"first\")) +\n    #geom_segment(aes(group = uf), arrow = arrow(length=unit(0.30,\"cm\"), ends=\"first\")) +\n    scale_fill_distiller(palette = \"RdYlBu\", direction = 1) +\n    scale_color_brewer(palette = \"Set1\", direction = -1) +\n    scale_x_continuous(limits = c(-55, 55), breaks = c(-50, -25, 0, 25, 50),\n                       labels = c(\"(pró-PT)\", \"25%\", \"0%\", \"25%\", \"(pró-JB)\")) +\n    labs(\n      title = \"Diferença na votação entre Bolsonaro e PT\",\n      subtitle = \"em 2022 (barras), 2018 (ponto) e diferença (seta)\",\n      x = \"Diferença no percentual de votos\"\n    ) +\n    theme_minimal() +\n    theme(axis.title.y = element_blank())\np\n\n\n\nggsave('plots/diferenca_votos_jb_pt_2022_2018.png', p, width=4, height=6)\n\n\nprint(eleicoes_dif_w$uf)\n\n [1] AC AL AM AP BA CE DF ES GO MA MG MS MT PA PB PE PI PR RJ RN RO RR RS SC SE\n[26] SP TO ZZ\n28 Levels: RR MA RO PA BA AC AL AM AP MT PI TO SE PE MS PR SC ES PB RN ... ZZ"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análise de Dados I @ DCX / CCAE / UFPB",
    "section": "",
    "text": "Material da disciplina de Análise de Dados I"
  },
  {
    "objectID": "index.html#introdução",
    "href": "index.html#introdução",
    "title": "Análise de Dados I @ DCX / CCAE / UFPB",
    "section": "1 Introdução",
    "text": "1 Introdução\n\n1.1 Introdução à análise de dados (Slides)\n1.2 Introdução ao Python e Jupyter Notebook (Atividade)\n1.3 Introdução ao Pandas (Notebook)\n1.4 Exemplo com Pandas: análise de notas (Notebook)\nLaboratório 1: Manipulação de dados (GitHub)"
  },
  {
    "objectID": "index.html#análise-exploratória-de-dados",
    "href": "index.html#análise-exploratória-de-dados",
    "title": "Análise de Dados I @ DCX / CCAE / UFPB",
    "section": "2 Análise Exploratória de Dados",
    "text": "2 Análise Exploratória de Dados\n\n2.1 Introdução à análise exploratória (Slides)\n2.2 Estatística descritiva (Slides)\n2.3 Análise de correlação (Slides)\nLaboratório 2: Análise Exploratória de Dados (GitHub)"
  },
  {
    "objectID": "index.html#regressão",
    "href": "index.html#regressão",
    "title": "Análise de Dados I @ DCX / CCAE / UFPB",
    "section": "3 Regressão",
    "text": "3 Regressão\n\n3.1 Regressão linear simples (Slides)\n3.2 Regressão linear múltipla (Slides)\n3.3 Regressão logística (Slides)\nLaboratório 3: Regressão (GitHub)"
  },
  {
    "objectID": "1-introducao/3-intro-pandas.html",
    "href": "1-introducao/3-intro-pandas.html",
    "title": "Aulas AD 1",
    "section": "",
    "text": "Referências\n\nPython for Data Analysis - 5 Getting Started with pandas"
  },
  {
    "objectID": "1-introducao/4-exemplo-pandas.html",
    "href": "1-introducao/4-exemplo-pandas.html",
    "title": "Aulas AD 1",
    "section": "",
    "text": "Vamos usar um exemplo de análise para demonstrar mais funções da biblioteca Pandas de Python.\n\n\nO primeiro passo será carregar os dados que estão armazenados em um arquivo CSV. O arquivo que possui as notas das 4 unidades da disciplina Sistemas Operacionais no período 2017.1. O parâmetro index_col=\"id\" indica que a coluna id que consta no arquivo será o identificar das linhas do DataFrame.\n\nimport numpy as np\nimport pandas as pd\n\nso_20171 = pd.read_csv(\"../dados/notas_so_20171.csv\")\nso_20171\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      nota_4\n    \n  \n  \n    \n      0\n      1\n      8.3\n      8.0\n      9.3\n      8.0\n    \n    \n      1\n      2\n      7.0\n      7.7\n      4.6\n      8.7\n    \n    \n      2\n      3\n      7.8\n      3.2\n      7.7\n      5.1\n    \n    \n      3\n      4\n      7.3\n      9.3\n      9.8\n      4.8\n    \n    \n      4\n      5\n      5.8\n      9.8\n      10.0\n      6.8\n    \n    \n      5\n      6\n      9.6\n      9.8\n      10.0\n      9.0\n    \n    \n      6\n      7\n      9.0\n      8.5\n      8.5\n      8.0\n    \n    \n      7\n      8\n      4.0\n      7.5\n      5.3\n      2.0\n    \n    \n      8\n      9\n      10.0\n      9.9\n      9.6\n      9.1\n    \n    \n      9\n      10\n      8.3\n      8.7\n      8.0\n      6.7\n    \n    \n      10\n      11\n      10.0\n      10.0\n      8.5\n      9.8\n    \n    \n      11\n      12\n      7.3\n      9.0\n      7.8\n      7.0\n    \n    \n      12\n      13\n      8.8\n      9.2\n      10.0\n      8.7\n    \n    \n      13\n      14\n      6.5\n      8.6\n      7.0\n      4.0\n    \n    \n      14\n      15\n      10.0\n      8.5\n      8.1\n      8.5\n    \n    \n      15\n      16\n      9.7\n      10.0\n      7.7\n      7.5\n    \n    \n      16\n      17\n      8.3\n      9.7\n      8.7\n      6.8\n    \n    \n      17\n      18\n      7.5\n      9.4\n      6.0\n      7.2\n    \n    \n      18\n      19\n      9.3\n      9.9\n      9.8\n      9.1\n    \n    \n      19\n      20\n      0.0\n      2.5\n      1.0\n      0.0\n    \n    \n      20\n      21\n      5.0\n      8.0\n      5.8\n      4.8\n    \n  \n\n\n\n\nPodemos usar o comando info() para obter informações sobre os tipos das colunas e a presença de dados nulos:\n\nso_20171.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 21 entries, 0 to 20\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   id      21 non-null     int64  \n 1   nota_1  21 non-null     float64\n 2   nota_2  21 non-null     float64\n 3   nota_3  21 non-null     float64\n 4   nota_4  21 non-null     float64\ndtypes: float64(4), int64(1)\nmemory usage: 968.0 bytes\n\n\nOutra função útil é a describe(), que calcula estatísticas básicas para as colunas de um DataFrame:\n\nso_20171.describe()\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      nota_4\n    \n  \n  \n    \n      count\n      21.000000\n      21.000000\n      21.000000\n      21.000000\n      21.000000\n    \n    \n      mean\n      11.000000\n      7.595238\n      8.438095\n      7.771429\n      6.742857\n    \n    \n      std\n      6.204837\n      2.408003\n      2.020266\n      2.242129\n      2.510492\n    \n    \n      min\n      1.000000\n      0.000000\n      2.500000\n      1.000000\n      0.000000\n    \n    \n      25%\n      6.000000\n      7.000000\n      8.000000\n      7.000000\n      5.100000\n    \n    \n      50%\n      11.000000\n      8.300000\n      9.000000\n      8.100000\n      7.200000\n    \n    \n      75%\n      16.000000\n      9.300000\n      9.800000\n      9.600000\n      8.700000\n    \n    \n      max\n      21.000000\n      10.000000\n      10.000000\n      10.000000\n      9.800000\n    \n  \n\n\n\n\nJá vimos que para calcular a média das notas de cada unidade podemos fazer:\n\nso_20171.mean(numeric_only=True)\n\nid        11.000000\nnota_1     7.595238\nnota_2     8.438095\nnota_3     7.771429\nnota_4     6.742857\ndtype: float64\n\n\nSuponha que queremos calcular a média das notas de cada uma das 4 unidades para todas as turmas de SO nos últimos 10 períodos. Para fazer do modo acima, precisaríamos de pelo menos 10 linhas de código, uma para cada turma. Isso não parece muito interessante. O jeito mais eficiente de fazer isso é juntar todos os dados em uma única tabela e aplicar funções em grupos de dados.\nVamos fazer um exemplo carregando os dados de mais um período (2017.2):\n\nso_20172 = pd.read_csv(\"../dados/notas_so_20172.csv\")\nso_20172\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      nota_4\n    \n  \n  \n    \n      0\n      1\n      9.8\n      9.7\n      9.5\n      7.0\n    \n    \n      1\n      2\n      9.2\n      9.8\n      7.0\n      7.2\n    \n    \n      2\n      3\n      9.1\n      8.0\n      9.5\n      8.0\n    \n    \n      3\n      4\n      5.6\n      7.8\n      8.5\n      6.5\n    \n    \n      4\n      5\n      6.3\n      8.0\n      6.3\n      5.5\n    \n    \n      5\n      6\n      6.6\n      6.7\n      7.2\n      3.8\n    \n    \n      6\n      7\n      9.3\n      9.0\n      9.2\n      8.5\n    \n    \n      7\n      8\n      9.0\n      10.0\n      8.7\n      8.1\n    \n    \n      8\n      9\n      7.8\n      8.5\n      9.0\n      7.5\n    \n    \n      9\n      10\n      7.5\n      8.0\n      9.0\n      5.3\n    \n    \n      10\n      11\n      8.3\n      9.0\n      5.2\n      6.2\n    \n    \n      11\n      12\n      7.5\n      9.8\n      8.2\n      6.0\n    \n    \n      12\n      13\n      7.7\n      7.3\n      4.5\n      6.5\n    \n    \n      13\n      14\n      5.8\n      7.5\n      6.5\n      6.2\n    \n    \n      14\n      15\n      7.8\n      9.3\n      8.8\n      4.5\n    \n    \n      15\n      16\n      0.0\n      2.5\n      2.0\n      3.5\n    \n    \n      16\n      17\n      5.8\n      8.3\n      7.5\n      3.5\n    \n    \n      17\n      18\n      9.6\n      9.0\n      10.0\n      5.5\n    \n    \n      18\n      19\n      7.0\n      6.7\n      6.6\n      8.0\n    \n    \n      19\n      20\n      7.3\n      8.6\n      7.3\n      0.0\n    \n    \n      20\n      21\n      9.5\n      8.8\n      7.8\n      6.2\n    \n    \n      21\n      22\n      2.0\n      0.0\n      1.0\n      2.0\n    \n    \n      22\n      23\n      7.3\n      8.0\n      6.7\n      7.5\n    \n    \n      23\n      24\n      9.5\n      8.8\n      7.0\n      8.3\n    \n    \n      24\n      25\n      9.7\n      9.0\n      3.4\n      7.4\n    \n    \n      25\n      26\n      8.9\n      6.7\n      5.5\n      4.5\n    \n  \n\n\n\n\n\n\n\nNote que a sua estrutura é semelhante à do DataFrame anterior. Desta forma, podemos juntá-los em um único data frame. Antes disso, temos que adicionar uma nova coluna que identifique o período que as notas se referem:\n\nso_20171[\"periodo\"] = \"2017.1\"\nso_20171.head()\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      nota_4\n      periodo\n    \n  \n  \n    \n      0\n      1\n      8.3\n      8.0\n      9.3\n      8.0\n      2017.1\n    \n    \n      1\n      2\n      7.0\n      7.7\n      4.6\n      8.7\n      2017.1\n    \n    \n      2\n      3\n      7.8\n      3.2\n      7.7\n      5.1\n      2017.1\n    \n    \n      3\n      4\n      7.3\n      9.3\n      9.8\n      4.8\n      2017.1\n    \n    \n      4\n      5\n      5.8\n      9.8\n      10.0\n      6.8\n      2017.1\n    \n  \n\n\n\n\n\nso_20172[\"periodo\"] = \"2017.2\"\nso_20172.head()\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      nota_4\n      periodo\n    \n  \n  \n    \n      0\n      1\n      9.8\n      9.7\n      9.5\n      7.0\n      2017.2\n    \n    \n      1\n      2\n      9.2\n      9.8\n      7.0\n      7.2\n      2017.2\n    \n    \n      2\n      3\n      9.1\n      8.0\n      9.5\n      8.0\n      2017.2\n    \n    \n      3\n      4\n      5.6\n      7.8\n      8.5\n      6.5\n      2017.2\n    \n    \n      4\n      5\n      6.3\n      8.0\n      6.3\n      5.5\n      2017.2\n    \n  \n\n\n\n\nO comando head() mostra apenas as primeiras linhas da tabela.\nComo os dois DataFrames possuem as mesmas colunas, podemos juntá-los com a função concat:\n\nso = pd.concat([so_20171, so_20172])\nso\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      nota_4\n      periodo\n    \n  \n  \n    \n      0\n      1\n      8.3\n      8.0\n      9.3\n      8.0\n      2017.1\n    \n    \n      1\n      2\n      7.0\n      7.7\n      4.6\n      8.7\n      2017.1\n    \n    \n      2\n      3\n      7.8\n      3.2\n      7.7\n      5.1\n      2017.1\n    \n    \n      3\n      4\n      7.3\n      9.3\n      9.8\n      4.8\n      2017.1\n    \n    \n      4\n      5\n      5.8\n      9.8\n      10.0\n      6.8\n      2017.1\n    \n    \n      5\n      6\n      9.6\n      9.8\n      10.0\n      9.0\n      2017.1\n    \n    \n      6\n      7\n      9.0\n      8.5\n      8.5\n      8.0\n      2017.1\n    \n    \n      7\n      8\n      4.0\n      7.5\n      5.3\n      2.0\n      2017.1\n    \n    \n      8\n      9\n      10.0\n      9.9\n      9.6\n      9.1\n      2017.1\n    \n    \n      9\n      10\n      8.3\n      8.7\n      8.0\n      6.7\n      2017.1\n    \n    \n      10\n      11\n      10.0\n      10.0\n      8.5\n      9.8\n      2017.1\n    \n    \n      11\n      12\n      7.3\n      9.0\n      7.8\n      7.0\n      2017.1\n    \n    \n      12\n      13\n      8.8\n      9.2\n      10.0\n      8.7\n      2017.1\n    \n    \n      13\n      14\n      6.5\n      8.6\n      7.0\n      4.0\n      2017.1\n    \n    \n      14\n      15\n      10.0\n      8.5\n      8.1\n      8.5\n      2017.1\n    \n    \n      15\n      16\n      9.7\n      10.0\n      7.7\n      7.5\n      2017.1\n    \n    \n      16\n      17\n      8.3\n      9.7\n      8.7\n      6.8\n      2017.1\n    \n    \n      17\n      18\n      7.5\n      9.4\n      6.0\n      7.2\n      2017.1\n    \n    \n      18\n      19\n      9.3\n      9.9\n      9.8\n      9.1\n      2017.1\n    \n    \n      19\n      20\n      0.0\n      2.5\n      1.0\n      0.0\n      2017.1\n    \n    \n      20\n      21\n      5.0\n      8.0\n      5.8\n      4.8\n      2017.1\n    \n    \n      0\n      1\n      9.8\n      9.7\n      9.5\n      7.0\n      2017.2\n    \n    \n      1\n      2\n      9.2\n      9.8\n      7.0\n      7.2\n      2017.2\n    \n    \n      2\n      3\n      9.1\n      8.0\n      9.5\n      8.0\n      2017.2\n    \n    \n      3\n      4\n      5.6\n      7.8\n      8.5\n      6.5\n      2017.2\n    \n    \n      4\n      5\n      6.3\n      8.0\n      6.3\n      5.5\n      2017.2\n    \n    \n      5\n      6\n      6.6\n      6.7\n      7.2\n      3.8\n      2017.2\n    \n    \n      6\n      7\n      9.3\n      9.0\n      9.2\n      8.5\n      2017.2\n    \n    \n      7\n      8\n      9.0\n      10.0\n      8.7\n      8.1\n      2017.2\n    \n    \n      8\n      9\n      7.8\n      8.5\n      9.0\n      7.5\n      2017.2\n    \n    \n      9\n      10\n      7.5\n      8.0\n      9.0\n      5.3\n      2017.2\n    \n    \n      10\n      11\n      8.3\n      9.0\n      5.2\n      6.2\n      2017.2\n    \n    \n      11\n      12\n      7.5\n      9.8\n      8.2\n      6.0\n      2017.2\n    \n    \n      12\n      13\n      7.7\n      7.3\n      4.5\n      6.5\n      2017.2\n    \n    \n      13\n      14\n      5.8\n      7.5\n      6.5\n      6.2\n      2017.2\n    \n    \n      14\n      15\n      7.8\n      9.3\n      8.8\n      4.5\n      2017.2\n    \n    \n      15\n      16\n      0.0\n      2.5\n      2.0\n      3.5\n      2017.2\n    \n    \n      16\n      17\n      5.8\n      8.3\n      7.5\n      3.5\n      2017.2\n    \n    \n      17\n      18\n      9.6\n      9.0\n      10.0\n      5.5\n      2017.2\n    \n    \n      18\n      19\n      7.0\n      6.7\n      6.6\n      8.0\n      2017.2\n    \n    \n      19\n      20\n      7.3\n      8.6\n      7.3\n      0.0\n      2017.2\n    \n    \n      20\n      21\n      9.5\n      8.8\n      7.8\n      6.2\n      2017.2\n    \n    \n      21\n      22\n      2.0\n      0.0\n      1.0\n      2.0\n      2017.2\n    \n    \n      22\n      23\n      7.3\n      8.0\n      6.7\n      7.5\n      2017.2\n    \n    \n      23\n      24\n      9.5\n      8.8\n      7.0\n      8.3\n      2017.2\n    \n    \n      24\n      25\n      9.7\n      9.0\n      3.4\n      7.4\n      2017.2\n    \n    \n      25\n      26\n      8.9\n      6.7\n      5.5\n      4.5\n      2017.2\n    \n  \n\n\n\n\nE se quisermos saber as médias de cada período específico, usando a tabela com todas as notas? Vamos mostrar duas formas de fazer isso.\n\n\n\nPodemos filtrar os dados de cada período e em seguida calcular a média da unidade 1:\n\nso[so[\"periodo\"] == \"2017.1\"][\"nota_1\"].mean()\n\n7.595238095238097\n\n\n\nso[so[\"periodo\"] == \"2017.2\"][\"nota_1\"].mean()\n\n7.457692307692308\n\n\nDa mesma forma, podemos ver a fração de alunos com nota acima da média na unidade 1:\n\nlen(so[(so[\"periodo\"] == \"2017.1\") & (so[\"nota_1\"] >= 7)].index) / len(so[(so[\"periodo\"] == \"2017.1\")].index)\n\n0.7619047619047619\n\n\n\n len(so[(so[\"periodo\"] == \"2017.2\") & (so[\"nota_1\"] >= 7)].index) / len(so[(so[\"periodo\"] == \"2017.2\")].index)\n\n0.7307692307692307\n\n\nExiste uma forma mais adequada para fazer esse tipo de cálculo agrupado por categoria.\n\n\n\nA função groupby agrupa os dados de acordo com categorias definidas pelas colunas e aplica funções para cada grupo. A função summarise aplica uma função para cada grupo de dados, gerando novas colunas no data frame.\nVamos usar como exemplo o cálculo da média para cada unidade, agrupado por período:\n\nso_media = so.groupby([\"periodo\"]).mean()\nso_media\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      nota_4\n    \n    \n      periodo\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2017.1\n      11.0\n      7.595238\n      8.438095\n      7.771429\n      6.742857\n    \n    \n      2017.2\n      13.5\n      7.457692\n      7.876923\n      6.996154\n      5.892308\n    \n  \n\n\n\n\nComo fazemos então para calcular a fração de aprovados na unidade 1 para cada período?\n\ndef prop_aprovados(df, col=\"nota_1\", nota_aprovacao=7):\n    return len(df[df[\"nota_1\"] >= nota_aprovacao].index) / len(df.index)\n    \nso.groupby([\"periodo\"]).apply(prop_aprovados)\n\nperiodo\n2017.1    0.761905\n2017.2    0.730769\ndtype: float64\n\n\nAinda temos um incoveniente: ainda precisamos fazer cálculos especificando cada unidade que pretendemos observar. Além disso, se em algum dos períodos tivermos disciplinas com uma quantidade diferente de unidades (3 ao invés de 4) teríamos problemas com a estrutura atual. Vamos ver a seguir como organizar os dados de uma forma ainda mais adequada.\n\n\n\nVamos carregar os dados de outras duas disciplinas, Sistemas Distribuídos e Avaliação de Desempenho, aproveitando para adicionar o nome da disciplina e o período no data frame:\n\nads_20171 = pd.read_csv(\"../dados/notas_ads_20171.csv\")\nads_20171[\"disciplina\"] = \"Avaliação de Desempenho de Sistemas\"\nads_20171[\"periodo\"] = \"2017.1\"\nads_20171.head()\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      disciplina\n      periodo\n    \n  \n  \n    \n      0\n      1\n      9.8\n      7.0\n      5.0\n      Avaliação de Desempenho de Sistemas\n      2017.1\n    \n    \n      1\n      2\n      8.8\n      9.7\n      3.5\n      Avaliação de Desempenho de Sistemas\n      2017.1\n    \n    \n      2\n      3\n      9.5\n      7.0\n      8.0\n      Avaliação de Desempenho de Sistemas\n      2017.1\n    \n    \n      3\n      4\n      7.0\n      9.8\n      9.8\n      Avaliação de Desempenho de Sistemas\n      2017.1\n    \n    \n      4\n      5\n      3.5\n      8.8\n      9.0\n      Avaliação de Desempenho de Sistemas\n      2017.1\n    \n  \n\n\n\n\n\nsd_20172 = pd.read_csv(\"../dados/notas_sd_20172.csv\")\nsd_20172[\"disciplina\"] = \"Sistemas Distribuidos\"\nsd_20172[\"periodo\"] = \"2017.2\"\nsd_20172.head()\n\n\n\n\n\n  \n    \n      \n      id\n      nota_1\n      nota_2\n      nota_3\n      disciplina\n      periodo\n    \n  \n  \n    \n      0\n      1\n      6.8\n      8.0\n      9.5\n      Sistemas Distribuidos\n      2017.2\n    \n    \n      1\n      2\n      4.2\n      3.0\n      9.5\n      Sistemas Distribuidos\n      2017.2\n    \n    \n      2\n      3\n      0.0\n      0.0\n      0.0\n      Sistemas Distribuidos\n      2017.2\n    \n    \n      3\n      4\n      8.0\n      7.5\n      9.5\n      Sistemas Distribuidos\n      2017.2\n    \n    \n      4\n      5\n      7.0\n      7.5\n      10.0\n      Sistemas Distribuidos\n      2017.2\n    \n  \n\n\n\n\nPodemos juntar as notas de todas as disciplinas para facilitar o processamento. Mas temos um problema: as disciplinas de SO têm notas de 4 unidades, enquanto as de ADS e SD só têm 3 unidades. Além disso, ter uma coluna na tabela para cada unidade nos força a ter que especificar para qual unidade queremos calcular estatísticas. E se uma disciplina tiver 10 unidades (mini-testes, por exemplo), temos que ter 10 cálculos para calcular as médias de cada unidade?\nAi que entra o tidy data, que é uma forma de organizar os dados que facilita muito a análise usando as ferramentas de análise de dados.\nExistem 3 regras para uma tabela ser tidy:\n\nCada variável deve ter sua própria coluna.\nCada observação deve ter sua própria linha.\nCada valor deve ter sua própria célula.\n\nA figura abaixo mostra essas regras visualmente:\n\nComo transformar então nossas tabelas de notas no formato tidy? Um dos problems atuais é que a variável nota está espalhada em várias colunas: nota_1, nota_2, nota_3, … Temos outra variável implícita que está misturada com a variável nota que é a variável unidade, que pode indicar a qual unidade uma prova se refere. Desta forma, a tabela no formato tidy poderia ter uma coluna para a variável unidade e ter apenas uma coluna para a variável nota.\nVamos usar o Pandas para transformar os dados no formato tidy, usando funções de reshaping e pivot tables (leia este tutorial do Pandas para entender mais).\nPrimeiro para as notas de Sistemas Operacionais, vamos adicionar uma coluna com o nome da disciplina e transformar as colunas nota_1, nota_2, nota_3 e nota_4 em duas colunas: unidade e nota. Ou seja, um aluno da turma ao invés de ter as 4 notas em apenas uma linha, ele terá as notas distribuídas em 4 linhas, uma para cada unidade. Para isto, vamos usar a função do Pandas wide_to_long:\n\nso[\"disciplina\"] = \"Sistemas Operacionais\"\nso_tidy = pd.wide_to_long(so, stubnames='nota', i=['id', 'periodo', 'disciplina'],\n                          j='unidade', sep=\"_\")\nso_tidy\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      nota\n    \n    \n      id\n      periodo\n      disciplina\n      unidade\n      \n    \n  \n  \n    \n      1\n      2017.1\n      Sistemas Operacionais\n      1\n      8.3\n    \n    \n      2\n      8.0\n    \n    \n      3\n      9.3\n    \n    \n      4\n      8.0\n    \n    \n      2\n      2017.1\n      Sistemas Operacionais\n      1\n      7.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      25\n      2017.2\n      Sistemas Operacionais\n      4\n      7.4\n    \n    \n      26\n      2017.2\n      Sistemas Operacionais\n      1\n      8.9\n    \n    \n      2\n      6.7\n    \n    \n      3\n      5.5\n    \n    \n      4\n      4.5\n    \n  \n\n188 rows × 1 columns\n\n\n\nO parâmetro stubname='nota' indica que ele vai transformar as colunas que começam com a palavra “nota”. O parâmetro i=['id', 'periodo', 'disciplina'] indica as colunas que identificarão unicamente cada linha de observação (serão os índices). E o parâmetro j='unidade' indica o nome da nova coluna que será criada, onde os valores serão os números encontrados nas colunas nota_X, considerando que sep=\"_\" e os nomes das colunas têm o formato nota_unidade.\nFazendo isso agora para as outras disciplinas e concatenando tudo em um único DataFrame. Vamos também renomear o index id para id_aluno para melhor compreensão dos dados. O parâmetro inplace=True faz com que o DataFrame notas seja alterado ao invés de retornar um novo DataFrame com a alteração realizada.\n\nads_tidy = pd.wide_to_long(ads_20171, stubnames='nota', i=['id', 'periodo', 'disciplina'], j='unidade', sep=\"_\")\nsd_tidy = pd.wide_to_long(sd_20172, stubnames='nota', i=['id', 'periodo', 'disciplina'], j='unidade', sep=\"_\")\n\nnotas = pd.concat([so_tidy, ads_tidy, sd_tidy])\nnotas.index.set_names({'id': 'id_aluno'}, inplace=True)\n        \nnotas\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      nota\n    \n    \n      id_aluno\n      periodo\n      disciplina\n      unidade\n      \n    \n  \n  \n    \n      1\n      2017.1\n      Sistemas Operacionais\n      1\n      8.3\n    \n    \n      2\n      8.0\n    \n    \n      3\n      9.3\n    \n    \n      4\n      8.0\n    \n    \n      2\n      2017.1\n      Sistemas Operacionais\n      1\n      7.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23\n      2017.2\n      Sistemas Distribuidos\n      2\n      9.7\n    \n    \n      3\n      10.0\n    \n    \n      24\n      2017.2\n      Sistemas Distribuidos\n      1\n      9.8\n    \n    \n      2\n      8.0\n    \n    \n      3\n      9.5\n    \n  \n\n344 rows × 1 columns\n\n\n\nComo calcular agora a nota média para cada disciplina, período e unidade? Ficou bem mais fácil.\n\n\n\nnotas.groupby([\"disciplina\", \"periodo\", \"unidade\"]).mean()\n\n\n\n\n\n  \n    \n      \n      \n      \n      nota\n    \n    \n      disciplina\n      periodo\n      unidade\n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      1\n      7.939286\n    \n    \n      2\n      8.382143\n    \n    \n      3\n      7.425000\n    \n    \n      Sistemas Distribuidos\n      2017.2\n      1\n      7.441667\n    \n    \n      2\n      6.729167\n    \n    \n      3\n      8.895833\n    \n    \n      Sistemas Operacionais\n      2017.1\n      1\n      7.595238\n    \n    \n      2\n      8.438095\n    \n    \n      3\n      7.771429\n    \n    \n      4\n      6.742857\n    \n    \n      2017.2\n      1\n      7.457692\n    \n    \n      2\n      7.876923\n    \n    \n      3\n      6.996154\n    \n    \n      4\n      5.892308\n    \n  \n\n\n\n\n\n\n\n\nmedia_alunos = notas.groupby([\"disciplina\", \"periodo\", \"id_aluno\"]).mean()\nmedia_alunos\n\n\n\n\n\n  \n    \n      \n      \n      \n      nota\n    \n    \n      disciplina\n      periodo\n      id_aluno\n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      1\n      7.266667\n    \n    \n      2\n      7.333333\n    \n    \n      3\n      8.166667\n    \n    \n      4\n      8.866667\n    \n    \n      5\n      7.100000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      Sistemas Operacionais\n      2017.2\n      22\n      1.250000\n    \n    \n      23\n      7.375000\n    \n    \n      24\n      8.400000\n    \n    \n      25\n      7.375000\n    \n    \n      26\n      6.400000\n    \n  \n\n99 rows × 1 columns\n\n\n\n\n\n\n\nmedia_turma = media_alunos.groupby([\"disciplina\", \"periodo\"]).mean()\nmedia_turma\n\n\n\n\n\n  \n    \n      \n      \n      nota\n    \n    \n      disciplina\n      periodo\n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      7.915476\n    \n    \n      Sistemas Distribuidos\n      2017.2\n      7.688889\n    \n    \n      Sistemas Operacionais\n      2017.1\n      7.636905\n    \n    \n      2017.2\n      7.055769\n    \n  \n\n\n\n\n\n\n\nE se eu quiser calcular outras estatísticas como a quantidade de alunos, nota mínima, mediana, média e máxima? Podemos usar a função agg para aplicar várias funções de agregação nos dados.\n\ndef prop_aprovados(medias, min_media_aprovacao=5):\n    return medias[medias >= min_media_aprovacao].count() / medias.count()\n\n(\n    media_alunos\n    .groupby([\"disciplina\", \"periodo\"])\n    .agg(['count', 'min', 'median', 'mean', 'max', prop_aprovados])\n)\n\n\n\n\n\n  \n    \n      \n      \n      nota\n    \n    \n      \n      \n      count\n      min\n      median\n      mean\n      max\n      prop_aprovados\n    \n    \n      disciplina\n      periodo\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      28\n      0.000\n      8.266667\n      7.915476\n      10.00\n      0.892857\n    \n    \n      Sistemas Distribuidos\n      2017.2\n      24\n      0.000\n      8.250000\n      7.688889\n      10.00\n      0.916667\n    \n    \n      Sistemas Operacionais\n      2017.1\n      21\n      0.875\n      8.100000\n      7.636905\n      9.65\n      0.904762\n    \n    \n      2017.2\n      26\n      1.250\n      7.375000\n      7.055769\n      9.00\n      0.923077\n    \n  \n\n\n\n\nVocê também pode aplicar uma função própria, por exemplo a que calcula a proporção de alunos aprovados em cada disciplina, que conta as linhas apenas de alunos com nota maior ou igual à 5 e divide pela quantidade total de alunos dentro do grupo:\n\ndef prop_aprovados(medias, min_media_aprovacao=5):\n    return medias[medias >= min_media_aprovacao].count() / medias.count()\n\n(\n    media_alunos\n    .groupby([\"disciplina\", \"periodo\"])\n    .agg([prop_aprovados])\n\"APROVADO\" if media_alunos[\"nota\"] >= 5 else \"REPROVADO\")\n\nSyntaxError: invalid syntax (2677895194.py, line 8)\n\n\n\n\n\nVamos adicionar uma coluna na tabela indicando se o aluno foi aprovado por média ou não. Para isso, vamos criar a nova função e aplicá-la para cada linha do dataframe com a função apply:\n\ndef calcula_resultado(media):\n    return 'APROVADO' if media >= 5 else 'REPROVADO'\n\nmedia_alunos[\"resultado\"] = media_alunos[\"nota\"].apply(calcula_resultado)\nmedia_alunos\n\n\n\n\n\n  \n    \n      \n      \n      \n      nota\n      resultado\n    \n    \n      disciplina\n      periodo\n      id_aluno\n      \n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      1\n      7.266667\n      APROVADO\n    \n    \n      2\n      7.333333\n      APROVADO\n    \n    \n      3\n      8.166667\n      APROVADO\n    \n    \n      4\n      8.866667\n      APROVADO\n    \n    \n      5\n      7.100000\n      APROVADO\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Sistemas Operacionais\n      2017.2\n      22\n      1.250000\n      REPROVADO\n    \n    \n      23\n      7.375000\n      APROVADO\n    \n    \n      24\n      8.400000\n      APROVADO\n    \n    \n      25\n      7.375000\n      APROVADO\n    \n    \n      26\n      6.400000\n      APROVADO\n    \n  \n\n99 rows × 2 columns\n\n\n\n\n\n\nA função sort_values ordena o dataframe com base nos valores de uma ou mais colunas. O parâmetro ascending=False é usado para indicar que a ordenação não será feita na ordem padrão ascendente (ou seja, será na ordem descendente do maior para o menor valor).\n\nmedia_alunos.sort_values('nota', ascending=False)\n\n\n\n\n\n  \n    \n      \n      \n      \n      nota\n      resultado\n    \n    \n      disciplina\n      periodo\n      id_aluno\n      \n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      9\n      10.000000\n      APROVADO\n    \n    \n      10\n      10.000000\n      APROVADO\n    \n    \n      23\n      10.000000\n      APROVADO\n    \n    \n      Sistemas Distribuidos\n      2017.2\n      12\n      10.000000\n      APROVADO\n    \n    \n      9\n      9.933333\n      APROVADO\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Sistemas Operacionais\n      2017.2\n      22\n      1.250000\n      REPROVADO\n    \n    \n      2017.1\n      20\n      0.875000\n      REPROVADO\n    \n    \n      Sistemas Distribuidos\n      2017.2\n      8\n      0.666667\n      REPROVADO\n    \n    \n      3\n      0.000000\n      REPROVADO\n    \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      20\n      0.000000\n      REPROVADO\n    \n  \n\n99 rows × 2 columns\n\n\n\n\n\n\nPega o dataframe com a média dos alunos, ordena pela média em ordem decrescente, agrupa por turma (disciplina e período) e pega as 3 primeiras linhas de cada grupo.\n\n(\n    media_alunos\n    .sort_values('nota', ascending=False)\n    .groupby(['disciplina', 'periodo'])\n    .head(3)\n)\n\n\n\n\n\n  \n    \n      \n      \n      \n      nota\n    \n    \n      disciplina\n      periodo\n      id_aluno\n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      9\n      10.000000\n    \n    \n      10\n      10.000000\n    \n    \n      23\n      10.000000\n    \n    \n      Sistemas Distribuidos\n      2017.2\n      12\n      10.000000\n    \n    \n      9\n      9.933333\n    \n    \n      23\n      9.900000\n    \n    \n      Sistemas Operacionais\n      2017.1\n      9\n      9.650000\n    \n    \n      6\n      9.600000\n    \n    \n      11\n      9.575000\n    \n    \n      2017.2\n      1\n      9.000000\n    \n    \n      7\n      9.000000\n    \n    \n      8\n      8.950000\n    \n  \n\n\n\n\nNote que organizamos a sequência de comandos com uma função por linha. Essa forma fica mais legível quando se tem muitos comandos seguidos. Como cada função retorna um dataframe, dá para encadear chamadas de funções dessa forma no Pandas. Para funcionar com um comando por linha é preciso colocar a sequência de funções entre parênteses.\n\n\n\n\nOutra utilidade do Pandas é pegar apenas “fatias” (slices) dos dados, seja filtrando linhas ou colunas. O pandas oferece a função .loc para fazer o slicing a partir dos índices e a função .iloc para filtrar com base no número da linha.\nPor exemplo, para pegar apenas as linhas que tem “Sistemas Operacionais” como primeiro índice (disciplina), podemos usar:\n\nmedia_alunos.loc[\"Sistemas Distribuidos\"]\n\n\n\n\n\n  \n    \n      \n      \n      nota\n    \n    \n      periodo\n      id_aluno\n      \n    \n  \n  \n    \n      2017.2\n      1\n      8.100000\n    \n    \n      2\n      5.566667\n    \n    \n      3\n      0.000000\n    \n    \n      4\n      8.333333\n    \n    \n      5\n      8.166667\n    \n    \n      6\n      9.533333\n    \n    \n      7\n      8.433333\n    \n    \n      8\n      0.666667\n    \n    \n      9\n      9.933333\n    \n    \n      10\n      6.266667\n    \n    \n      11\n      9.700000\n    \n    \n      12\n      10.000000\n    \n    \n      13\n      7.666667\n    \n    \n      14\n      6.033333\n    \n    \n      15\n      9.133333\n    \n    \n      16\n      7.266667\n    \n    \n      17\n      8.833333\n    \n    \n      18\n      7.566667\n    \n    \n      19\n      8.766667\n    \n    \n      20\n      9.833333\n    \n    \n      21\n      7.933333\n    \n    \n      22\n      7.800000\n    \n    \n      23\n      9.900000\n    \n    \n      24\n      9.100000\n    \n  \n\n\n\n\nE se quisermos pegar apenas as linhas 5 a 10 do DataFrame podemos usar:\n\nmedia_alunos.iloc[5:10]\n\n\n\n\n\n  \n    \n      \n      \n      \n      nota\n    \n    \n      disciplina\n      periodo\n      id_aluno\n      \n    \n  \n  \n    \n      Avaliação de Desempenho de Sistemas\n      2017.1\n      6\n      8.200000\n    \n    \n      7\n      9.933333\n    \n    \n      8\n      3.766667\n    \n    \n      9\n      10.000000\n    \n    \n      10\n      10.000000\n    \n  \n\n\n\n\nPodemos também fazer um slice de linha e coluna ao mesmo tempo. Por exemplo, para filtrar as linhas com índices de 0 a 4 e a coluna nota_3, podemos fazer:\n\nso.loc[0:4, \"nota_3\"]\n\n0     9.3\n1     4.6\n2     7.7\n3     9.8\n4    10.0\nName: nota_3, dtype: float64\n\n\nOu para pegar o valor que está na linha 10 e coluna 3 (lembrando que Python indexa a partir do 0):\n\nso.iloc[10, 3]\n\n8.5\n\n\n\n\n\nTambém podemos exportar um dataframe pra um arquivo. Por exemplo, para exportar a tabela de média dos alunos para um arquivo podemos rodar:\nmedia_alunos.to_csv(\"../dados/notas_medias_alunos.csv\")"
  },
  {
    "objectID": "2-eda/4-exemplo-eda-eleicoes.html",
    "href": "2-eda/4-exemplo-eda-eleicoes.html",
    "title": "Aulas AD 1",
    "section": "",
    "text": "cand = pd.read_csv('~/local/datasets/dados-tse/consulta_cand_2018/consulta_cand_2018_PB.csv',\n                   encoding='latin1', delimiter=';')\nbem = pd.read_csv('~/local/datasets/dados-tse/bem_candidato_2018/bem_candidato_2018_PB.csv',\n                   encoding='latin1', delimiter=';', decimal=\",\")\n\nfed_cand = cand[cand.DS_CARGO == 'DEPUTADO FEDERAL']\nfed_cand.VR_DESPESA_MAX_CAMPANHA\n\n0      2500000\n1      2500000\n2      2500000\n4      2500000\n8      2500000\n        ...   \n594    2500000\n595    2500000\n598    2500000\n599    2500000\n615    2500000\nName: VR_DESPESA_MAX_CAMPANHA, Length: 159, dtype: int64\n\n\n\nbem_total = bem.groupby(['SQ_CANDIDATO']).VR_BEM_CANDIDATO.sum()\nbem_total\n\nSQ_CANDIDATO\n150000600157    299157.64\n150000600158      5000.00\n150000600159    104342.05\n150000600160    134000.00\n150000600161    154800.00\n                  ...    \n150000627767    100000.00\n150000628272     12000.00\n150000628316    150000.00\n150000629583    133777.18\n150000630006    152000.00\nName: VR_BEM_CANDIDATO, Length: 346, dtype: float64\n\n\n\nbem_total.describe()\n\ncount    3.460000e+02\nmean     5.694704e+05\nstd      9.459294e+05\nmin      3.276000e+01\n25%      8.000000e+04\n50%      2.320457e+05\n75%      6.322912e+05\nmax      8.037674e+06\nName: VR_BEM_CANDIDATO, dtype: float64\n\n\n\nsns.histplot(bem_total, bins=20)\n\n<AxesSubplot:xlabel='VR_BEM_CANDIDATO', ylabel='Count'>\n\n\n\n\n\n\nbem_total\n\nSQ_CANDIDATO\n150000600157    299157.64\n150000600158      5000.00\n150000600159    104342.05\n150000600160    134000.00\n150000600161    154800.00\n                  ...    \n150000627767    100000.00\n150000628272     12000.00\n150000628316    150000.00\n150000629583    133777.18\n150000630006    152000.00\nName: VR_BEM_CANDIDATO, Length: 346, dtype: float64\n\n\n\nfed_cand.describe()\n\n\n\n\n\n  \n    \n      \n      ANO_ELEICAO\n      CD_TIPO_ELEICAO\n      NR_TURNO\n      CD_ELEICAO\n      CD_CARGO\n      SQ_CANDIDATO\n      NR_CANDIDATO\n      NR_CPF_CANDIDATO\n      CD_SITUACAO_CANDIDATURA\n      CD_DETALHE_SITUACAO_CAND\n      ...\n      CD_GRAU_INSTRUCAO\n      CD_ESTADO_CIVIL\n      CD_COR_RACA\n      CD_OCUPACAO\n      VR_DESPESA_MAX_CAMPANHA\n      CD_SIT_TOT_TURNO\n      NR_PROTOCOLO_CANDIDATURA\n      NR_PROCESSO\n      CD_SITUACAO_CANDIDATO_PLEITO\n      CD_SITUACAO_CANDIDATO_URNA\n    \n  \n  \n    \n      count\n      159.0\n      159.0\n      159.0\n      159.0\n      159.0\n      1.590000e+02\n      159.000000\n      1.590000e+02\n      159.000000\n      159.000000\n      ...\n      159.000000\n      159.000000\n      159.000000\n      159.000000\n      159.0\n      159.000000\n      159.0\n      1.590000e+02\n      159.000000\n      159.000000\n    \n    \n      mean\n      2018.0\n      2.0\n      1.0\n      297.0\n      6.0\n      1.500006e+11\n      3585.672956\n      3.334962e+10\n      10.754717\n      3.232704\n      ...\n      7.025157\n      3.540881\n      1.635220\n      424.213836\n      2500000.0\n      4.132075\n      -1.0\n      6.004408e+18\n      2.176101\n      2.006289\n    \n    \n      std\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      7.936943e+03\n      1994.729598\n      2.980634e+10\n      3.117360\n      3.396191\n      ...\n      1.453715\n      2.668955\n      0.903029\n      338.249110\n      0.0\n      1.547203\n      0.0\n      1.802354e+15\n      1.924310\n      1.481946\n    \n    \n      min\n      2018.0\n      2.0\n      1.0\n      297.0\n      6.0\n      1.500006e+11\n      1001.000000\n      7.811349e+07\n      3.000000\n      2.000000\n      ...\n      3.000000\n      1.000000\n      1.000000\n      101.000000\n      2500000.0\n      -1.000000\n      -1.0\n      6.001577e+18\n      -1.000000\n      -1.000000\n    \n    \n      25%\n      2018.0\n      2.0\n      1.0\n      297.0\n      6.0\n      1.500006e+11\n      1768.500000\n      6.058035e+09\n      12.000000\n      2.000000\n      ...\n      6.000000\n      1.000000\n      1.000000\n      167.500000\n      2500000.0\n      4.000000\n      -1.0\n      6.003690e+18\n      2.000000\n      2.000000\n    \n    \n      50%\n      2018.0\n      2.0\n      1.0\n      297.0\n      6.0\n      1.500006e+11\n      3131.000000\n      2.625417e+10\n      12.000000\n      2.000000\n      ...\n      8.000000\n      3.000000\n      1.000000\n      266.000000\n      2500000.0\n      5.000000\n      -1.0\n      6.004097e+18\n      2.000000\n      2.000000\n    \n    \n      75%\n      2018.0\n      2.0\n      1.0\n      297.0\n      6.0\n      1.500006e+11\n      5014.000000\n      5.634612e+10\n      12.000000\n      2.000000\n      ...\n      8.000000\n      3.000000\n      3.000000\n      755.500000\n      2500000.0\n      5.000000\n      -1.0\n      6.004690e+18\n      2.000000\n      2.000000\n    \n    \n      max\n      2018.0\n      2.0\n      1.0\n      297.0\n      6.0\n      1.500006e+11\n      9090.000000\n      9.814353e+10\n      12.000000\n      14.000000\n      ...\n      8.000000\n      9.000000\n      4.000000\n      999.000000\n      2500000.0\n      5.000000\n      -1.0\n      6.007925e+18\n      14.000000\n      17.000000\n    \n  \n\n8 rows × 27 columns\n\n\n\n\nvot_cand = pd.read_csv('~/local/datasets/dados-tse/votacao_candidato.csv', encoding='latin1',\n                       delimiter=';')\nvot_cand\n\n\n\n\n\n  \n    \n      \n      ds_cargo\n      cd_municipio\n      nm_municipio\n      nm_candidato\n      nr_candidato\n      sg_partido\n      nm_regiao\n      ds_sit_totalizacao\n      nr_turno\n      sg_uf\n      qt_votos_nominais\n      qt_votos_validos\n      dt_carga\n    \n  \n  \n    \n      0\n      Deputado Estadual\n      19003\n      RIACHÃO DO POÇO\n      ADEILTON FERREIRA DE SOUZA\n      15111\n      MDB\n      NORDESTE\n      Suplente\n      1\n      PB\n      0\n      2982\n      2022-09-11 04:27:51\n    \n    \n      1\n      Deputado Estadual\n      19003\n      RIACHÃO DO POÇO\n      ADERALDO GONÇALVES DO NASCIMENTO JÚNIOR\n      33333\n      PMN\n      NORDESTE\n      Suplente\n      1\n      PB\n      1\n      2982\n      2022-09-11 04:27:51\n    \n    \n      2\n      Deputado Estadual\n      19003\n      RIACHÃO DO POÇO\n      ADILSON MORAES DE LUCENA\n      65224\n      PC do B\n      NORDESTE\n      Suplente\n      1\n      PB\n      0\n      2982\n      2022-09-11 04:27:51\n    \n    \n      3\n      Deputado Estadual\n      19003\n      RIACHÃO DO POÇO\n      ADRIANO CEZAR GALDINO DE ARAÚJO\n      40444\n      PSB\n      NORDESTE\n      Eleito\n      1\n      PB\n      0\n      2982\n      2022-09-11 04:27:51\n    \n    \n      4\n      Deputado Estadual\n      19003\n      RIACHÃO DO POÇO\n      ALIPIO BARBOSA DE CARVALHO FILHO\n      36014\n      PTC\n      NORDESTE\n      Suplente\n      1\n      PB\n      0\n      2982\n      2022-09-11 04:27:51\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      81613\n      Deputado Estadual\n      22438\n      VÁRZEA\n      WILSON PAULO DE LIRA\n      15112\n      MDB\n      NORDESTE\n      Suplente\n      1\n      PB\n      0\n      1743\n      2022-09-11 04:27:51\n    \n    \n      81614\n      Deputado Estadual\n      22438\n      VÁRZEA\n      WIVIANE EUGÊNIA PAIVA\n      28123\n      PRTB\n      NORDESTE\n      Suplente\n      1\n      PB\n      0\n      1743\n      2022-09-11 04:27:51\n    \n    \n      81615\n      Deputado Estadual\n      22438\n      VÁRZEA\n      YASNAIA POLLYANNA WERTON DUTRA\n      40111\n      PSB\n      NORDESTE\n      Eleito\n      1\n      PB\n      14\n      1743\n      2022-09-11 04:27:51\n    \n    \n      81616\n      Deputado Estadual\n      22438\n      VÁRZEA\n      ZENIVALDO DE OLIVEIRA BARBOSA\n      13069\n      PT\n      NORDESTE\n      Suplente\n      1\n      PB\n      0\n      1743\n      2022-09-11 04:27:51\n    \n    \n      81617\n      Deputado Estadual\n      22438\n      VÁRZEA\n      ZENNEDY BEZERRA\n      43660\n      PV\n      NORDESTE\n      Suplente\n      1\n      PB\n      0\n      1743\n      2022-09-11 04:27:51\n    \n  \n\n81618 rows × 13 columns"
  },
  {
    "objectID": "2-eda/analise_eleicoes_2022.html",
    "href": "2-eda/analise_eleicoes_2022.html",
    "title": "Aulas AD 1",
    "section": "",
    "text": "eleicoes = pd.read_csv(ELEICOES_FILE)\n\nbolsonaro = eleicoes.loc[(eleicoes.nm_urna_candidato == 'JAIR BOLSONARO') & (eleicoes.nr_turno == 1)]\nfig, ax = plt.subplots(figsize=(14, 8))\np = sns.barplot(x='uf', y='perc_votos', hue='ano_eleicao', ax=ax, data=bolsonaro)\nplt.show()\n\n\n\n\nbolsonaro_dif = (\n    bolsonaro.pivot_table(index='uf', columns='ano_eleicao', values='perc_votos')\n    .reset_index()\n    .query(\"uf != 'ZZ'\")\n)\nbolsonaro_dif['dif'] = bolsonaro_dif[2022] - bolsonaro_dif[2018]\n\np = sns.barplot(x='uf', y='dif', data=bolsonaro_dif, palette=cm.RdYlBu(bolsonaro_dif['dif']))\np.set(xlabel='UF', ylabel='Diferença no percentual de votos',\n      title='Diferença no percentual de votos de Bolsonaro entre 2022 e 2018')\nplt.show()\n\n\n\n\n\npt = eleicoes.loc[(eleicoes.nr_candidato == 13) & (eleicoes.nr_turno == 1)]\nax = sns.barplot(x='uf', y='perc_votos', hue='ano_eleicao', data=pt)\nplt.show()\n\n\n\n\n\npt_dif = (\n    pt.pivot_table(index='uf', columns='ano_eleicao', values='perc_votos')\n    .reset_index()\n    .query(\"uf != 'ZZ'\")\n)\npt_dif['dif'] = pt_dif[2022] - pt_dif[2018]\n\np = sns.barplot(x='uf', y='dif', data=pt_dif, palette=cm.RdYlBu(pt_dif['dif']))\np.set(xlabel='UF', ylabel='Diferença no percentual de votos',\n      title='Diferença no percentual de votos do PT entre 2022 e 2018 no 1o turno')\nplt.show()\n\n\n\n\n\njb_pt = bolsonaro.merge(pt, on=['ano_eleicao', 'nr_turno', 'uf'], suffixes=['_jb', '_pt'])\njb_pt['dif_jb_pt'] = jb_pt['perc_votos_jb'] - jb_pt['perc_votos_pt']\njb_pt\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0_jb\n      ano_eleicao\n      nr_turno\n      nr_candidato_jb\n      nm_urna_candidato_jb\n      uf\n      votos_jb\n      perc_votos_jb\n      Unnamed: 0_pt\n      nr_candidato_pt\n      nm_urna_candidato_pt\n      votos_pt\n      perc_votos_pt\n      dif_jb_pt\n    \n  \n  \n    \n      0\n      112\n      2018\n      1\n      17\n      JAIR BOLSONARO\n      AC\n      262508\n      62.238429\n      28\n      13\n      FERNANDO HADDAD\n      78170\n      18.533447\n      43.704982\n    \n    \n      1\n      113\n      2018\n      1\n      17\n      JAIR BOLSONARO\n      AL\n      528355\n      34.403801\n      29\n      13\n      FERNANDO HADDAD\n      687247\n      44.750043\n      -10.346242\n    \n    \n      2\n      114\n      2018\n      1\n      17\n      JAIR BOLSONARO\n      AM\n      805902\n      43.477876\n      30\n      13\n      FERNANDO HADDAD\n      746998\n      40.300045\n      3.177832\n    \n    \n      3\n      115\n      2018\n      1\n      17\n      JAIR BOLSONARO\n      AP\n      166935\n      40.740000\n      31\n      13\n      FERNANDO HADDAD\n      134287\n      32.772350\n      7.967649\n    \n    \n      4\n      116\n      2018\n      1\n      17\n      JAIR BOLSONARO\n      BA\n      1725140\n      23.412220\n      32\n      13\n      FERNANDO HADDAD\n      4441955\n      60.282661\n      -36.870441\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6323\n      5455\n      2022\n      1\n      22\n      JAIR BOLSONARO\n      ZZ\n      122548\n      41.610000\n      4106\n      13\n      LULA\n      138933\n      47.170000\n      -5.560000\n    \n    \n      6324\n      5455\n      2022\n      1\n      22\n      JAIR BOLSONARO\n      ZZ\n      122548\n      41.610000\n      4442\n      13\n      LULA\n      138933\n      47.170000\n      -5.560000\n    \n    \n      6325\n      5455\n      2022\n      1\n      22\n      JAIR BOLSONARO\n      ZZ\n      122548\n      41.610000\n      4778\n      13\n      LULA\n      138933\n      47.170000\n      -5.560000\n    \n    \n      6326\n      5455\n      2022\n      1\n      22\n      JAIR BOLSONARO\n      ZZ\n      122548\n      41.610000\n      5114\n      13\n      LULA\n      138933\n      47.170000\n      -5.560000\n    \n    \n      6327\n      5455\n      2022\n      1\n      22\n      JAIR BOLSONARO\n      ZZ\n      122548\n      41.610000\n      5450\n      13\n      LULA\n      138933\n      47.170000\n      -5.560000\n    \n  \n\n6328 rows × 14 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 10))\ndf = jb_pt[jb_pt.nr_turno == 1]\np = sns.barplot(x='dif_jb_pt', y='uf', hue='ano_eleicao', ax=ax, data=df)\nplt.show()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 10))\ndf = jb_pt[jb_pt.nr_turno == 1]\n#p = sns.scatterplot(x='dif_jb_pt', y='uf', hue='ano_eleicao', palette=\"deep\", ax=ax, data=df)\np = sns.scatterplot(x='dif_jb_pt', y='uf', hue='ano_eleicao', palette=\"deep\", ax=ax, data=df)\nsns.lineplot(x='dif_jb_pt', y='uf', hue='uf', palette=\"deep\", ax=ax, data=df)\nplt.show()\n\n\n\n\n\njb_pt_dif = bolsonaro_dif.merge(pt_dif, on=['uf'], suffixes=['_jb', '_pt'])\njb_pt_dif['dif_2018'] = jb_pt_dif['2018_jb'] - jb_pt_dif['2018_pt']\njb_pt_dif['dif_2022'] = jb_pt_dif['2022_jb'] - jb_pt_dif['2022_pt']\njb_pt_dif['dif_jb_pt'] = jb_pt_dif['dif_2022'] - jb_pt_dif['dif_2018']\njb_pt_dif\n\n\n\n\n\n  \n    \n      ano_eleicao\n      uf\n      2018_jb\n      2022_jb\n      dif_jb\n      2018_pt\n      2022_pt\n      dif_pt\n      dif_2018\n      dif_2022\n      dif_jb_pt\n    \n  \n  \n    \n      0\n      AC\n      62.238429\n      62.50\n      0.261571\n      18.533447\n      29.26\n      10.726553\n      43.704982\n      33.24\n      -10.464982\n    \n    \n      1\n      AL\n      34.403801\n      36.05\n      1.646199\n      44.750043\n      56.50\n      11.749957\n      -10.346242\n      -20.45\n      -10.103758\n    \n    \n      2\n      AM\n      43.477876\n      42.80\n      -0.677876\n      40.300045\n      49.58\n      9.279955\n      3.177832\n      -6.78\n      -9.957832\n    \n    \n      3\n      AP\n      40.740000\n      43.41\n      2.670000\n      32.772350\n      45.67\n      12.897650\n      7.967649\n      -2.26\n      -10.227649\n    \n    \n      4\n      BA\n      23.412220\n      24.31\n      0.897780\n      60.282661\n      69.73\n      9.447339\n      -36.870441\n      -45.42\n      -8.549559\n    \n    \n      5\n      CE\n      21.742650\n      25.38\n      3.637350\n      33.123784\n      65.91\n      32.786216\n      -11.381134\n      -40.53\n      -29.148866\n    \n    \n      6\n      DF\n      58.366100\n      51.65\n      -6.716100\n      11.873230\n      36.85\n      24.976770\n      46.492870\n      14.80\n      -31.692870\n    \n    \n      7\n      ES\n      54.760857\n      52.23\n      -2.530857\n      24.198740\n      40.40\n      16.201260\n      30.562117\n      11.83\n      -18.732117\n    \n    \n      8\n      GO\n      57.237573\n      52.16\n      -5.077573\n      21.855471\n      39.51\n      17.654529\n      35.382103\n      12.65\n      -22.732103\n    \n    \n      9\n      MA\n      24.281077\n      26.02\n      1.738923\n      61.261374\n      68.84\n      7.578626\n      -36.980297\n      -42.82\n      -5.839703\n    \n    \n      10\n      MG\n      48.308929\n      43.60\n      -4.708929\n      27.648672\n      48.29\n      20.641328\n      20.660257\n      -4.69\n      -25.350257\n    \n    \n      11\n      MS\n      55.056999\n      52.70\n      -2.356999\n      23.866867\n      39.04\n      15.173133\n      31.190133\n      13.66\n      -17.530133\n    \n    \n      12\n      MT\n      60.039005\n      59.84\n      -0.199005\n      24.759506\n      34.39\n      9.630494\n      35.279499\n      25.45\n      -9.829499\n    \n    \n      13\n      PA\n      36.190922\n      40.27\n      4.079078\n      41.393476\n      52.22\n      10.826524\n      -5.202553\n      -11.95\n      -6.747447\n    \n    \n      14\n      PB\n      31.295508\n      29.62\n      -1.675508\n      45.457308\n      64.21\n      18.752692\n      -14.161800\n      -34.59\n      -20.428200\n    \n    \n      15\n      PE\n      30.573946\n      29.91\n      -0.663946\n      48.867692\n      65.27\n      16.402308\n      -18.293746\n      -35.36\n      -17.066254\n    \n    \n      16\n      PI\n      18.764451\n      19.90\n      1.135549\n      63.395518\n      74.25\n      10.854482\n      -44.631068\n      -54.35\n      -9.718932\n    \n    \n      17\n      PR\n      56.892693\n      55.26\n      -1.632693\n      19.704446\n      35.99\n      16.285554\n      37.188247\n      19.27\n      -17.918247\n    \n    \n      18\n      RJ\n      59.785790\n      51.09\n      -8.695790\n      14.694689\n      40.68\n      25.985311\n      45.091101\n      10.41\n      -34.681101\n    \n    \n      19\n      RN\n      30.210467\n      31.02\n      0.809533\n      41.186428\n      62.98\n      21.793572\n      -10.975962\n      -31.96\n      -20.984038\n    \n    \n      20\n      RO\n      62.239609\n      64.36\n      2.120391\n      20.361521\n      28.98\n      8.618479\n      41.878088\n      35.38\n      -6.498088\n    \n    \n      21\n      RR\n      62.974551\n      69.57\n      6.595449\n      17.849762\n      23.05\n      5.200238\n      45.124789\n      46.52\n      1.395211\n    \n    \n      22\n      RS\n      52.627017\n      48.89\n      -3.737017\n      22.805894\n      42.28\n      19.474106\n      29.821123\n      6.61\n      -23.211123\n    \n    \n      23\n      SC\n      65.819207\n      62.21\n      -3.609207\n      15.131720\n      29.54\n      14.408280\n      50.687487\n      32.67\n      -18.017487\n    \n    \n      24\n      SE\n      27.211105\n      29.16\n      1.948895\n      50.091548\n      63.82\n      13.728452\n      -22.880443\n      -34.66\n      -11.779557\n    \n    \n      25\n      SP\n      53.004450\n      47.71\n      -5.294450\n      16.417669\n      40.89\n      24.472331\n      36.586781\n      6.82\n      -29.766781\n    \n    \n      26\n      TO\n      44.635287\n      44.00\n      -0.635287\n      41.124267\n      50.40\n      9.275733\n      3.511021\n      -6.40\n      -9.911021\n    \n  \n\n\n\n\n\np = sns.barplot(x='uf', y='dif_jb_pt', data=jb_pt_dif, palette=cm.RdYlBu(jb_pt_dif['dif_jb_pt']))\np.set(xlabel='UF', ylabel='Diferença no percentual de votos',\n      title='Diferença no percentual de votos do PT entre 2022 e 2018 no 1o turno')\nplt.show()"
  }
]